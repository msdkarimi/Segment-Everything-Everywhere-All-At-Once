{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PB870ucW4br_"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KiXvT00A0-bY"
      },
      "outputs": [],
      "source": [
        "focal = 1\n",
        "focalt = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4BHEtvagk_1j",
        "outputId": "71a80b50-7346-479d-97b4-3347a6c6bd53"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "v9TMxh1MP3JH",
        "outputId": "4998f3c3-4430-4295-9a70-9b2ba6ecedde"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/drive/My Drive/SEEM weights/SEEM_DARE_30_29_classes.pt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cp  /content/drive/My\\ Drive/SEEM\\ weights/SEEM_DARE_30_29_classes.pt /content/data/output/test/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "47gqYcQWvcsH",
        "outputId": "f662d439-ba20-4f0d-f2bf-81097b39e6e8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/drive/My Drive/SEEM weights/H_SEEM_polyp_10e_best.pt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cp  /content/drive/My\\ Drive/SEEM\\ weights/H_SEEM_polyp_10e_best.pt /content/data/output/test/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "I3o6W7JUqZ5T",
        "outputId": "3b7730fb-5410-4d1e-d862-c71e5e8aa52a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'Segment-Everything-Everywhere-All-At-Once'...\n",
            "remote: Enumerating objects: 1167, done.\u001b[K\n",
            "remote: Counting objects: 100% (430/430), done.\u001b[K\n",
            "remote: Compressing objects: 100% (121/121), done.\u001b[K\n",
            "remote: Total 1167 (delta 335), reused 334 (delta 307), pack-reused 737\u001b[K\n",
            "Receiving objects: 100% (1167/1167), 303.85 MiB | 45.37 MiB/s, done.\n",
            "Resolving deltas: 100% (510/510), done.\n"
          ]
        }
      ],
      "source": [
        "# !rm -rf /content/*\n",
        "# !git clone https://github.com/UX-Decoder/Segment-Everything-Everywhere-All-At-Once.git /content/SEEM\n",
        "# !git clone -b v1.0 https://github.com/msdkarimi/Segment-Everything-Everywhere-All-At-Once.git /content/SEEM.\n",
        "# !git clone -b adopter_v1 https://github.com/msdkarimi/Segment-Everything-Everywhere-All-At-Once.git /content/SEEM\n",
        "# !git clone -b VL-ADAPTER_2 https://github.com/msdkarimi/Segment-Everything-Everywhere-All-At-Once.git /content/SEEM\n",
        "# !git clone -b adapter_prediction_heads https://github.com/msdkarimi/Segment-Everything-Everywhere-All-At-Once.git /content/SEEM\n",
        "# !git clone -b promt_eng https://github.com/msdkarimi/Segment-Everything-Everywhere-All-At-Once.git /content/SEEM\n",
        "# !git clone https://github.com/msdkarimi/SEEM.git\n",
        "# !git clone -b back_head_2 https://github.com/msdkarimi/SEEM.git\n",
        "# !git clone -b back_2 https://github.com/msdkarimi/SEEM.git\n",
        "!git clone -b fine-tune  https://github.com/msdkarimi/Segment-Everything-Everywhere-All-At-Once.git\n",
        "\n",
        "\n",
        "# !mv /content/SEEM/* /content/\n",
        "!mv /content/Segment-Everything-Everywhere-All-At-Once/* /content/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfICQQbU1gDH",
        "outputId": "4eb1e8fa-dfd3-4d61-90a4-b497994589ad"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/cocodataset/panopticapi.git\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-req-build-wrw1a8e6\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-req-build-wrw1a8e6\n",
            "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1) (1.25.2)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.10/dist-packages (from panopticapi==0.1) (9.4.0)\n",
            "Building wheels for collected packages: panopticapi\n",
            "  Building wheel for panopticapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for panopticapi: filename=panopticapi-0.1-py3-none-any.whl size=8260 sha256=67db89cafe5ef9b64ab1e0034772b8d31e9872ad34d7cc363e4c3bb3b62273f6\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-vbmpt3sr/wheels/70/87/ae/5c2b138c967549070e3fe35f3b5fcaf1ed56e9f5483a09ee65\n",
            "Successfully built panopticapi\n",
            "Installing collected packages: panopticapi\n",
            "Successfully installed panopticapi-0.1\n"
          ]
        }
      ],
      "source": [
        "# !pip install git+https://github.com/cocodataset/panopticapi.git"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nql6gG7dwQi5"
      },
      "source": [
        "Install requirements and required downgrades"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uM8_vei3voVe",
        "outputId": "7c21c300-91ce-4fd8-e0a2-f3a9af88f1ef"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting pandas==1.5.3\n",
            "  Downloading pandas-1.5.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.1/12.1 MB\u001b[0m \u001b[31m22.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (2023.4)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.10/dist-packages (from pandas==1.5.3) (1.25.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.1->pandas==1.5.3) (1.16.0)\n",
            "Installing collected packages: pandas\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 2.0.3\n",
            "    Uninstalling pandas-2.0.3:\n",
            "      Successfully uninstalled pandas-2.0.3\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "cudf-cu12 24.4.1 requires pandas<2.2.2dev0,>=2.0, but you have pandas 1.5.3 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.0.3, but you have pandas 1.5.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed pandas-1.5.3\n",
            "Collecting kaleido\n",
            "  Downloading kaleido-0.2.1-py2.py3-none-manylinux1_x86_64.whl (79.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.9/79.9 MB\u001b[0m \u001b[31m7.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: kaleido\n",
            "Successfully installed kaleido-0.2.1\n",
            "/bin/bash: line 1: 13: No such file or directory\n",
            "/bin/bash: line 1: 4.6.0: No such file or directory\n"
          ]
        }
      ],
      "source": [
        "\n",
        "!pip install pandas==1.5.3\n",
        "!pip install kaleido\n",
        "!pip install pyarrow<13\n",
        "!pip install numpy>=1.23.5\n",
        "!pip install typing-extensions<4.6.0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "osmTn3uUqlSZ",
        "outputId": "d91e3ea3-40ee-4504-ef99-cad2dcd8639f"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting torch==2.1.0 (from -r assets/requirements/requirements.txt (line 1))\n",
            "  Downloading torch-2.1.0-cp310-cp310-manylinux1_x86_64.whl (670.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m670.2/670.2 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting torchvision==0.16.0 (from -r assets/requirements/requirements.txt (line 2))\n",
            "  Downloading torchvision-0.16.0-cp310-cp310-manylinux1_x86_64.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m98.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pillow==9.4.0 in /usr/local/lib/python3.10/dist-packages (from -r assets/requirements/requirements.txt (line 3)) (9.4.0)\n",
            "Collecting opencv-python==4.8.1.78 (from -r assets/requirements/requirements.txt (line 4))\n",
            "  Downloading opencv_python-4.8.1.78-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (61.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m61.7/61.7 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pyyaml==6.0.1 in /usr/local/lib/python3.10/dist-packages (from -r assets/requirements/requirements.txt (line 5)) (6.0.1)\n",
            "Collecting json_tricks==3.17.3 (from -r assets/requirements/requirements.txt (line 6))\n",
            "  Downloading json_tricks-3.17.3-py2.py3-none-any.whl (27 kB)\n",
            "Collecting yacs==0.1.8 (from -r assets/requirements/requirements.txt (line 7))\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Collecting scikit-learn==1.3.1 (from -r assets/requirements/requirements.txt (line 8))\n",
            "  Downloading scikit_learn-1.3.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (10.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m106.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pandas==2.0.3 (from -r assets/requirements/requirements.txt (line 9))\n",
            "  Downloading pandas-2.0.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.3/12.3 MB\u001b[0m \u001b[31m100.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting timm==0.4.12 (from -r assets/requirements/requirements.txt (line 10))\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m377.0/377.0 kB\u001b[0m \u001b[31m50.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting numpy==1.23.1 (from -r assets/requirements/requirements.txt (line 11))\n",
            "  Downloading numpy-1.23.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.0/17.0 MB\u001b[0m \u001b[31m81.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting einops==0.7.0 (from -r assets/requirements/requirements.txt (line 12))\n",
            "  Downloading einops-0.7.0-py3-none-any.whl (44 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m44.6/44.6 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fvcore==0.1.5.post20221221 (from -r assets/requirements/requirements.txt (line 13))\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.2/50.2 kB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting transformers==4.34.0 (from -r assets/requirements/requirements.txt (line 14))\n",
            "  Downloading transformers-4.34.0-py3-none-any.whl (7.7 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.7/7.7 MB\u001b[0m \u001b[31m106.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sentencepiece==0.1.99 in /usr/local/lib/python3.10/dist-packages (from -r assets/requirements/requirements.txt (line 15)) (0.1.99)\n",
            "Collecting ftfy==6.1.1 (from -r assets/requirements/requirements.txt (line 16))\n",
            "  Downloading ftfy-6.1.1-py3-none-any.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.1/53.1 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting regex==2023.10.3 (from -r assets/requirements/requirements.txt (line 17))\n",
            "  Downloading regex-2023.10.3-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (773 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m773.9/773.9 kB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: nltk==3.8.1 in /usr/local/lib/python3.10/dist-packages (from -r assets/requirements/requirements.txt (line 18)) (3.8.1)\n",
            "Collecting mpi4py==3.1.5 (from -r assets/requirements/requirements.txt (line 19))\n",
            "  Downloading mpi4py-3.1.5.tar.gz (2.5 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m106.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting vision-datasets==0.2.2 (from -r assets/requirements/requirements.txt (line 20))\n",
            "  Downloading vision_datasets-0.2.2-py3-none-any.whl (39 kB)\n",
            "Collecting cython==3.0.2 (from -r assets/requirements/requirements.txt (line 21))\n",
            "  Downloading Cython-3.0.2-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.6/3.6 MB\u001b[0m \u001b[31m109.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: pycocotools==2.0.7 in /usr/local/lib/python3.10/dist-packages (from -r assets/requirements/requirements.txt (line 22)) (2.0.7)\n",
            "Collecting diffdist==0.1 (from -r assets/requirements/requirements.txt (line 23))\n",
            "  Downloading diffdist-0.1.tar.gz (4.6 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting pyarrow==13.0.0 (from -r assets/requirements/requirements.txt (line 24))\n",
            "  Downloading pyarrow-13.0.0-cp310-cp310-manylinux_2_28_x86_64.whl (40.0 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.0/40.0 MB\u001b[0m \u001b[31m17.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting cityscapesscripts==2.2.2 (from -r assets/requirements/requirements.txt (line 25))\n",
            "  Downloading cityscapesScripts-2.2.2-py3-none-any.whl (473 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m473.3/473.3 kB\u001b[0m \u001b[31m51.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shapely==1.8.0 (from -r assets/requirements/requirements.txt (line 26))\n",
            "  Downloading Shapely-1.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m87.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting scikit-image==0.21.0 (from -r assets/requirements/requirements.txt (line 27))\n",
            "  Downloading scikit_image-0.21.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (13.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m93.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting mup==1.0.0 (from -r assets/requirements/requirements.txt (line 28))\n",
            "  Downloading mup-1.0.0.tar.gz (28 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting accelerate==0.23.0 (from -r assets/requirements/requirements.txt (line 29))\n",
            "  Downloading accelerate-0.23.0-py3-none-any.whl (258 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m258.1/258.1 kB\u001b[0m \u001b[31m29.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting kornia==0.7.0 (from -r assets/requirements/requirements.txt (line 30))\n",
            "  Downloading kornia-0.7.0-py2.py3-none-any.whl (705 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m705.7/705.7 kB\u001b[0m \u001b[31m15.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting deepspeed==0.10.3 (from -r assets/requirements/requirements.txt (line 31))\n",
            "  Downloading deepspeed-0.10.3.tar.gz (867 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m867.3/867.3 kB\u001b[0m \u001b[31m78.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting wandb==0.15.12 (from -r assets/requirements/requirements.txt (line 32))\n",
            "  Downloading wandb-0.15.12-py3-none-any.whl (2.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.1/2.1 MB\u001b[0m \u001b[31m87.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting infinibatch==0.1.1 (from -r assets/requirements/requirements.txt (line 33))\n",
            "  Downloading infinibatch-0.1.1-py3-none-any.whl (32 kB)\n",
            "Collecting gradio==3.42.0 (from -r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading gradio-3.42.0-py3-none-any.whl (20.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m20.1/20.1 MB\u001b[0m \u001b[31m78.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pycocoevalcap (from -r assets/requirements/requirements.txt (line 35))\n",
            "  Downloading pycocoevalcap-1.2-py3-none-any.whl (104.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.3/104.3 MB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1)) (3.14.0)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1)) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1)) (2023.6.0)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.1.105 (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_nvrtc_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (23.7 MB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.1.105 (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_runtime_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (823 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.1.105 (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cuda_cupti_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (14.1 MB)\n",
            "Collecting nvidia-cudnn-cu12==8.9.2.26 (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cudnn_cu12-8.9.2.26-py3-none-manylinux1_x86_64.whl (731.7 MB)\n",
            "Collecting nvidia-cublas-cu12==12.1.3.1 (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cublas_cu12-12.1.3.1-py3-none-manylinux1_x86_64.whl (410.6 MB)\n",
            "Collecting nvidia-cufft-cu12==11.0.2.54 (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cufft_cu12-11.0.2.54-py3-none-manylinux1_x86_64.whl (121.6 MB)\n",
            "Collecting nvidia-curand-cu12==10.3.2.106 (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_curand_cu12-10.3.2.106-py3-none-manylinux1_x86_64.whl (56.5 MB)\n",
            "Collecting nvidia-cusolver-cu12==11.4.5.107 (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cusolver_cu12-11.4.5.107-py3-none-manylinux1_x86_64.whl (124.2 MB)\n",
            "Collecting nvidia-cusparse-cu12==12.1.0.106 (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_cusparse_cu12-12.1.0.106-py3-none-manylinux1_x86_64.whl (196.0 MB)\n",
            "Collecting nvidia-nccl-cu12==2.18.1 (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Downloading nvidia_nccl_cu12-2.18.1-py3-none-manylinux1_x86_64.whl (209.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m209.8/209.8 MB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvtx-cu12==12.1.105 (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Using cached nvidia_nvtx_cu12-12.1.105-py3-none-manylinux1_x86_64.whl (99 kB)\n",
            "Collecting triton==2.1.0 (from torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Downloading triton-2.1.0-0-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.2/89.2 MB\u001b[0m \u001b[31m9.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: requests in /usr/local/lib/python3.10/dist-packages (from torchvision==0.16.0->-r assets/requirements/requirements.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: scipy>=1.5.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.1->-r assets/requirements/requirements.txt (line 8)) (1.11.4)\n",
            "Requirement already satisfied: joblib>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.1->-r assets/requirements/requirements.txt (line 8)) (1.4.2)\n",
            "Requirement already satisfied: threadpoolctl>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from scikit-learn==1.3.1->-r assets/requirements/requirements.txt (line 8)) (3.5.0)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r assets/requirements/requirements.txt (line 9)) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r assets/requirements/requirements.txt (line 9)) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas==2.0.3->-r assets/requirements/requirements.txt (line 9)) (2024.1)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.5.post20221221->-r assets/requirements/requirements.txt (line 13)) (4.66.4)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.5.post20221221->-r assets/requirements/requirements.txt (line 13)) (2.4.0)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from fvcore==0.1.5.post20221221->-r assets/requirements/requirements.txt (line 13)) (0.9.0)\n",
            "Collecting iopath>=0.1.7 (from fvcore==0.1.5.post20221221->-r assets/requirements/requirements.txt (line 13))\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m42.2/42.2 kB\u001b[0m \u001b[31m6.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0->-r assets/requirements/requirements.txt (line 14)) (0.23.2)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0->-r assets/requirements/requirements.txt (line 14)) (24.0)\n",
            "Collecting tokenizers<0.15,>=0.14 (from transformers==4.34.0->-r assets/requirements/requirements.txt (line 14))\n",
            "  Downloading tokenizers-0.14.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.8 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.8/3.8 MB\u001b[0m \u001b[31m97.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: safetensors>=0.3.1 in /usr/local/lib/python3.10/dist-packages (from transformers==4.34.0->-r assets/requirements/requirements.txt (line 14)) (0.4.3)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.10/dist-packages (from ftfy==6.1.1->-r assets/requirements/requirements.txt (line 16)) (0.2.13)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk==3.8.1->-r assets/requirements/requirements.txt (line 18)) (8.1.7)\n",
            "Requirement already satisfied: tenacity>=6.2.0 in /usr/local/lib/python3.10/dist-packages (from vision-datasets==0.2.2->-r assets/requirements/requirements.txt (line 20)) (8.3.0)\n",
            "Requirement already satisfied: matplotlib>=2.1.0 in /usr/local/lib/python3.10/dist-packages (from pycocotools==2.0.7->-r assets/requirements/requirements.txt (line 22)) (3.7.1)\n",
            "Collecting appdirs (from cityscapesscripts==2.2.2->-r assets/requirements/requirements.txt (line 25))\n",
            "  Downloading appdirs-1.4.4-py2.py3-none-any.whl (9.6 kB)\n",
            "Collecting pyquaternion (from cityscapesscripts==2.2.2->-r assets/requirements/requirements.txt (line 25))\n",
            "  Downloading pyquaternion-0.9.9-py3-none-any.whl (14 kB)\n",
            "Collecting coloredlogs (from cityscapesscripts==2.2.2->-r assets/requirements/requirements.txt (line 25))\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m7.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typing (from cityscapesscripts==2.2.2->-r assets/requirements/requirements.txt (line 25))\n",
            "  Downloading typing-3.7.4.3.tar.gz (78 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.6/78.6 kB\u001b[0m \u001b[31m13.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: imageio>=2.27 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.21.0->-r assets/requirements/requirements.txt (line 27)) (2.31.6)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.21.0->-r assets/requirements/requirements.txt (line 27)) (2024.5.22)\n",
            "Requirement already satisfied: PyWavelets>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.21.0->-r assets/requirements/requirements.txt (line 27)) (1.6.0)\n",
            "Requirement already satisfied: lazy_loader>=0.2 in /usr/local/lib/python3.10/dist-packages (from scikit-image==0.21.0->-r assets/requirements/requirements.txt (line 27)) (0.4)\n",
            "Requirement already satisfied: seaborn in /usr/local/lib/python3.10/dist-packages (from mup==1.0.0->-r assets/requirements/requirements.txt (line 28)) (0.13.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.10/dist-packages (from accelerate==0.23.0->-r assets/requirements/requirements.txt (line 29)) (5.9.5)\n",
            "Collecting hjson (from deepspeed==0.10.3->-r assets/requirements/requirements.txt (line 31))\n",
            "  Downloading hjson-3.1.0-py3-none-any.whl (54 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.0/54.0 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ninja (from deepspeed==0.10.3->-r assets/requirements/requirements.txt (line 31))\n",
            "  Downloading ninja-1.11.1.1-py2.py3-none-manylinux1_x86_64.manylinux_2_5_x86_64.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.2/307.2 kB\u001b[0m \u001b[31m40.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: py-cpuinfo in /usr/local/lib/python3.10/dist-packages (from deepspeed==0.10.3->-r assets/requirements/requirements.txt (line 31)) (9.0.0)\n",
            "Collecting pydantic<2.0.0 (from deepspeed==0.10.3->-r assets/requirements/requirements.txt (line 31))\n",
            "  Downloading pydantic-1.10.15-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.1/3.1 MB\u001b[0m \u001b[31m106.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting GitPython!=3.1.29,>=1.0.0 (from wandb==0.15.12->-r assets/requirements/requirements.txt (line 32))\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m30.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting sentry-sdk>=1.0.0 (from wandb==0.15.12->-r assets/requirements/requirements.txt (line 32))\n",
            "  Downloading sentry_sdk-2.5.1-py2.py3-none-any.whl (289 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m289.6/289.6 kB\u001b[0m \u001b[31m37.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting docker-pycreds>=0.4.0 (from wandb==0.15.12->-r assets/requirements/requirements.txt (line 32))\n",
            "  Downloading docker_pycreds-0.4.0-py2.py3-none-any.whl (9.0 kB)\n",
            "Collecting pathtools (from wandb==0.15.12->-r assets/requirements/requirements.txt (line 32))\n",
            "  Downloading pathtools-0.1.2.tar.gz (11 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting setproctitle (from wandb==0.15.12->-r assets/requirements/requirements.txt (line 32))\n",
            "  Downloading setproctitle-1.3.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (30 kB)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.12->-r assets/requirements/requirements.txt (line 32)) (67.7.2)\n",
            "Requirement already satisfied: protobuf!=4.21.0,<5,>=3.19.0 in /usr/local/lib/python3.10/dist-packages (from wandb==0.15.12->-r assets/requirements/requirements.txt (line 32)) (3.20.3)\n",
            "Collecting aiofiles<24.0,>=22.0 (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading aiofiles-23.2.1-py3-none-any.whl (15 kB)\n",
            "Requirement already satisfied: altair<6.0,>=4.2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (4.2.2)\n",
            "Collecting fastapi (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading fastapi-0.111.0-py3-none-any.whl (91 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m92.0/92.0 kB\u001b[0m \u001b[31m14.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting ffmpy (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading ffmpy-0.3.2.tar.gz (5.5 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting gradio-client==0.5.0 (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading gradio_client-0.5.0-py3-none-any.whl (298 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m298.2/298.2 kB\u001b[0m \u001b[31m38.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting httpx (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading httpx-0.27.0-py3-none-any.whl (75 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m75.6/75.6 kB\u001b[0m \u001b[31m12.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: importlib-resources<7.0,>=1.3 in /usr/local/lib/python3.10/dist-packages (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (6.4.0)\n",
            "Requirement already satisfied: markupsafe~=2.0 in /usr/local/lib/python3.10/dist-packages (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (2.1.5)\n",
            "Collecting orjson~=3.0 (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading orjson-3.10.4-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (142 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m142.7/142.7 kB\u001b[0m \u001b[31m20.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydub (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading pydub-0.25.1-py2.py3-none-any.whl (32 kB)\n",
            "Collecting python-multipart (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading python_multipart-0.0.9-py3-none-any.whl (22 kB)\n",
            "Collecting semantic-version~=2.0 (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading semantic_version-2.10.0-py2.py3-none-any.whl (15 kB)\n",
            "Collecting uvicorn>=0.14.0 (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading uvicorn-0.30.1-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.4/62.4 kB\u001b[0m \u001b[31m8.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting websockets<12.0,>=10.0 (from gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading websockets-11.0.3-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (129 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m129.9/129.9 kB\u001b[0m \u001b[31m17.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting nvidia-nvjitlink-cu12 (from nvidia-cusolver-cu12==11.4.5.107->torch==2.1.0->-r assets/requirements/requirements.txt (line 1))\n",
            "  Downloading nvidia_nvjitlink_cu12-12.5.40-py3-none-manylinux2014_x86_64.whl (21.3 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.3/21.3 MB\u001b[0m \u001b[31m53.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (0.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6.0,>=4.2.0->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (0.12.1)\n",
            "Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.10/dist-packages (from docker-pycreds>=0.4.0->wandb==0.15.12->-r assets/requirements/requirements.txt (line 32)) (1.16.0)\n",
            "Collecting gitdb<5,>=4.0.1 (from GitPython!=3.1.29,>=1.0.0->wandb==0.15.12->-r assets/requirements/requirements.txt (line 32))\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting portalocker (from iopath>=0.1.7->fvcore==0.1.5.post20221221->-r assets/requirements/requirements.txt (line 13))\n",
            "  Downloading portalocker-2.8.2-py3-none-any.whl (17 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7->-r assets/requirements/requirements.txt (line 22)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7->-r assets/requirements/requirements.txt (line 22)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7->-r assets/requirements/requirements.txt (line 22)) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7->-r assets/requirements/requirements.txt (line 22)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib>=2.1.0->pycocotools==2.0.7->-r assets/requirements/requirements.txt (line 22)) (3.1.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0->-r assets/requirements/requirements.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0->-r assets/requirements/requirements.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0->-r assets/requirements/requirements.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests->torchvision==0.16.0->-r assets/requirements/requirements.txt (line 2)) (2024.6.2)\n",
            "Collecting huggingface-hub<1.0,>=0.16.4 (from transformers==4.34.0->-r assets/requirements/requirements.txt (line 14))\n",
            "  Downloading huggingface_hub-0.17.3-py3-none-any.whl (295 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m295.0/295.0 kB\u001b[0m \u001b[31m32.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting h11>=0.8 (from uvicorn>=0.14.0->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m8.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting humanfriendly>=9.1 (from coloredlogs->cityscapesscripts==2.2.2->-r assets/requirements/requirements.txt (line 25))\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting starlette<0.38.0,>=0.37.2 (from fastapi->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m71.9/71.9 kB\u001b[0m \u001b[31m10.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting fastapi-cli>=0.0.2 (from fastapi->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading fastapi_cli-0.0.4-py3-none-any.whl (9.5 kB)\n",
            "Collecting ujson!=4.0.2,!=4.1.0,!=4.2.0,!=4.3.0,!=5.0.0,!=5.1.0,>=4.0.1 (from fastapi->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading ujson-5.10.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (53 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m53.6/53.6 kB\u001b[0m \u001b[31m8.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting email_validator>=2.0.0 (from fastapi->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading email_validator-2.1.1-py3-none-any.whl (30 kB)\n",
            "Requirement already satisfied: anyio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (3.7.1)\n",
            "Collecting httpcore==1.* (from httpx->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading httpcore-1.0.5-py3-none-any.whl (77 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m77.9/77.9 kB\u001b[0m \u001b[31m12.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from httpx->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (1.3.1)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch==2.1.0->-r assets/requirements/requirements.txt (line 1)) (1.3.0)\n",
            "Collecting dnspython>=2.0.0 (from email_validator>=2.0.0->fastapi->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading dnspython-2.6.1-py3-none-any.whl (307 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m307.7/307.7 kB\u001b[0m \u001b[31m39.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting typer>=0.12.3 (from fastapi-cli>=0.0.2->fastapi->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading typer-0.12.3-py3-none-any.whl (47 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m47.2/47.2 kB\u001b[0m \u001b[31m7.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->GitPython!=3.1.29,>=1.0.0->wandb==0.15.12->-r assets/requirements/requirements.txt (line 32))\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6.0,>=4.2.0->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (0.18.1)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio->httpx->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (1.2.1)\n",
            "Collecting httptools>=0.5.0 (from uvicorn>=0.14.0->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading httptools-0.6.1-cp310-cp310-manylinux_2_5_x86_64.manylinux1_x86_64.manylinux_2_17_x86_64.manylinux2014_x86_64.whl (341 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m341.4/341.4 kB\u001b[0m \u001b[31m40.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting python-dotenv>=0.13 (from uvicorn>=0.14.0->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Collecting uvloop!=0.15.0,!=0.15.1,>=0.14.0 (from uvicorn>=0.14.0->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading uvloop-0.19.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.4 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m97.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting watchfiles>=0.13 (from uvicorn>=0.14.0->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading watchfiles-0.22.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m63.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting shellingham>=1.3.0 (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34))\n",
            "  Downloading shellingham-1.5.4-py2.py3-none-any.whl (9.8 kB)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (13.7.1)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (2.16.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer>=0.12.3->fastapi-cli>=0.0.2->fastapi->gradio==3.42.0->-r assets/requirements/requirements.txt (line 34)) (0.1.2)\n",
            "Building wheels for collected packages: fvcore, mpi4py, diffdist, mup, deepspeed, iopath, ffmpy, pathtools, typing\n",
            "  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61400 sha256=cb7a176b9f676ebbfbe3ddf514f95bfd390155a377ef99e8034a6657354f6134\n",
            "  Stored in directory: /root/.cache/pip/wheels/01/c0/af/77c1cf53a1be9e42a52b48e5af2169d40ec2e89f7362489dd0\n",
            "  Building wheel for mpi4py (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mpi4py: filename=mpi4py-3.1.5-cp310-cp310-linux_x86_64.whl size=2746522 sha256=86f5838c2a56577d5f5f81ecde7c1fdfb9e8849532ca303540666bdf76a1a1db\n",
            "  Stored in directory: /root/.cache/pip/wheels/18/2b/7f/c852523089e9182b45fca50ff56f49a51eeb6284fd25a66713\n",
            "  Building wheel for diffdist (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for diffdist: filename=diffdist-0.1-py3-none-any.whl size=6536 sha256=38c830d8c527e8d8796c18e3eff97d3f27dc07c15c64f8fccfc0453a5265bbaa\n",
            "  Stored in directory: /root/.cache/pip/wheels/f1/29/f6/5331a14ab74e769799b76eb32460f447c7feb7375a16b07854\n",
            "  Building wheel for mup (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for mup: filename=mup-1.0.0-py3-none-any.whl size=23630 sha256=b5604f55dd97249790a9fa9fb82a175db92b8ce44dadb0444489a09100725041\n",
            "  Stored in directory: /root/.cache/pip/wheels/f4/c8/88/3c23a3d10c50053b6552d2d30aee5b53ba89a47f742420036c\n",
            "  Building wheel for deepspeed (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for deepspeed: filename=deepspeed-0.10.3-py3-none-any.whl size=907835 sha256=765f84eaf1f76f589a66b14e163aef72c155d185acda9768cf960b0ad7d887de\n",
            "  Stored in directory: /root/.cache/pip/wheels/d8/8e/fe/bd6467e058672bf39888e67b763f706053f6f969fe0542423d\n",
            "  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31532 sha256=7d46591c2a21ce8a25f6c85bf950c3f2629acc2f2e549a9c4fdd672350e3b58c\n",
            "  Stored in directory: /root/.cache/pip/wheels/9a/a3/b6/ac0fcd1b4ed5cfeb3db92e6a0e476cfd48ed0df92b91080c1d\n",
            "  Building wheel for ffmpy (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for ffmpy: filename=ffmpy-0.3.2-py3-none-any.whl size=5584 sha256=870c8f4acf1a90f4e44a14b462328ad12f2f0bfa078ac86603c9b85540408b29\n",
            "  Stored in directory: /root/.cache/pip/wheels/bd/65/9a/671fc6dcde07d4418df0c592f8df512b26d7a0029c2a23dd81\n",
            "  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pathtools: filename=pathtools-0.1.2-py3-none-any.whl size=8791 sha256=6f69f418f5d03e7e8be5d099823ac2c85ab0c38b656774a5b92b632f5d004592\n",
            "  Stored in directory: /root/.cache/pip/wheels/e7/f3/22/152153d6eb222ee7a56ff8617d80ee5207207a8c00a7aab794\n",
            "  Building wheel for typing (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for typing: filename=typing-3.7.4.3-py3-none-any.whl size=26306 sha256=5e0958f9a4bf0037a531d9a7a14808a838f4c783ddb13254035f9130efd229bf\n",
            "  Stored in directory: /root/.cache/pip/wheels/7c/d0/9e/1f26ebb66d9e1732e4098bc5a6c2d91f6c9a529838f0284890\n",
            "Successfully built fvcore mpi4py diffdist mup deepspeed iopath ffmpy pathtools typing\n",
            "Installing collected packages: pydub, pathtools, ninja, json_tricks, hjson, ffmpy, diffdist, appdirs, yacs, websockets, uvloop, ujson, typing, triton, smmap, shellingham, shapely, setproctitle, sentry-sdk, semantic-version, regex, python-multipart, python-dotenv, pydantic, portalocker, orjson, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, numpy, mpi4py, infinibatch, humanfriendly, httptools, h11, ftfy, einops, docker-pycreds, dnspython, cython, aiofiles, watchfiles, vision-datasets, uvicorn, starlette, pyquaternion, pyarrow, pandas, opencv-python, nvidia-cusparse-cu12, nvidia-cudnn-cu12, iopath, huggingface-hub, httpcore, gitdb, email_validator, coloredlogs, typer, tokenizers, scikit-learn, scikit-image, nvidia-cusolver-cu12, httpx, GitPython, fvcore, wandb, transformers, torch, gradio-client, fastapi-cli, cityscapesscripts, torchvision, pycocoevalcap, kornia, fastapi, deepspeed, accelerate, timm, mup, gradio\n",
            "  Attempting uninstall: triton\n",
            "    Found existing installation: triton 2.3.0\n",
            "    Uninstalling triton-2.3.0:\n",
            "      Successfully uninstalled triton-2.3.0\n",
            "  Attempting uninstall: shapely\n",
            "    Found existing installation: shapely 2.0.4\n",
            "    Uninstalling shapely-2.0.4:\n",
            "      Successfully uninstalled shapely-2.0.4\n",
            "  Attempting uninstall: regex\n",
            "    Found existing installation: regex 2024.5.15\n",
            "    Uninstalling regex-2024.5.15:\n",
            "      Successfully uninstalled regex-2024.5.15\n",
            "  Attempting uninstall: pydantic\n",
            "    Found existing installation: pydantic 2.7.3\n",
            "    Uninstalling pydantic-2.7.3:\n",
            "      Successfully uninstalled pydantic-2.7.3\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 1.25.2\n",
            "    Uninstalling numpy-1.25.2:\n",
            "      Successfully uninstalled numpy-1.25.2\n",
            "  Attempting uninstall: cython\n",
            "    Found existing installation: Cython 3.0.10\n",
            "    Uninstalling Cython-3.0.10:\n",
            "      Successfully uninstalled Cython-3.0.10\n",
            "  Attempting uninstall: pyarrow\n",
            "    Found existing installation: pyarrow 14.0.2\n",
            "    Uninstalling pyarrow-14.0.2:\n",
            "      Successfully uninstalled pyarrow-14.0.2\n",
            "  Attempting uninstall: pandas\n",
            "    Found existing installation: pandas 1.5.3\n",
            "    Uninstalling pandas-1.5.3:\n",
            "      Successfully uninstalled pandas-1.5.3\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.8.0.76\n",
            "    Uninstalling opencv-python-4.8.0.76:\n",
            "      Successfully uninstalled opencv-python-4.8.0.76\n",
            "  Attempting uninstall: huggingface-hub\n",
            "    Found existing installation: huggingface-hub 0.23.2\n",
            "    Uninstalling huggingface-hub-0.23.2:\n",
            "      Successfully uninstalled huggingface-hub-0.23.2\n",
            "  Attempting uninstall: typer\n",
            "    Found existing installation: typer 0.9.4\n",
            "    Uninstalling typer-0.9.4:\n",
            "      Successfully uninstalled typer-0.9.4\n",
            "  Attempting uninstall: tokenizers\n",
            "    Found existing installation: tokenizers 0.19.1\n",
            "    Uninstalling tokenizers-0.19.1:\n",
            "      Successfully uninstalled tokenizers-0.19.1\n",
            "  Attempting uninstall: scikit-learn\n",
            "    Found existing installation: scikit-learn 1.2.2\n",
            "    Uninstalling scikit-learn-1.2.2:\n",
            "      Successfully uninstalled scikit-learn-1.2.2\n",
            "  Attempting uninstall: scikit-image\n",
            "    Found existing installation: scikit-image 0.19.3\n",
            "    Uninstalling scikit-image-0.19.3:\n",
            "      Successfully uninstalled scikit-image-0.19.3\n",
            "  Attempting uninstall: transformers\n",
            "    Found existing installation: transformers 4.41.2\n",
            "    Uninstalling transformers-4.41.2:\n",
            "      Successfully uninstalled transformers-4.41.2\n",
            "  Attempting uninstall: torch\n",
            "    Found existing installation: torch 2.3.0+cu121\n",
            "    Uninstalling torch-2.3.0+cu121:\n",
            "      Successfully uninstalled torch-2.3.0+cu121\n",
            "  Attempting uninstall: torchvision\n",
            "    Found existing installation: torchvision 0.18.0+cu121\n",
            "    Uninstalling torchvision-0.18.0+cu121:\n",
            "      Successfully uninstalled torchvision-0.18.0+cu121\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "chex 0.1.86 requires numpy>=1.24.1, but you have numpy 1.23.1 which is incompatible.\n",
            "cudf-cu12 24.4.1 requires pyarrow<15.0.0a0,>=14.0.1, but you have pyarrow 13.0.0 which is incompatible.\n",
            "pandas-stubs 2.0.3.230814 requires numpy>=1.25.0; python_version >= \"3.9\", but you have numpy 1.23.1 which is incompatible.\n",
            "spacy 3.7.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\n",
            "tensorflow 2.15.0 requires numpy<2.0.0,>=1.23.5, but you have numpy 1.23.1 which is incompatible.\n",
            "torchaudio 2.3.0+cu121 requires torch==2.3.0, but you have torch 2.1.0 which is incompatible.\n",
            "torchtext 0.18.0 requires torch>=2.3.0, but you have torch 2.1.0 which is incompatible.\n",
            "weasel 0.3.4 requires typer<0.10.0,>=0.3.0, but you have typer 0.12.3 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed GitPython-3.1.43 accelerate-0.23.0 aiofiles-23.2.1 appdirs-1.4.4 cityscapesscripts-2.2.2 coloredlogs-15.0.1 cython-3.0.2 deepspeed-0.10.3 diffdist-0.1 dnspython-2.6.1 docker-pycreds-0.4.0 einops-0.7.0 email_validator-2.1.1 fastapi-0.111.0 fastapi-cli-0.0.4 ffmpy-0.3.2 ftfy-6.1.1 fvcore-0.1.5.post20221221 gitdb-4.0.11 gradio-3.42.0 gradio-client-0.5.0 h11-0.14.0 hjson-3.1.0 httpcore-1.0.5 httptools-0.6.1 httpx-0.27.0 huggingface-hub-0.17.3 humanfriendly-10.0 infinibatch-0.1.1 iopath-0.1.10 json_tricks-3.17.3 kornia-0.7.0 mpi4py-3.1.5 mup-1.0.0 ninja-1.11.1.1 numpy-1.23.1 nvidia-cublas-cu12-12.1.3.1 nvidia-cuda-cupti-cu12-12.1.105 nvidia-cuda-nvrtc-cu12-12.1.105 nvidia-cuda-runtime-cu12-12.1.105 nvidia-cudnn-cu12-8.9.2.26 nvidia-cufft-cu12-11.0.2.54 nvidia-curand-cu12-10.3.2.106 nvidia-cusolver-cu12-11.4.5.107 nvidia-cusparse-cu12-12.1.0.106 nvidia-nccl-cu12-2.18.1 nvidia-nvjitlink-cu12-12.5.40 nvidia-nvtx-cu12-12.1.105 opencv-python-4.8.1.78 orjson-3.10.4 pandas-2.0.3 pathtools-0.1.2 portalocker-2.8.2 pyarrow-13.0.0 pycocoevalcap-1.2 pydantic-1.10.15 pydub-0.25.1 pyquaternion-0.9.9 python-dotenv-1.0.1 python-multipart-0.0.9 regex-2023.10.3 scikit-image-0.21.0 scikit-learn-1.3.1 semantic-version-2.10.0 sentry-sdk-2.5.1 setproctitle-1.3.3 shapely-1.8.0 shellingham-1.5.4 smmap-5.0.1 starlette-0.37.2 timm-0.4.12 tokenizers-0.14.1 torch-2.1.0 torchvision-0.16.0 transformers-4.34.0 triton-2.1.0 typer-0.12.3 typing-3.7.4.3 ujson-5.10.0 uvicorn-0.30.1 uvloop-0.19.0 vision-datasets-0.2.2 wandb-0.15.12 watchfiles-0.22.0 websockets-11.0.3 yacs-0.1.8\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "numpy",
                  "typing"
                ]
              },
              "id": "6b5bce6137f1491d9062ea49dd99c6f0"
            }
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting git+https://github.com/arogozhnikov/einops.git (from -r assets/requirements/requirements_custom.txt (line 1))\n",
            "  Cloning https://github.com/arogozhnikov/einops.git to /tmp/pip-req-build-hqu8tnxm\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/arogozhnikov/einops.git /tmp/pip-req-build-hqu8tnxm\n",
            "  Resolved https://github.com/arogozhnikov/einops.git to commit 6181e1e95dc58c00a3143c1726da1c6ee0463164\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/MaureenZOU/detectron2-xyz.git (from -r assets/requirements/requirements_custom.txt (line 2))\n",
            "  Cloning https://github.com/MaureenZOU/detectron2-xyz.git to /tmp/pip-req-build-mnbm4yrv\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/MaureenZOU/detectron2-xyz.git /tmp/pip-req-build-mnbm4yrv\n",
            "  Resolved https://github.com/MaureenZOU/detectron2-xyz.git to commit 42121d75e10d9f858f3a91b6a39f5722c02868f0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/openai/whisper.git (from -r assets/requirements/requirements_custom.txt (line 3))\n",
            "  Cloning https://github.com/openai/whisper.git to /tmp/pip-req-build-hbgaz82d\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/openai/whisper.git /tmp/pip-req-build-hbgaz82d\n",
            "  Resolved https://github.com/openai/whisper.git to commit ba3f3cd54b0e5b8ce1ab3de13e32122d0d5f98ab\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting git+https://github.com/cocodataset/panopticapi.git (from -r assets/requirements/requirements_custom.txt (line 4))\n",
            "  Cloning https://github.com/cocodataset/panopticapi.git to /tmp/pip-req-build-hr8obm43\n",
            "  Running command git clone --filter=blob:none --quiet https://github.com/cocodataset/panopticapi.git /tmp/pip-req-build-hr8obm43\n",
            "  Resolved https://github.com/cocodataset/panopticapi.git to commit 7bb4655548f98f3fedc07bf37e9040a992b054b0\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: Pillow>=7.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (9.4.0)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (3.7.1)\n",
            "Requirement already satisfied: pycocotools>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (2.4.0)\n",
            "Requirement already satisfied: yacs>=0.1.8 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (0.1.8)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (0.9.0)\n",
            "Requirement already satisfied: cloudpickle in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (2.2.1)\n",
            "Requirement already satisfied: tqdm>4.29.0 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (4.66.4)\n",
            "Requirement already satisfied: tensorboard in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (2.15.2)\n",
            "Requirement already satisfied: fvcore<0.1.6,>=0.1.5 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (0.1.5.post20221221)\n",
            "Collecting iopath<0.1.10,>=0.1.7 (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2))\n",
            "  Downloading iopath-0.1.9-py3-none-any.whl (27 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (0.18.3)\n",
            "Requirement already satisfied: pydot in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (1.4.2)\n",
            "Collecting omegaconf>=2.1 (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2))\n",
            "  Downloading omegaconf-2.3.0-py3-none-any.whl (79 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m79.5/79.5 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting hydra-core>=1.1 (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2))\n",
            "  Downloading hydra_core-1.3.2-py3-none-any.whl (154 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.5/154.5 kB\u001b[0m \u001b[31m6.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting black==21.4b2 (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2))\n",
            "  Downloading black-21.4b2-py3-none-any.whl (130 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m131.0/131.0 kB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: scipy>1.5.1 in /usr/local/lib/python3.10/dist-packages (from detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (1.11.4)\n",
            "Requirement already satisfied: click>=7.1.2 in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (8.1.7)\n",
            "Requirement already satisfied: appdirs in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (1.4.4)\n",
            "Requirement already satisfied: toml>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (0.10.2)\n",
            "Requirement already satisfied: regex>=2020.1.8 in /usr/local/lib/python3.10/dist-packages (from black==21.4b2->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (2023.10.3)\n",
            "Collecting pathspec<1,>=0.8.1 (from black==21.4b2->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2))\n",
            "  Using cached pathspec-0.12.1-py3-none-any.whl (31 kB)\n",
            "Collecting mypy-extensions>=0.4.3 (from black==21.4b2->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2))\n",
            "  Downloading mypy_extensions-1.0.0-py3-none-any.whl (4.7 kB)\n",
            "Requirement already satisfied: numba in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (0.58.1)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (1.23.1)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (2.1.0)\n",
            "Requirement already satisfied: more-itertools in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (10.1.0)\n",
            "Collecting tiktoken (from openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3))\n",
            "  Downloading tiktoken-0.7.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: triton<3,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (2.1.0)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.10/dist-packages (from fvcore<0.1.6,>=0.1.5->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (6.0.1)\n",
            "Collecting antlr4-python3-runtime==4.9.* (from hydra-core>=1.1->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2))\n",
            "  Downloading antlr4-python3-runtime-4.9.3.tar.gz (117 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m117.0/117.0 kB\u001b[0m \u001b[31m10.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.10/dist-packages (from hydra-core>=1.1->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (24.0)\n",
            "Requirement already satisfied: portalocker in /usr/local/lib/python3.10/dist-packages (from iopath<0.1.10,>=0.1.7->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (1.2.1)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (4.53.0)\n",
            "Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (1.4.5)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (3.1.2)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.10/dist-packages (from matplotlib->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (2.8.2)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.10/dist-packages (from triton<3,>=2.0.0->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (3.14.0)\n",
            "Requirement already satisfied: llvmlite<0.42,>=0.41.0dev0 in /usr/local/lib/python3.10/dist-packages (from numba->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (0.41.1)\n",
            "Requirement already satisfied: absl-py>=0.4 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (1.4.0)\n",
            "Requirement already satisfied: grpcio>=1.48.2 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (1.64.1)\n",
            "Requirement already satisfied: google-auth<3,>=1.6.3 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (2.27.0)\n",
            "Requirement already satisfied: google-auth-oauthlib<2,>=0.5 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (1.2.0)\n",
            "Requirement already satisfied: markdown>=2.6.8 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (3.6)\n",
            "Requirement already satisfied: protobuf!=4.24.0,>=3.19.6 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (3.20.3)\n",
            "Requirement already satisfied: requests<3,>=2.21.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (2.31.0)\n",
            "Requirement already satisfied: setuptools>=41.0.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (67.7.2)\n",
            "Requirement already satisfied: six>1.9 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (1.16.0)\n",
            "Requirement already satisfied: tensorboard-data-server<0.8.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (0.7.2)\n",
            "Requirement already satisfied: werkzeug>=1.0.1 in /usr/local/lib/python3.10/dist-packages (from tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (3.0.3)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (4.12.1)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (1.12.1)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (3.3)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (3.1.4)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (2023.6.0)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==8.9.2.26 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (8.9.2.26)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.1.3.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (12.1.3.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.0.2.54 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (11.0.2.54)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.2.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (10.3.2.106)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.4.5.107 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (11.4.5.107)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.1.0.106 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (12.1.0.106)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.18.1 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (2.18.1)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.1.105 in /usr/local/lib/python3.10/dist-packages (from torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (12.1.105)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12 in /usr/local/lib/python3.10/dist-packages (from nvidia-cusolver-cu12==11.4.5.107->torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (12.5.40)\n",
            "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (5.3.3)\n",
            "Requirement already satisfied: pyasn1-modules>=0.2.1 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (0.4.0)\n",
            "Requirement already satisfied: rsa<5,>=3.1.4 in /usr/local/lib/python3.10/dist-packages (from google-auth<3,>=1.6.3->tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (4.9)\n",
            "Requirement already satisfied: requests-oauthlib>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (1.3.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.21.0->tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (2024.6.2)\n",
            "Requirement already satisfied: MarkupSafe>=2.1.1 in /usr/local/lib/python3.10/dist-packages (from werkzeug>=1.0.1->tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (2.1.5)\n",
            "Requirement already satisfied: mpmath<1.4.0,>=1.1.0 in /usr/local/lib/python3.10/dist-packages (from sympy->torch->openai-whisper==20231117->-r assets/requirements/requirements_custom.txt (line 3)) (1.3.0)\n",
            "Requirement already satisfied: pyasn1<0.7.0,>=0.4.6 in /usr/local/lib/python3.10/dist-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (0.6.0)\n",
            "Requirement already satisfied: oauthlib>=3.0.0 in /usr/local/lib/python3.10/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<2,>=0.5->tensorboard->detectron2==0.6->-r assets/requirements/requirements_custom.txt (line 2)) (3.2.2)\n",
            "Building wheels for collected packages: einops, detectron2, openai-whisper, panopticapi, antlr4-python3-runtime\n",
            "  Building wheel for einops (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for einops: filename=einops-0.8.0-py3-none-any.whl size=43239 sha256=cd57f3b757de254152939f1ae85aff4f1c54f0287cb32fe5d3337427634b8634\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qk5s7ov_/wheels/6e/81/97/257bfc800cca55f48e13dfa13e35b10e805825ee9fa2cdca9b\n",
            "  Building wheel for detectron2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for detectron2: filename=detectron2-0.6-cp310-cp310-linux_x86_64.whl size=6111564 sha256=6c6f00f370ebf02016190be85c3f212004a38f4ea83c5797a278cb440a9274ae\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qk5s7ov_/wheels/7f/01/26/a8173423e3f7366079469ba614da45c1a875b4cd72bc269568\n",
            "  Building wheel for openai-whisper (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for openai-whisper: filename=openai_whisper-20231117-py3-none-any.whl size=802826 sha256=0c3178c45f9a34c08f10631b64d43aa78ceefb805db5f35e138c90b9d62109e9\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qk5s7ov_/wheels/8b/6c/d0/622666868c179f156cf595c8b6f06f88bc5d80c4b31dccaa03\n",
            "  Building wheel for panopticapi (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for panopticapi: filename=panopticapi-0.1-py3-none-any.whl size=8260 sha256=acf305b5353530dc478189bdb339895551bf7c7744c22dfb819a2f1f004f4c70\n",
            "  Stored in directory: /tmp/pip-ephem-wheel-cache-qk5s7ov_/wheels/70/87/ae/5c2b138c967549070e3fe35f3b5fcaf1ed56e9f5483a09ee65\n",
            "  Building wheel for antlr4-python3-runtime (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for antlr4-python3-runtime: filename=antlr4_python3_runtime-4.9.3-py3-none-any.whl size=144554 sha256=90eb1bb6ca31e3a5eb0650719f402ff33dc111df09ef8c199ae8545914e39697\n",
            "  Stored in directory: /root/.cache/pip/wheels/12/93/dd/1f6a127edc45659556564c5730f6d4e300888f4bca2d4c5a88\n",
            "Successfully built einops detectron2 openai-whisper panopticapi antlr4-python3-runtime\n",
            "Installing collected packages: antlr4-python3-runtime, pathspec, panopticapi, omegaconf, mypy-extensions, iopath, einops, tiktoken, hydra-core, black, openai-whisper, detectron2\n",
            "  Attempting uninstall: iopath\n",
            "    Found existing installation: iopath 0.1.10\n",
            "    Uninstalling iopath-0.1.10:\n",
            "      Successfully uninstalled iopath-0.1.10\n",
            "  Attempting uninstall: einops\n",
            "    Found existing installation: einops 0.7.0\n",
            "    Uninstalling einops-0.7.0:\n",
            "      Successfully uninstalled einops-0.7.0\n",
            "Successfully installed antlr4-python3-runtime-4.9.3 black-21.4b2 detectron2-0.6 einops-0.8.0 hydra-core-1.3.2 iopath-0.1.9 mypy-extensions-1.0.0 omegaconf-2.3.0 openai-whisper-20231117 panopticapi-0.1 pathspec-0.12.1 tiktoken-0.7.0\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pydevd_plugins"
                ]
              },
              "id": "3845078d9e7c42a8816f97d5dba42c36"
            }
          },
          "metadata": {}
        }
      ],
      "source": [
        "!pip install -r assets/requirements/requirements.txt\n",
        "!pip install -r assets/requirements/requirements_custom.txt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WJFCY8Yq6SEs",
        "outputId": "07bc8dc9-df7f-4824-d272-6f4e1dd01bd6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-11 18:17:19--  https://huggingface.co/xdecoder/SEEM/resolve/main/seem_focall_v1.pt\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.37, 3.163.189.90, 3.163.189.114, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.37|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/07/6f/076f8f2ff8feca33440379b3deba18d63a0d6335efdcf41f4ca5f06ecf7e0508/06cad58bde442ce4f2b3ce00e3a218791dfb060a2d7f2d709ff509669c28a705?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27seem_focall_v1.pt%3B+filename%3D%22seem_focall_v1.pt%22%3B&Expires=1718389039&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODM4OTAzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8wNy82Zi8wNzZmOGYyZmY4ZmVjYTMzNDQwMzc5YjNkZWJhMThkNjNhMGQ2MzM1ZWZkY2Y0MWY0Y2E1ZjA2ZWNmN2UwNTA4LzA2Y2FkNThiZGU0NDJjZTRmMmIzY2UwMGUzYTIxODc5MWRmYjA2MGEyZDdmMmQ3MDlmZjUwOTY2OWMyOGE3MDU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=l%7E051s6KWCZZHmavf8OscyBonvPlsFkjFwBArG%7EEmSmW7WrGJt9o8D16OyQos-T02Qf6yfQ6thTXaG7HZu0Xp0tBPf4kkTH2vjM0EAkJ4k1mV3Bgx-7yq5IQDCHTE9Rgtz6HLYqW-uE05yzRLsKBLBVBr0UDpEPLZOl-atGuZxPXiJBXpkmk51Trdlh6q96N0ljOVDtgJxo6Xe4Dq%7Ex9bjNuScrrit7V8wnPRcTHOmzjzX%7EEBCPE7ZaXgi85ZAkc4DSOFHsQeeNr-ogWwf%7E2ZXLi5vTVHi-9wiYNvGSXADMMKRu4OUB%7EuELq%7E3cmLWMiiLM0kJKq5eHRGOJ9l2xKbA__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-06-11 18:17:19--  https://cdn-lfs.huggingface.co/repos/07/6f/076f8f2ff8feca33440379b3deba18d63a0d6335efdcf41f4ca5f06ecf7e0508/06cad58bde442ce4f2b3ce00e3a218791dfb060a2d7f2d709ff509669c28a705?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27seem_focall_v1.pt%3B+filename%3D%22seem_focall_v1.pt%22%3B&Expires=1718389039&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODM4OTAzOX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8wNy82Zi8wNzZmOGYyZmY4ZmVjYTMzNDQwMzc5YjNkZWJhMThkNjNhMGQ2MzM1ZWZkY2Y0MWY0Y2E1ZjA2ZWNmN2UwNTA4LzA2Y2FkNThiZGU0NDJjZTRmMmIzY2UwMGUzYTIxODc5MWRmYjA2MGEyZDdmMmQ3MDlmZjUwOTY2OWMyOGE3MDU%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=l%7E051s6KWCZZHmavf8OscyBonvPlsFkjFwBArG%7EEmSmW7WrGJt9o8D16OyQos-T02Qf6yfQ6thTXaG7HZu0Xp0tBPf4kkTH2vjM0EAkJ4k1mV3Bgx-7yq5IQDCHTE9Rgtz6HLYqW-uE05yzRLsKBLBVBr0UDpEPLZOl-atGuZxPXiJBXpkmk51Trdlh6q96N0ljOVDtgJxo6Xe4Dq%7Ex9bjNuScrrit7V8wnPRcTHOmzjzX%7EEBCPE7ZaXgi85ZAkc4DSOFHsQeeNr-ogWwf%7E2ZXLi5vTVHi-9wiYNvGSXADMMKRu4OUB%7EuELq%7E3cmLWMiiLM0kJKq5eHRGOJ9l2xKbA__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.25, 108.138.94.23, 108.138.94.122, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 1365136278 (1.3G) [binary/octet-stream]\n",
            "Saving to: ‘pretrained_w.pt’\n",
            "\n",
            "pretrained_w.pt     100%[===================>]   1.27G  20.2MB/s    in 49s     \n",
            "\n",
            "2024-06-11 18:18:08 (26.6 MB/s) - ‘pretrained_w.pt’ saved [1365136278/1365136278]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# !wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/xdecoder_focalt_last.pt\n",
        "\n",
        "\n",
        "if focal == 1:\n",
        "  if focalt == 0:\n",
        "    !wget -O pretrained_w.pt https://huggingface.co/xdecoder/SEEM/resolve/main/seem_focall_v1.pt\n",
        "  else:\n",
        "    !wget -O pretrained_w.pt https://huggingface.co/xdecoder/SEEM/resolve/main/seem_focalt_v1.pt\n",
        "else:\n",
        "  !wget -O pretrained_w.pth https://dl.fbaipublicfiles.com/segment_anything/sam_vit_b_01ec64.pth\n",
        "  !wget -O base_w.pt https://huggingface.co/xdecoder/X-Decoder/resolve/main/focalb_lang_unicl.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "alr1gps7uVHr"
      },
      "source": [
        "Train Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O6G2Wb3BuH7R",
        "outputId": "a1f479fb-066d-4816-eace-ee0a7aa61516"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-11 18:18:09--  https://huggingface.co/xdecoder/X-Decoder/resolve/main/caption_class_similarity.pth\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.74, 3.163.189.114, 3.163.189.90, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/03/e9/03e9fb2702ff3bee67568bdcfa721f4f21b0a9fce27a6f74c8ec5581050844a1/0d648c92c200906616cd64339f01bf692c93dd950bad96aa33f42c007a3cf57a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27caption_class_similarity.pth%3B+filename%3D%22caption_class_similarity.pth%22%3B&Expires=1718389089&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODM4OTA4OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8wMy9lOS8wM2U5ZmIyNzAyZmYzYmVlNjc1NjhiZGNmYTcyMWY0ZjIxYjBhOWZjZTI3YTZmNzRjOGVjNTU4MTA1MDg0NGExLzBkNjQ4YzkyYzIwMDkwNjYxNmNkNjQzMzlmMDFiZjY5MmM5M2RkOTUwYmFkOTZhYTMzZjQyYzAwN2EzY2Y1N2E%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=FJlAnuXd2qihL-eDzmXdkA3YN1rGXs4Ln0ww9dM9owQjcKzZNsjf0c9H-X2j-86QZiSFDc6tOtq7G9-33F7UEN9XBYgj7yBqlDnNmNw8QvfOrV9-OdgPD8viUQ5BNLZ5RtMkTTcFGDIvrpBmEEFZnE93mpSjlGgXIw0xbNEg3i0slU6xlwjDyAGgzpBbzCXxz4xANhDpZzMaIErMVMUrk%7EbzIINn-qHW1HfOyVyXSVroLVOPUUOypV55mLOSiee0M5NYnM-Fb4dU%7EhyTw-6YZs6Doz36Q4FH3wzSeNrwpQvyyzzdzDqoM5qTB-t%7EQkyWlAHHE%7EKETazv0dX28k8vyQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-06-11 18:18:09--  https://cdn-lfs.huggingface.co/repos/03/e9/03e9fb2702ff3bee67568bdcfa721f4f21b0a9fce27a6f74c8ec5581050844a1/0d648c92c200906616cd64339f01bf692c93dd950bad96aa33f42c007a3cf57a?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27caption_class_similarity.pth%3B+filename%3D%22caption_class_similarity.pth%22%3B&Expires=1718389089&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODM4OTA4OX19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8wMy9lOS8wM2U5ZmIyNzAyZmYzYmVlNjc1NjhiZGNmYTcyMWY0ZjIxYjBhOWZjZTI3YTZmNzRjOGVjNTU4MTA1MDg0NGExLzBkNjQ4YzkyYzIwMDkwNjYxNmNkNjQzMzlmMDFiZjY5MmM5M2RkOTUwYmFkOTZhYTMzZjQyYzAwN2EzY2Y1N2E%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qIn1dfQ__&Signature=FJlAnuXd2qihL-eDzmXdkA3YN1rGXs4Ln0ww9dM9owQjcKzZNsjf0c9H-X2j-86QZiSFDc6tOtq7G9-33F7UEN9XBYgj7yBqlDnNmNw8QvfOrV9-OdgPD8viUQ5BNLZ5RtMkTTcFGDIvrpBmEEFZnE93mpSjlGgXIw0xbNEg3i0slU6xlwjDyAGgzpBbzCXxz4xANhDpZzMaIErMVMUrk%7EbzIINn-qHW1HfOyVyXSVroLVOPUUOypV55mLOSiee0M5NYnM-Fb4dU%7EhyTw-6YZs6Doz36Q4FH3wzSeNrwpQvyyzzdzDqoM5qTB-t%7EQkyWlAHHE%7EKETazv0dX28k8vyQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.25, 108.138.94.122, 108.138.94.23, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 912303 (891K) [binary/octet-stream]\n",
            "Saving to: ‘caption_class_similarity.pth’\n",
            "\n",
            "caption_class_simil 100%[===================>] 890.92K  2.71MB/s    in 0.3s    \n",
            "\n",
            "2024-06-11 18:18:09 (2.71 MB/s) - ‘caption_class_similarity.pth’ saved [912303/912303]\n",
            "\n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/caption_class_similarity.pth\n",
        "# !wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/captions_train2017_filtrefgumdval_filtvlp.json\n",
        "# !wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/grounding_train2017_filtrefgumdval_filtvlp.json\n",
        "# !wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/panoptic_train2017_filtrefgumdval_filtvlp.json\n",
        "# !wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/refcocog_umd_val.json\n",
        "# !wget https://github.com/peteanderson80/coco-caption/blob/master/annotations/captions_val2014.json\n",
        "\n",
        "# # (SEEM) Download LVIS annotations for mask preparation\n",
        "# !wget https://huggingface.co/xdecoder/SEEM/resolve/main/coco_train2017_filtrefgumdval_lvis.json\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jnka1W-juTUT"
      },
      "outputs": [],
      "source": [
        "from pathlib import Path\n",
        "Path(\"datasets/xdecoder_data\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"datasets/xdecoder_data/lvis\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"datasets/xdecoder_data/coco\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"datasets/xdecoder_data/coco/val2017\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"datasets/xdecoder_data/coco/panoptic_train2017\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"datasets/xdecoder_data/coco/panoptic_semseg_train2017\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"datasets/xdecoder_data/coco/panoptic_val2017\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"datasets/xdecoder_data/coco/panoptic_semseg_val2017\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"datasets/xdecoder_data/coco/annotations\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"datasets/xdecoder_data/pretrained\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"data/output/test\").mkdir(parents=True, exist_ok=True)\n",
        "Path(\"/content/imgs\").mkdir(parents=True, exist_ok=True)\n",
        "!mv caption_class_similarity.pth /content/datasets/xdecoder_data/coco/annotations\n",
        "\n",
        "# !mv captions_train2017_filtrefgumdval_filtvlp.json /content/datasets/xdecoder_data/coco/annotations\n",
        "# !mv grounding_train2017_filtrefgumdval_filtvlp.json /content/datasets/xdecoder_data/coco/annotations\n",
        "# !mv panoptic_train2017_filtrefgumdval_filtvlp.json /content/datasets/xdecoder_data/coco/annotations\n",
        "# !mv refcocog_umd_val.json /content/datasets/xdecoder_data/coco/annotations\n",
        "# !mv captions_val2014.json /content/datasets/xdecoder_data/coco/annotations\n",
        "# !mv coco_train2017_filtrefgumdval_lvis.json /content/datasets/xdecoder_data/lvis\n",
        "\n",
        "# !mv xdecoder_focalt_last.pt /content/datasets/xdecoder_data/pretrained\n",
        "if focal == 1:\n",
        "  !mv pretrained_w.pt /content/datasets/xdecoder_data/pretrained\n",
        "else:\n",
        "  !mv pretrained_w.pth /content/datasets/xdecoder_data/pretrained\n",
        "  !mv base_w.pt /content/datasets/xdecoder_data/pretrained\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "gTreboYLkFqK",
        "outputId": "eaafff80-3180-40b6-c5a4-610e5b14eb7c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "mv: cannot stat 'coco_train2017_lvis.json': No such file or directory\n",
            "mv: cannot stat 'coco_panoptic_train2017.json': No such file or directory\n",
            "mv: cannot stat 'coco_captions_train2017.json': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!mv coco_train2017_lvis.json /content/datasets/xdecoder_data/coco/annotations\n",
        "!mv coco_panoptic_train2017.json /content/datasets/xdecoder_data/coco/annotations\n",
        "!mv coco_captions_train2017.json /content/datasets/xdecoder_data/coco/annotations"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IdkHaKe-9vXL",
        "outputId": "fbe2fa96-325b-4abd-fd73-11d1092fb5ac"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "--2024-06-11 18:18:10--  https://huggingface.co/xdecoder/X-Decoder/resolve/main/coco_caption.zip\n",
            "Resolving huggingface.co (huggingface.co)... 3.163.189.74, 3.163.189.114, 3.163.189.90, ...\n",
            "Connecting to huggingface.co (huggingface.co)|3.163.189.74|:443... connected.\n",
            "HTTP request sent, awaiting response... 302 Found\n",
            "Location: https://cdn-lfs.huggingface.co/repos/03/e9/03e9fb2702ff3bee67568bdcfa721f4f21b0a9fce27a6f74c8ec5581050844a1/12b38ca2d6f7cfb3920f37c0037e49ae3b59e2dd7d017270b80429051997b259?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27coco_caption.zip%3B+filename%3D%22coco_caption.zip%22%3B&response-content-type=application%2Fzip&Expires=1718389090&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODM4OTA5MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8wMy9lOS8wM2U5ZmIyNzAyZmYzYmVlNjc1NjhiZGNmYTcyMWY0ZjIxYjBhOWZjZTI3YTZmNzRjOGVjNTU4MTA1MDg0NGExLzEyYjM4Y2EyZDZmN2NmYjM5MjBmMzdjMDAzN2U0OWFlM2I1OWUyZGQ3ZDAxNzI3MGI4MDQyOTA1MTk5N2IyNTk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=nWR3yW%7EDNzRzNLkfvNVn0hMlze24bCfIEIPbimq1jd7tH5In2e5cTDtFRtMkdwbovEPiW1epHkVMJ38EKLBCMyp85O-E%7Eemwmhnamo28swXYq2rrN5R0w0gWVWiEza%7EWFRW6EujRuGstSA1Xj6ugUGKPao9Enb41uxKKxky5HdorSglZHbZpgpMjXVfFQaW56qAAONW9lzFkjcVNkF9AH5TxOIOSY7vIHZ2xziyHRJ8JuGvk8z%7EWggOylW%7EnX1sW3OCPhI%7Euq5zimDyShwfXJwmP-cRDwQ8aLjZCkWU03oYhw0ioHrlzeg8oMu54w-4CdKjqrjRHNjsoruliTWwHIQ__&Key-Pair-Id=KVTP0A1DKRTAX [following]\n",
            "--2024-06-11 18:18:10--  https://cdn-lfs.huggingface.co/repos/03/e9/03e9fb2702ff3bee67568bdcfa721f4f21b0a9fce27a6f74c8ec5581050844a1/12b38ca2d6f7cfb3920f37c0037e49ae3b59e2dd7d017270b80429051997b259?response-content-disposition=inline%3B+filename*%3DUTF-8%27%27coco_caption.zip%3B+filename%3D%22coco_caption.zip%22%3B&response-content-type=application%2Fzip&Expires=1718389090&Policy=eyJTdGF0ZW1lbnQiOlt7IkNvbmRpdGlvbiI6eyJEYXRlTGVzc1RoYW4iOnsiQVdTOkVwb2NoVGltZSI6MTcxODM4OTA5MH19LCJSZXNvdXJjZSI6Imh0dHBzOi8vY2RuLWxmcy5odWdnaW5nZmFjZS5jby9yZXBvcy8wMy9lOS8wM2U5ZmIyNzAyZmYzYmVlNjc1NjhiZGNmYTcyMWY0ZjIxYjBhOWZjZTI3YTZmNzRjOGVjNTU4MTA1MDg0NGExLzEyYjM4Y2EyZDZmN2NmYjM5MjBmMzdjMDAzN2U0OWFlM2I1OWUyZGQ3ZDAxNzI3MGI4MDQyOTA1MTk5N2IyNTk%7EcmVzcG9uc2UtY29udGVudC1kaXNwb3NpdGlvbj0qJnJlc3BvbnNlLWNvbnRlbnQtdHlwZT0qIn1dfQ__&Signature=nWR3yW%7EDNzRzNLkfvNVn0hMlze24bCfIEIPbimq1jd7tH5In2e5cTDtFRtMkdwbovEPiW1epHkVMJ38EKLBCMyp85O-E%7Eemwmhnamo28swXYq2rrN5R0w0gWVWiEza%7EWFRW6EujRuGstSA1Xj6ugUGKPao9Enb41uxKKxky5HdorSglZHbZpgpMjXVfFQaW56qAAONW9lzFkjcVNkF9AH5TxOIOSY7vIHZ2xziyHRJ8JuGvk8z%7EWggOylW%7EnX1sW3OCPhI%7Euq5zimDyShwfXJwmP-cRDwQ8aLjZCkWU03oYhw0ioHrlzeg8oMu54w-4CdKjqrjRHNjsoruliTWwHIQ__&Key-Pair-Id=KVTP0A1DKRTAX\n",
            "Resolving cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)... 108.138.94.25, 108.138.94.122, 108.138.94.23, ...\n",
            "Connecting to cdn-lfs.huggingface.co (cdn-lfs.huggingface.co)|108.138.94.25|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 753259011 (718M) [application/zip]\n",
            "Saving to: ‘coco_caption.zip’\n",
            "\n",
            "coco_caption.zip    100%[===================>] 718.36M  39.5MB/s    in 19s     \n",
            "\n",
            "2024-06-11 18:18:29 (38.7 MB/s) - ‘coco_caption.zip’ saved [753259011/753259011]\n",
            "\n",
            "Archive:  coco_caption.zip\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.eval.py.un~  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/config  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/description  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/.git/HEAD  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/hooks/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/hooks/applypatch-msg.sample  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/hooks/commit-msg.sample  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/hooks/fsmonitor-watchman.sample  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/hooks/post-update.sample  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/hooks/pre-applypatch.sample  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/hooks/pre-commit.sample  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/hooks/pre-push.sample  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/hooks/pre-rebase.sample  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/hooks/pre-receive.sample  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/hooks/prepare-commit-msg.sample  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/hooks/update.sample  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/index  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/info/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/info/exclude  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/logs/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/logs/HEAD  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/logs/refs/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/logs/refs/heads/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/logs/refs/heads/master  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/logs/refs/remotes/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/logs/refs/remotes/origin/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/logs/refs/remotes/origin/HEAD  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/objects/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/objects/pack/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/objects/pack/pack-e637da7b1f5e310a02ec71bd5aebdee5ab7d5cfb.idx  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/.git/objects/pack/pack-e637da7b1f5e310a02ec71bd5aebdee5ab7d5cfb.pack  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/.git/packed-refs  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/refs/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/refs/heads/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/.git/refs/heads/master  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/refs/remotes/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/.git/refs/remotes/origin/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/.git/refs/remotes/origin/HEAD  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/.gitignore  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/annotations/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/annotations/captions_val2014.json  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/annotations/caption_flickr30k.json  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/caption_pycocotools/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/caption_pycocotools/.coco.py.un~  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/caption_pycocotools/coco.py  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/caption_pycocotools/__init__.py  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/caption_pycocotools/__pycache__/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/caption_pycocotools/__pycache__/coco.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/caption_pycocotools/__pycache__/coco.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/caption_pycocotools/__pycache__/coco.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/caption_pycocotools/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/caption_pycocotools/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/caption_pycocotools/__pycache__/__init__.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/cocoEvalCapDemo.ipynb  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/eval.py  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/get_stanford_models.sh  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/ControlPanel  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/java  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/javaws  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/jcontrol  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/jjs  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/keytool  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/orbd  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/pack200  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/policytool  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/rmid  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/rmiregistry  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/servertool  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/tnameserv  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin/unpack200  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/COPYRIGHT  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/javafx/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/javafx/directshow.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/javafx/glib.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/javafx/gstreamer.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/javafx/icu_web.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/javafx/jpeg_fx.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/javafx/libffi.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/javafx/libxml2.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/javafx/libxslt.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/javafx/mesa3d.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/javafx/public_suffix.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/javafx/webkit.md  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/asm.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/bcel.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/cldr.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/colorimaging.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/cryptix.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/dom.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/dynalink.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/ecc.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/freebxml.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/giflib.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/icu.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/jcup.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/joni.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/jopt-simple.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/jpeg.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/lcms.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/libpng.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/libxrandr.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/mesa3d.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/pcsclite.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/pkcs11cryptotoken.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/pkcs11wrapper.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/relaxngcc.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/relaxngdatatype.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/relaxngom.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/santuario.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/thaidict.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/unicode.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/xalan.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/xerces.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/xmlresolver.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/xorgproto.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/xwd.md  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/legal/jdk/zlib.md  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/jli/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/jli/libjli.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/jvm.cfg  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libavplugin-53.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libavplugin-54.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libavplugin-55.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libavplugin-56.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libavplugin-57.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libavplugin-ffmpeg-56.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libavplugin-ffmpeg-57.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libavplugin-ffmpeg-58.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libawt.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libawt_headless.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libawt_xawt.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libbci.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libdcpr.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libdecora_sse.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libdeploy.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libdt_socket.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libfontmanager.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libfxplugins.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libglass.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libglassgtk2.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libglassgtk3.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libglib-lite.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libgstreamer-lite.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libhprof.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libinstrument.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libj2gss.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libj2pcsc.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libj2pkcs11.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjaas_unix.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjava.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjavafx_font.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjavafx_font_freetype.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjavafx_font_pango.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjavafx_iio.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjava_crw_demo.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjawt.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjdwp.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjfr.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjfxmedia.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjfxwebkit.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjpeg.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjsdt.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjsig.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjsound.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libjsoundalsa.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/liblcms.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libmanagement.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libmlib_image.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libnet.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libnio.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libnpt.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libprism_common.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libprism_es2.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libprism_sw.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libresource.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libsctp.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libsplashscreen.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libsunec.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libt2k.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libunpack.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libverify.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/libzip.so  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/server/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/server/libjsig.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/server/libjvm.so  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/amd64/server/Xusage.txt  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/calendars.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/charsets.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/classlist  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/cmm/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/cmm/CIEXYZ.pf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/cmm/GRAY.pf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/cmm/LINEAR_RGB.pf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/cmm/PYCC.pf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/cmm/sRGB.pf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/content-types.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/currency.data  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/cautionshield.icns  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/java-icon.ico  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/messages.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/messages_de.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/messages_es.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/messages_fr.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/messages_it.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/messages_ja.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/messages_ko.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/messages_pt_BR.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/messages_sv.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/messages_zh_CN.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/messages_zh_HK.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/messages_zh_TW.properties  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/mixcode_s.png  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/MixedCodeMainDialog.ui  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/MixedCodeMainDialogJs.ui  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/splash.gif  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/splash@2x.gif  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/splash_11-lic.gif  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy/splash_11@2x-lic.gif  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/deploy.jar  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/applications/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/applications/sun-java.desktop  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/applications/sun-javaws.desktop  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/applications/sun_java.desktop  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/16x16/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/16x16/apps/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/16x16/apps/sun-java.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/16x16/apps/sun-javaws.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/16x16/apps/sun-jcontrol.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/16x16/mimetypes/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/16x16/mimetypes/gnome-mime-application-x-java-archive.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/16x16/mimetypes/gnome-mime-application-x-java-jnlp-file.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/16x16/mimetypes/gnome-mime-text-x-java.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/48x48/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/48x48/apps/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/48x48/apps/sun-java.png  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/48x48/apps/sun-javaws.png  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/48x48/apps/sun-jcontrol.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/48x48/mimetypes/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/48x48/mimetypes/gnome-mime-application-x-java-archive.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/48x48/mimetypes/gnome-mime-application-x-java-jnlp-file.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/hicolor/48x48/mimetypes/gnome-mime-text-x-java.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/16x16/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/16x16/apps/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/16x16/apps/sun-java.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/16x16/apps/sun-javaws.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/16x16/apps/sun-jcontrol.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/16x16/mimetypes/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/16x16/mimetypes/gnome-mime-application-x-java-archive.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/16x16/mimetypes/gnome-mime-application-x-java-jnlp-file.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/16x16/mimetypes/gnome-mime-text-x-java.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/48x48/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/48x48/apps/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/48x48/apps/sun-java.png  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/48x48/apps/sun-javaws.png  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/48x48/apps/sun-jcontrol.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/48x48/mimetypes/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/48x48/mimetypes/gnome-mime-application-x-java-archive.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/48x48/mimetypes/gnome-mime-application-x-java-jnlp-file.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrast/48x48/mimetypes/gnome-mime-text-x-java.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/16x16/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/16x16/apps/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/16x16/apps/sun-java.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/16x16/apps/sun-javaws.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/16x16/apps/sun-jcontrol.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/16x16/mimetypes/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/16x16/mimetypes/gnome-mime-application-x-java-archive.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/16x16/mimetypes/gnome-mime-application-x-java-jnlp-file.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/16x16/mimetypes/gnome-mime-text-x-java.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/48x48/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/48x48/apps/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/48x48/apps/sun-java.png  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/48x48/apps/sun-javaws.png  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/48x48/apps/sun-jcontrol.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/48x48/mimetypes/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/48x48/mimetypes/gnome-mime-application-x-java-archive.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/48x48/mimetypes/gnome-mime-application-x-java-jnlp-file.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/HighContrastInverse/48x48/mimetypes/gnome-mime-text-x-java.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/16x16/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/16x16/apps/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/16x16/apps/sun-java.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/16x16/apps/sun-javaws.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/16x16/apps/sun-jcontrol.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/16x16/mimetypes/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/16x16/mimetypes/gnome-mime-application-x-java-archive.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/16x16/mimetypes/gnome-mime-application-x-java-jnlp-file.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/16x16/mimetypes/gnome-mime-text-x-java.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/48x48/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/48x48/apps/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/48x48/apps/sun-java.png  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/48x48/apps/sun-javaws.png  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/48x48/apps/sun-jcontrol.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/48x48/mimetypes/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/48x48/mimetypes/gnome-mime-application-x-java-archive.png  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/48x48/mimetypes/gnome-mime-application-x-java-jnlp-file.png  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/icons/LowContrast/48x48/mimetypes/gnome-mime-text-x-java.png  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/mime/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/mime/packages/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/mime/packages/x-java-archive.xml  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/desktop/mime/packages/x-java-jnlp-file.xml  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/ext/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/ext/cldrdata.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/ext/dnsns.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/ext/jaccess.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/ext/jfxrt.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/ext/localedata.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/ext/meta-index  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/ext/nashorn.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/ext/sunec.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/ext/sunjce_provider.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/ext/sunpkcs11.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/ext/zipfs.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/flavormap.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fontconfig.bfc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fontconfig.properties.src  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fontconfig.RedHat.5.bfc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fontconfig.RedHat.5.properties.src  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fontconfig.RedHat.6.bfc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fontconfig.RedHat.6.properties.src  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fontconfig.SuSE.10.bfc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fontconfig.SuSE.10.properties.src  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fontconfig.SuSE.11.bfc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fontconfig.SuSE.11.properties.src  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fontconfig.Turbo.bfc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fontconfig.Turbo.properties.src  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fonts/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fonts/fonts.dir  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fonts/LucidaBrightDemiBold.ttf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fonts/LucidaBrightDemiItalic.ttf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fonts/LucidaBrightItalic.ttf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fonts/LucidaBrightRegular.ttf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fonts/LucidaSansDemiBold.ttf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fonts/LucidaSansRegular.ttf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fonts/LucidaTypewriterBold.ttf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/fonts/LucidaTypewriterRegular.ttf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/hijrah-config-umalqura.properties  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/cursors/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/cursors/cursors.properties  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/cursors/invalid32x32.gif  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/cursors/motif_CopyDrop32x32.gif  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/cursors/motif_CopyNoDrop32x32.gif  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/cursors/motif_LinkDrop32x32.gif  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/cursors/motif_LinkNoDrop32x32.gif  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/cursors/motif_MoveDrop32x32.gif  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/cursors/motif_MoveNoDrop32x32.gif  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/icons/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/icons/sun-java.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/icons/sun-java_HighContrast.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/icons/sun-java_HighContrastInverse.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/images/icons/sun-java_LowContrast.png  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/javafx.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/javaws.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/jce.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/jexec  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/jfr/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/jfr/default.jfc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/jfr/profile.jfc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/jfr.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/jfxswt.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/jsse.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/jvm.hprof.txt  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/de/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/de/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/de/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/es/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/es/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/es/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/fr/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/fr/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/fr/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/it/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/it/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/it/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/ja/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/ja/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/ja/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/ko/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/ko/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/ko/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/ko.UTF-8/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/ko.UTF-8/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/ko.UTF-8/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/pt_BR/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/pt_BR/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/pt_BR/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/sv/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/sv/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/sv/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh.GBK/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh.GBK/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh.GBK/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh_HK.BIG5HK/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh_HK.BIG5HK/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh_HK.BIG5HK/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh_TW/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh_TW/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh_TW/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh_TW.BIG5/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh_TW.BIG5/LC_MESSAGES/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/locale/zh_TW.BIG5/LC_MESSAGES/sunw_java_plugin.mo  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/logging.properties  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/management/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/management/jmxremote.access  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/management/jmxremote.password.template  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/management/management.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/management/snmp.acl.template  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/management-agent.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/meta-index  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/net.properties  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/oblique-fonts/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/oblique-fonts/fonts.dir  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/oblique-fonts/LucidaSansDemiOblique.ttf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/oblique-fonts/LucidaSansOblique.ttf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/oblique-fonts/LucidaTypewriterBoldOblique.ttf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/oblique-fonts/LucidaTypewriterOblique.ttf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/plugin.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/psfont.properties.ja  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/psfontj2d.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/resources.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/rt.jar  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/blacklist  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/blacklisted.certs  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/cacerts  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/java.policy  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/java.security  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/javaws.policy  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/policy/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/policy/limited/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/policy/limited/local_policy.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/policy/limited/US_export_policy.jar  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/policy/unlimited/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/policy/unlimited/local_policy.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/policy/unlimited/US_export_policy.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/public_suffix_list.dat  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/security/trusted.libraries  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/sound.properties  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/lib/tzdb.dat  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/LICENSE  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/java.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/javaws.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/jjs.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/keytool.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/orbd.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/pack200.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/policytool.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/rmid.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/rmiregistry.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/servertool.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/tnameserv.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja/man1/unpack200.1  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/java.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/javaws.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/jjs.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/keytool.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/orbd.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/pack200.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/policytool.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/rmid.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/rmiregistry.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/servertool.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/tnameserv.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/ja_JP.UTF-8/man1/unpack200.1  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/java.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/javaws.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/jjs.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/keytool.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/orbd.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/pack200.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/policytool.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/rmid.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/rmiregistry.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/servertool.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/tnameserv.1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/man/man1/unpack200.1  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/plugin/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/plugin/desktop/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/plugin/desktop/sun_java.desktop  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/plugin/desktop/sun_java.png  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/README  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/release  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/THIRDPARTYLICENSEREADME-JAVAFX.txt  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/THIRDPARTYLICENSEREADME.txt  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/Welcome.html  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/license.txt  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/.eval.py.un~  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/bleu.py  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/bleu_scorer.py  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/LICENSE  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/__init__.py  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/__pycache__/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/__pycache__/.fuse_hidden0000004100000001  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/__pycache__/bleu.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/__pycache__/bleu.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/__pycache__/bleu.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/__pycache__/bleu_scorer.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/__pycache__/bleu_scorer.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/__pycache__/bleu_scorer.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/bleu/__pycache__/__init__.cpython-39.pyc  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/.cider.py.un~  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/.cider_scorer.py.un~  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/cider.py  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/cider_scorer.py  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__init__.py  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider.cpython-38.pyc.139810637649792  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider.cpython-38.pyc.139858489895360  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider_scorer.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider_scorer.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider_scorer.cpython-38.pyc.139718978043600  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider_scorer.cpython-38.pyc.139736400424080  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider_scorer.cpython-38.pyc.139861529558160  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider_scorer.cpython-38.pyc.140364774591200  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider_scorer.cpython-38.pyc.140430319791824  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider_scorer.cpython-38.pyc.140621213950144  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/cider_scorer.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/cider/__pycache__/__init__.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/eval.py  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/data/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/data/paraphrase-en.gz  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/meteor-1.5.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/meteor.py  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/__init__.py  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/__pycache__/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/__pycache__/meteor.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/__pycache__/meteor.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/__pycache__/meteor.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/meteor/__pycache__/__init__.cpython-39.pyc  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/rouge/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/rouge/rouge.py  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/rouge/__init__.py  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/rouge/__pycache__/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/rouge/__pycache__/rouge.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/rouge/__pycache__/rouge.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/rouge/__pycache__/rouge.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/rouge/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/rouge/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/rouge/__pycache__/__init__.cpython-39.pyc  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/\n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/cache/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/cache/data.mdb  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/cache/lock.mdb  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/hs_err_pid13088.log  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/hs_err_pid17609.log  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/hs_err_pid2705.log  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/hs_err_pid30499.log  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/hs_err_pid34765.log  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/hs_err_pid4198.log  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/ejml-0.23.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/fst-2.47.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/guava-19.0.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/hamcrest-core-1.3.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/jackson-core-2.5.3.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/javassist-3.19.0-GA.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/json-simple-1.1.1.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/junit-4.12.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/lmdbjni-0.4.6.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/lmdbjni-linux64-0.4.6.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/lmdbjni-osx64-0.4.6.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/lmdbjni-win64-0.4.6.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/Meteor-1.5.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/objenesis-2.4.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/SceneGraphParser-1.0.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/slf4j-api-1.7.12.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/slf4j-simple-1.7.21.jar  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/stanford-corenlp-3.6.0-models.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/lib/stanford-corenlp-3.6.0.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/spice-1.0.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/spice.py  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/\n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmp2j0_s9t1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmp2nv4397s  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmp327cdpuz  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmp390dn96r  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmp5cdu3g8m  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmp6tnp73dr  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmp7tk0xw9l  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmp86lpvw19  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmp91c53x1m  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpaxtu68y8  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpbzj83f1r  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpcxuc0zrb  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpgb3apgmz  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmph8d64zld  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmphzckvv4s  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpi7yxr5wo  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpi8sqsrcr  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpj0de8h4e  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpjl54lw0l  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpjy5axsj1  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmplh0diw_x  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpmkf2rdq9  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpmusk4q8p  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpmz0onad2  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpnirz2i_n  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpnnktd511  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpo75qx6cx  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpoh0gy57d  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpokd793ef  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpqydfppdh  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmprdp36rij  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpvmz46ynp  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpvrclg1gi  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpwc64skr_  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpx386_2l7  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpz4ukcniv  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmpze5dq851  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/tmp/tmp_9sgyyro  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/__init__.py  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/__pycache__/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/__pycache__/spice.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/__pycache__/spice.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/__pycache__/spice.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/spice/__pycache__/__init__.cpython-39.pyc  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/ptbtokenizer.py  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/stanford-corenlp-3.4.1.jar  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/tmp097473sf  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/tmp1jxg38mo  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/tmp3rsy_3hs  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/tmp3z60p3g9  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/tmp66eb5b__  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/tmpdtspw68s  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/tmpeyinvi9o  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/tmpu5_kr71r  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/tmpwebssvwz  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/__init__.py  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/__pycache__/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/__pycache__/ptbtokenizer.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/__pycache__/ptbtokenizer.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/__pycache__/ptbtokenizer.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/tokenizer/__pycache__/__init__.cpython-39.pyc  \n",
            " extracting: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/__init__.py  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/__pycache__/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/__pycache__/eval.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/__pycache__/eval.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/__pycache__/eval.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/__pycache__/__init__.cpython-37.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/__pycache__/__init__.cpython-38.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/pycocoevalcap/__pycache__/__init__.cpython-39.pyc  \n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/README.md  \n",
            "   creating: /content/datasets/xdecoder_data/coco_caption/results/\n",
            "  inflating: /content/datasets/xdecoder_data/coco_caption/results/captions_val2014_fakecap_results.json  \n"
          ]
        }
      ],
      "source": [
        "!wget https://huggingface.co/xdecoder/X-Decoder/resolve/main/coco_caption.zip\n",
        "!unzip -d /content/datasets/xdecoder_data coco_caption.zip"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hJS5t-rQ7nG2"
      },
      "outputs": [],
      "source": [
        "!export DETECTRON2_DATASETS=/content/datasets/xdecoder_data\n",
        "!export DATASET=/content/datasets/xdecoder_data\n",
        "!export DATASET2=/content/datasets/xdecoder_data\n",
        "!export VLDATASET=/content/datasets/xdecoder_data\n",
        "!export PATH=$PATH:/content/datasets/xdecoder_data/coco_caption/jre1.8.0_321/bin\n",
        "!export PYTHONPATH=$PYTHONPATH:/content/datasets/xdecoder_data/coco_caption\n",
        "!export WANDB_KEY='1c54a49c6cbfb12cd8b196375ba51a900a1774f4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8YFdSKDdj0_8"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['WANDB_KEY'] = '1c54a49c6cbfb12cd8b196375ba51a900a1774f4'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1T2CtouV_hoo"
      },
      "outputs": [],
      "source": [
        "# !pip install git+https://github.com/flauted/coco-caption.git@python23"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bvmbfVtslCU5"
      },
      "outputs": [],
      "source": [
        "# !pip install pycocoevalcap"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kiwsCTmMMpfw"
      },
      "outputs": [],
      "source": [
        "!cp -r /content/datasets/xdecoder_data/* /content/datasets/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xOBSWvmRJsBI",
        "outputId": "634db79c-a39d-4156-a864-9e3b534c5052"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "rm: cannot remove '/content/data/output/test/*': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!rm -r /content/data/output/test/*"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nu1d2my3bVH7"
      },
      "outputs": [],
      "source": [
        "# RESUME_FROM /content/datasets/xdecoder_data/pretrained/pretrained_w.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hq0aeDii_YLv",
        "outputId": "84ea1b9d-e16e-45e9-b4fb-905034b9434d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "WARNING:utils.arguments:Overrided COCO.INPUT.IMAGE_SIZE from 1024 to 256\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.HIDDEN_DIM from 512 to 512\n",
            "WARNING:utils.arguments:Overrided MODEL.ENCODER.CONVS_DIM from 512 to 512\n",
            "WARNING:utils.arguments:Overrided MODEL.ENCODER.MASK_DIM from 512 to 512\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.LVIS.ENABLED from True to False\n",
            "WARNING:utils.arguments:Overrided TEST.BATCH_SIZE_TOTAL from 8 to 1\n",
            "WARNING:utils.arguments:Overrided TRAIN.BATCH_SIZE_TOTAL from 4 to 4\n",
            "WARNING:utils.arguments:Overrided TRAIN.BATCH_SIZE_PER_GPU from 4 to 2\n",
            "WARNING:utils.arguments:Overrided SOLVER.BASE_LR from 0.0001 to 0.0001\n",
            "WARNING:utils.arguments:Overrided SOLVER.FIX_PARAM.backbone from True to True\n",
            "WARNING:utils.arguments:Overrided SOLVER.FIX_PARAM.lang_encoder from True to True\n",
            "WARNING:utils.arguments:Overrided SOLVER.FIX_PARAM.pixel_decoder from True to True\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.COST_SPATIAL.CLASS_WEIGHT from 5.0 to 5.0\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.COST_SPATIAL.MASK_WEIGHT from 2.0 to 2.0\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.COST_SPATIAL.DICE_WEIGHT from 2.0 to 2.0\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.TOP_SPATIAL_LAYERS from 10 to 10\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.SPATIAL.ENABLED from True to True\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.GROUNDING.ENABLED from True to True\n",
            "WARNING:utils.arguments:Overrided ATTENTION_ARCH.SPATIAL_MEMORIES from 32 to 32\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.SPATIAL.MAX_ITER from 1 to 5\n",
            "WARNING:utils.arguments:Overrided ATTENTION_ARCH.QUERY_NUMBER from 3 to 3\n",
            "WARNING:utils.arguments:Overrided STROKE_SAMPLER.MAX_CANDIDATE from 1 to 10\n",
            "WARNING:utils.arguments:Overrided MODEL.ENCODER.NUM_CLASSES from 133 to 5\n",
            "WARNING:utils.arguments:Overrided SOLVER.MAX_NUM_EPOCHS from 50 to 30\n",
            "WARNING:utils.arguments:Overrided LOG_EVERY from 100 to 100\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.TEST.PANOPTIC_ON from True to False\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.TEST.INSTANCE_ON from True to False\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "Deformable Transformer Encoder is not available.\n",
            "INFO:trainer.distributed_trainer:Setting SAVE_DIR as /content/data/output/test\n",
            "INFO:trainer.distributed_trainer:Using CUDA\n",
            "WARNING:trainer.utils.mpi_adapter:----------------\n",
            "WARNING:trainer.utils.mpi_adapter:MPI Adapter data\n",
            "WARNING:trainer.utils.mpi_adapter:----------------\n",
            "WARNING:trainer.utils.mpi_adapter:environment info: no MPI\n",
            "WARNING:trainer.utils.mpi_adapter:init method url: tcp://127.0.0.1:36873\n",
            "WARNING:trainer.utils.mpi_adapter:world size: 1\n",
            "WARNING:trainer.utils.mpi_adapter:local size: 1\n",
            "WARNING:trainer.utils.mpi_adapter:rank: 0\n",
            "WARNING:trainer.utils.mpi_adapter:local rank: 0\n",
            "WARNING:trainer.utils.mpi_adapter:master address: 127.0.0.1\n",
            "WARNING:trainer.utils.mpi_adapter:master port: 36873\n",
            "WARNING:trainer.utils.mpi_adapter:----------------\n",
            "INFO:trainer.distributed_trainer:Save config file to /content/data/output/test/conf_copy.yaml\n",
            "INFO:trainer.distributed_trainer:Base learning rate: 0.0001\n",
            "INFO:trainer.distributed_trainer:Number of GPUs: 1\n",
            "INFO:trainer.distributed_trainer:Gradient accumulation steps: 1\n",
            "INFO:trainer.default_trainer:Imported base_dir at base_path ./\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mmsdk\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: wandb version 0.17.1 is available!  To upgrade, please run:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  $ pip install wandb --upgrade\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.15.12\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/data/output/test/focall_unicl_lang_v1.yaml_conf~/run_2/wandb/wandb/run-20240611_182401-ygm0nv0l\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33m/content/data/output/test/focall_unicl_lang_v1.yaml_conf~/run_2\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/msdk/xdecoder\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/msdk/xdecoder/runs/ygm0nv0l\u001b[0m\n",
            "INFO:trainer.default_trainer:-------------------------------------------------------\n",
            "INFO:trainer.default_trainer:Training on rank: 0\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:pipeline.XDecoderPipeline:GeneralizedSEEM(\n",
            "  (backbone): D2FocalNet(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 192, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
            "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): FocalModulationBlock(\n",
            "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=192, out_features=389, bias=True)\n",
            "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (adopter_layer): Adapter_Layer(\n",
            "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (channel): Sequential(\n",
            "                (0): Linear(in_features=192, out_features=48, bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): Linear(in_features=48, out_features=192, bias=False)\n",
            "                (3): Sigmoid()\n",
            "              )\n",
            "              (spatial): Sequential(\n",
            "                (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): ConvTranspose2d(192, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (3): ReLU()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (1): FocalModulationBlock(\n",
            "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=192, out_features=389, bias=True)\n",
            "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (adopter_layer): Adapter_Layer(\n",
            "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (channel): Sequential(\n",
            "                (0): Linear(in_features=192, out_features=48, bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): Linear(in_features=48, out_features=192, bias=False)\n",
            "                (3): Sigmoid()\n",
            "              )\n",
            "              (spatial): Sequential(\n",
            "                (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): ConvTranspose2d(192, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (3): ReLU()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchEmbed(\n",
            "          (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0-1): 2 x FocalModulationBlock(\n",
            "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=384, out_features=773, bias=True)\n",
            "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(384, 384, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=384, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (adopter_layer): Adapter_Layer(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (channel): Sequential(\n",
            "                (0): Linear(in_features=384, out_features=96, bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): Linear(in_features=96, out_features=384, bias=False)\n",
            "                (3): Sigmoid()\n",
            "              )\n",
            "              (spatial): Sequential(\n",
            "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): ConvTranspose2d(384, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (3): ReLU()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchEmbed(\n",
            "          (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0-17): 18 x FocalModulationBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=768, out_features=1541, bias=True)\n",
            "              (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(768, 768, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=768, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (adopter_layer): Adapter_Layer(\n",
            "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (channel): Sequential(\n",
            "                (0): Linear(in_features=768, out_features=192, bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): Linear(in_features=192, out_features=768, bias=False)\n",
            "                (3): Sigmoid()\n",
            "              )\n",
            "              (spatial): Sequential(\n",
            "                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): ConvTranspose2d(768, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (3): ReLU()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchEmbed(\n",
            "          (proj): Conv2d(768, 1536, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (3): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0-1): 2 x FocalModulationBlock(\n",
            "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=1536, out_features=3077, bias=True)\n",
            "              (h): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(1536, 1536, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1536, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(1536, 1536, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=1536, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (adopter_layer): Adapter_Layer(\n",
            "              (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (channel): Sequential(\n",
            "                (0): Linear(in_features=1536, out_features=384, bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): Linear(in_features=384, out_features=1536, bias=False)\n",
            "                (3): Sigmoid()\n",
            "              )\n",
            "              (spatial): Sequential(\n",
            "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): ConvTranspose2d(1536, 1536, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (3): ReLU()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (sem_seg_head): XdecoderHead(\n",
            "    (pixel_decoder): TransformerEncoderPixelDecoder(\n",
            "      (adapter_1): Conv2d(\n",
            "        192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (adapter_2): Conv2d(\n",
            "        384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_2): Conv2d(\n",
            "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (adapter_3): Conv2d(\n",
            "        768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_3): Conv2d(\n",
            "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (mask_features): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (input_proj): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (transformer): TransformerEncoderOnly(\n",
            "        (encoder): TransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x TransformerEncoderLayer(\n",
            "              (self_attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "              )\n",
            "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 256\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (layer_4): Conv2d(\n",
            "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): SEEMDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 256\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(101, 512)\n",
            "      (query_embed): Embedding(101, 512)\n",
            "      (level_embed): Embedding(3, 512)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (lang_encoder): LanguageEncoder(\n",
            "        (lang_encoder): Transformer(\n",
            "          (token_embedding): Embedding(49408, 512)\n",
            "          (resblocks): ModuleList(\n",
            "            (0-11): 12 x ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm()\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm()\n",
            "              (drop_path): Identity()\n",
            "            )\n",
            "          )\n",
            "          (ln_final): LayerNorm()\n",
            "        )\n",
            "      )\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (mask_sptial_embed): ParameterList(\n",
            "          (0): Parameter containing: [torch.float32 of size 512x512]\n",
            "          (1): Parameter containing: [torch.float32 of size 512x512]\n",
            "          (2): Parameter containing: [torch.float32 of size 512x512]\n",
            "      )\n",
            "      (spatial_embed): Embedding(32, 512)\n",
            "      (spatial_featured): Embedding(32, 512)\n",
            "      (pn_indicator): Embedding(2, 512)\n",
            "      (attention_data): AttentionDataStruct()\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: []\n",
            "      weight_dict: {'loss_mask_ce_0': 2.0, 'loss_mask_dice_0': 5.0, 'loss_mask_bce_0': 5.0, 'loss_spatial_ce_0': 0.4, 'loss_spatial_dice_0': 1.0, 'loss_spatial_bce_0': 1.0, 'loss_grounding_ce_0': 0.4, 'loss_grounding_dice_0': 1.0, 'loss_grounding_bce_0': 1.0, 'loss_openimage_ce_0': 0.4, 'loss_openimage_dice_0': 1.0, 'loss_openimage_bce_0': 1.0, 'loss_mask_ce_1': 2.0, 'loss_mask_dice_1': 5.0, 'loss_mask_bce_1': 5.0, 'loss_spatial_ce_1': 0.4, 'loss_spatial_dice_1': 1.0, 'loss_spatial_bce_1': 1.0, 'loss_grounding_ce_1': 0.4, 'loss_grounding_dice_1': 1.0, 'loss_grounding_bce_1': 1.0, 'loss_openimage_ce_1': 0.4, 'loss_openimage_dice_1': 1.0, 'loss_openimage_bce_1': 1.0, 'loss_mask_ce_2': 2.0, 'loss_mask_dice_2': 5.0, 'loss_mask_bce_2': 5.0, 'loss_spatial_ce_2': 0.4, 'loss_spatial_dice_2': 1.0, 'loss_spatial_bce_2': 1.0, 'loss_grounding_ce_2': 0.4, 'loss_grounding_dice_2': 1.0, 'loss_grounding_bce_2': 1.0, 'loss_openimage_ce_2': 0.4, 'loss_openimage_dice_2': 1.0, 'loss_openimage_bce_2': 1.0, 'loss_mask_ce_3': 2.0, 'loss_mask_dice_3': 5.0, 'loss_mask_bce_3': 5.0, 'loss_spatial_ce_3': 0.4, 'loss_spatial_dice_3': 1.0, 'loss_spatial_bce_3': 1.0, 'loss_grounding_ce_3': 0.4, 'loss_grounding_dice_3': 1.0, 'loss_grounding_bce_3': 1.0, 'loss_openimage_ce_3': 0.4, 'loss_openimage_dice_3': 1.0, 'loss_openimage_bce_3': 1.0, 'loss_mask_ce_4': 2.0, 'loss_mask_dice_4': 5.0, 'loss_mask_bce_4': 5.0, 'loss_spatial_ce_4': 0.4, 'loss_spatial_dice_4': 1.0, 'loss_spatial_bce_4': 1.0, 'loss_grounding_ce_4': 0.4, 'loss_grounding_dice_4': 1.0, 'loss_grounding_bce_4': 1.0, 'loss_openimage_ce_4': 0.4, 'loss_openimage_dice_4': 1.0, 'loss_openimage_bce_4': 1.0, 'loss_mask_ce_5': 2.0, 'loss_mask_dice_5': 5.0, 'loss_mask_bce_5': 5.0, 'loss_spatial_ce_5': 0.4, 'loss_spatial_dice_5': 1.0, 'loss_spatial_bce_5': 1.0, 'loss_grounding_ce_5': 0.4, 'loss_grounding_dice_5': 1.0, 'loss_grounding_bce_5': 1.0, 'loss_openimage_ce_5': 0.4, 'loss_openimage_dice_5': 1.0, 'loss_openimage_bce_5': 1.0, 'loss_mask_ce_6': 2.0, 'loss_mask_dice_6': 5.0, 'loss_mask_bce_6': 5.0, 'loss_spatial_ce_6': 0.4, 'loss_spatial_dice_6': 1.0, 'loss_spatial_bce_6': 1.0, 'loss_grounding_ce_6': 0.4, 'loss_grounding_dice_6': 1.0, 'loss_grounding_bce_6': 1.0, 'loss_openimage_ce_6': 0.4, 'loss_openimage_dice_6': 1.0, 'loss_openimage_bce_6': 1.0, 'loss_mask_ce_7': 2.0, 'loss_mask_dice_7': 5.0, 'loss_mask_bce_7': 5.0, 'loss_spatial_ce_7': 0.4, 'loss_spatial_dice_7': 1.0, 'loss_spatial_bce_7': 1.0, 'loss_grounding_ce_7': 0.4, 'loss_grounding_dice_7': 1.0, 'loss_grounding_bce_7': 1.0, 'loss_openimage_ce_7': 0.4, 'loss_openimage_dice_7': 1.0, 'loss_openimage_bce_7': 1.0, 'loss_mask_ce_8': 2.0, 'loss_mask_dice_8': 5.0, 'loss_mask_bce_8': 5.0, 'loss_spatial_ce_8': 0.4, 'loss_spatial_dice_8': 1.0, 'loss_spatial_bce_8': 1.0, 'loss_grounding_ce_8': 0.4, 'loss_grounding_dice_8': 1.0, 'loss_grounding_bce_8': 1.0, 'loss_openimage_ce_8': 0.4, 'loss_openimage_dice_8': 1.0, 'loss_openimage_bce_8': 1.0, 'loss_mask_ce_9': 2.0, 'loss_mask_dice_9': 5.0, 'loss_mask_bce_9': 5.0, 'loss_spatial_ce_9': 0.4, 'loss_spatial_dice_9': 1.0, 'loss_spatial_bce_9': 1.0, 'loss_grounding_ce_9': 0.4, 'loss_grounding_dice_9': 1.0, 'loss_grounding_bce_9': 1.0, 'loss_openimage_ce_9': 0.4, 'loss_openimage_dice_9': 1.0, 'loss_openimage_bce_9': 1.0}\n",
            "      num_classes: 5\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")\n",
            "INFO:datasets.dataset_mappers.coco_panoptic_interactive_dataset_mapper:[COCOPanopticNewBaselineDatasetMapper] Full TransformGens used in training: [RandomFlip(), ResizeScale(min_scale=0.1, max_scale=2.0, target_height=256, target_width=256), FixedSizeCrop(crop_size=(256, 256))]\n",
            "INFO:datasets.build:Using training sampler TrainingSampler\n",
            "INFO:detectron2.data.common:Serializing 500 elements to byte tensors and concatenating them all ...\n",
            "INFO:detectron2.data.common:Serialized dataset takes 0.61 MiB\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "INFO:pipeline.XDecoderPipeline:num of train samples: 125\n",
            "INFO:trainer.xdecoder_trainer:Calculate MAX_ITER @ 3750 and STEPS @ [3333, 3611]\n",
            "INFO:trainer.xdecoder_trainer:Total number of parameters in default module (on each GPU): 741670905\n",
            "INFO:trainer.xdecoder_trainer:Number of trainable parameters in default module (on each GPU): 39812608\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.bias, Model Shape: torch.Size([389]) <-> Ckpt Shape: torch.Size([389])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.weight, Model Shape: torch.Size([389, 192]) <-> Ckpt Shape: torch.Size([389, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([192, 1, 9, 9]) <-> Ckpt Shape: torch.Size([192, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.bias, Model Shape: torch.Size([389]) <-> Ckpt Shape: torch.Size([389])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.weight, Model Shape: torch.Size([389, 192]) <-> Ckpt Shape: torch.Size([389, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([192, 1, 9, 9]) <-> Ckpt Shape: torch.Size([192, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.weight, Model Shape: torch.Size([384, 192, 3, 3]) <-> Ckpt Shape: torch.Size([384, 192, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.bias, Model Shape: torch.Size([773]) <-> Ckpt Shape: torch.Size([773])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.weight, Model Shape: torch.Size([773, 384]) <-> Ckpt Shape: torch.Size([773, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([384, 1, 9, 9]) <-> Ckpt Shape: torch.Size([384, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.bias, Model Shape: torch.Size([773]) <-> Ckpt Shape: torch.Size([773])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.weight, Model Shape: torch.Size([773, 384]) <-> Ckpt Shape: torch.Size([773, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([384, 1, 9, 9]) <-> Ckpt Shape: torch.Size([384, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.weight, Model Shape: torch.Size([768, 384, 3, 3]) <-> Ckpt Shape: torch.Size([768, 384, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.weight, Model Shape: torch.Size([1536, 768, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_1, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_2, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([6144]) <-> Ckpt Shape: torch.Size([6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([6144, 1536]) <-> Ckpt Shape: torch.Size([6144, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([1536, 6144]) <-> Ckpt Shape: torch.Size([1536, 6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.bias, Model Shape: torch.Size([3077]) <-> Ckpt Shape: torch.Size([3077])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.weight, Model Shape: torch.Size([3077, 1536]) <-> Ckpt Shape: torch.Size([3077, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([1536, 1, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([1536, 1, 5, 5]) <-> Ckpt Shape: torch.Size([1536, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([1536, 1, 7, 7]) <-> Ckpt Shape: torch.Size([1536, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([1536, 1, 9, 9]) <-> Ckpt Shape: torch.Size([1536, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.weight, Model Shape: torch.Size([1536, 1536, 1, 1]) <-> Ckpt Shape: torch.Size([1536, 1536, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.weight, Model Shape: torch.Size([1536, 1536]) <-> Ckpt Shape: torch.Size([1536, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_1, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_2, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([6144]) <-> Ckpt Shape: torch.Size([6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([6144, 1536]) <-> Ckpt Shape: torch.Size([6144, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([1536, 6144]) <-> Ckpt Shape: torch.Size([1536, 6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.bias, Model Shape: torch.Size([3077]) <-> Ckpt Shape: torch.Size([3077])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.weight, Model Shape: torch.Size([3077, 1536]) <-> Ckpt Shape: torch.Size([3077, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([1536, 1, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([1536, 1, 5, 5]) <-> Ckpt Shape: torch.Size([1536, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([1536, 1, 7, 7]) <-> Ckpt Shape: torch.Size([1536, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([1536, 1, 9, 9]) <-> Ckpt Shape: torch.Size([1536, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.weight, Model Shape: torch.Size([1536, 1536, 1, 1]) <-> Ckpt Shape: torch.Size([1536, 1536, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.weight, Model Shape: torch.Size([1536, 1536]) <-> Ckpt Shape: torch.Size([1536, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.norm0.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.norm0.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.norm3.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.norm3.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.norm.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.norm.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.proj.weight, Model Shape: torch.Size([192, 3, 7, 7]) <-> Ckpt Shape: torch.Size([192, 3, 7, 7])\n",
            "INFO:utils.model:Loaded dilation_kernel, Model Shape: torch.Size([1, 1, 3, 3]) <-> Ckpt Shape: torch.Size([1, 1, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_1.weight, Model Shape: torch.Size([512, 192, 1, 1]) <-> Ckpt Shape: torch.Size([512, 192, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_2.weight, Model Shape: torch.Size([512, 384, 1, 1]) <-> Ckpt Shape: torch.Size([512, 384, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_3.weight, Model Shape: torch.Size([512, 768, 1, 1]) <-> Ckpt Shape: torch.Size([512, 768, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.input_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.input_proj.weight, Model Shape: torch.Size([512, 1536, 1, 1]) <-> Ckpt Shape: torch.Size([512, 1536, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_1.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_2.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_3.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_4.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.mask_features.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.mask_features.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.class_embed, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.decoder_norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.decoder_norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding, Model Shape: torch.Size([77, 512]) <-> Ckpt Shape: torch.Size([77, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.token_embedding.weight, Model Shape: torch.Size([49408, 512]) <-> Ckpt Shape: torch.Size([49408, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_proj, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.logit_scale, Model Shape: torch.Size([]) <-> Ckpt Shape: torch.Size([])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.level_embed.weight, Model Shape: torch.Size([3, 512]) <-> Ckpt Shape: torch.Size([3, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.0.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.0.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.1.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.2.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_sptial_embed.0, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_sptial_embed.1, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_sptial_embed.2, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.pn_indicator.weight, Model Shape: torch.Size([2, 512]) <-> Ckpt Shape: torch.Size([2, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.query_embed.weight, Model Shape: torch.Size([101, 512]) <-> Ckpt Shape: torch.Size([101, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.query_feat.weight, Model Shape: torch.Size([101, 512]) <-> Ckpt Shape: torch.Size([101, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.spatial_embed.weight, Model Shape: torch.Size([32, 512]) <-> Ckpt Shape: torch.Size([32, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.spatial_featured.weight, Model Shape: torch.Size([32, 512]) <-> Ckpt Shape: torch.Size([32, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.0.blocks.0.adopter_layer.channel.0.weight, Model Shape: torch.Size([48, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.0.blocks.0.adopter_layer.channel.2.weight, Model Shape: torch.Size([192, 48])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.0.blocks.0.adopter_layer.norm.bias, Model Shape: torch.Size([192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.0.blocks.0.adopter_layer.norm.weight, Model Shape: torch.Size([192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.0.blocks.0.adopter_layer.spatial.0.weight, Model Shape: torch.Size([192, 192, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.0.blocks.0.adopter_layer.spatial.2.weight, Model Shape: torch.Size([192, 192, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.0.blocks.1.adopter_layer.channel.0.weight, Model Shape: torch.Size([48, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.0.blocks.1.adopter_layer.channel.2.weight, Model Shape: torch.Size([192, 48])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.0.blocks.1.adopter_layer.norm.bias, Model Shape: torch.Size([192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.0.blocks.1.adopter_layer.norm.weight, Model Shape: torch.Size([192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.0.blocks.1.adopter_layer.spatial.0.weight, Model Shape: torch.Size([192, 192, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.0.blocks.1.adopter_layer.spatial.2.weight, Model Shape: torch.Size([192, 192, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.1.blocks.0.adopter_layer.channel.0.weight, Model Shape: torch.Size([96, 384])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.1.blocks.0.adopter_layer.channel.2.weight, Model Shape: torch.Size([384, 96])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.1.blocks.0.adopter_layer.norm.bias, Model Shape: torch.Size([384])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.1.blocks.0.adopter_layer.norm.weight, Model Shape: torch.Size([384])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.1.blocks.0.adopter_layer.spatial.0.weight, Model Shape: torch.Size([384, 384, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.1.blocks.0.adopter_layer.spatial.2.weight, Model Shape: torch.Size([384, 384, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.1.blocks.1.adopter_layer.channel.0.weight, Model Shape: torch.Size([96, 384])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.1.blocks.1.adopter_layer.channel.2.weight, Model Shape: torch.Size([384, 96])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.1.blocks.1.adopter_layer.norm.bias, Model Shape: torch.Size([384])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.1.blocks.1.adopter_layer.norm.weight, Model Shape: torch.Size([384])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.1.blocks.1.adopter_layer.spatial.0.weight, Model Shape: torch.Size([384, 384, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.1.blocks.1.adopter_layer.spatial.2.weight, Model Shape: torch.Size([384, 384, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.0.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.0.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.0.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.0.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.0.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.0.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.1.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.1.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.1.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.1.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.1.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.1.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.10.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.10.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.10.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.10.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.10.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.10.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.11.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.11.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.11.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.11.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.11.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.11.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.12.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.12.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.12.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.12.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.12.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.12.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.13.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.13.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.13.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.13.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.13.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.13.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.14.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.14.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.14.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.14.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.14.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.14.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.15.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.15.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.15.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.15.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.15.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.15.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.16.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.16.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.16.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.16.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.16.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.16.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.17.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.17.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.17.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.17.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.17.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.17.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.2.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.2.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.2.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.2.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.2.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.2.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.3.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.3.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.3.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.3.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.3.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.3.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.4.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.4.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.4.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.4.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.4.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.4.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.5.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.5.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.5.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.5.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.5.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.5.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.6.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.6.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.6.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.6.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.6.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.6.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.7.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.7.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.7.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.7.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.7.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.7.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.8.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.8.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.8.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.8.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.8.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.8.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.9.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.9.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.9.adopter_layer.norm.bias, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.9.adopter_layer.norm.weight, Model Shape: torch.Size([768])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.9.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.2.blocks.9.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.3.blocks.0.adopter_layer.channel.0.weight, Model Shape: torch.Size([384, 1536])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.3.blocks.0.adopter_layer.channel.2.weight, Model Shape: torch.Size([1536, 384])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.3.blocks.0.adopter_layer.norm.bias, Model Shape: torch.Size([1536])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.3.blocks.0.adopter_layer.norm.weight, Model Shape: torch.Size([1536])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.3.blocks.0.adopter_layer.spatial.0.weight, Model Shape: torch.Size([1536, 1536, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.3.blocks.0.adopter_layer.spatial.2.weight, Model Shape: torch.Size([1536, 1536, 4, 4])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.3.blocks.1.adopter_layer.channel.0.weight, Model Shape: torch.Size([384, 1536])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.3.blocks.1.adopter_layer.channel.2.weight, Model Shape: torch.Size([1536, 384])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.3.blocks.1.adopter_layer.norm.bias, Model Shape: torch.Size([1536])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.3.blocks.1.adopter_layer.norm.weight, Model Shape: torch.Size([1536])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.3.blocks.1.adopter_layer.spatial.0.weight, Model Shape: torch.Size([1536, 1536, 3, 3])\n",
            "WARNING:utils.model:*UNLOADED* backbone.layers.3.blocks.1.adopter_layer.spatial.2.weight, Model Shape: torch.Size([1536, 1536, 4, 4])\n",
            "WARNING:utils.model:$UNUSED$ criterion.empty_weight, Ckpt Shape: torch.Size([134])\n",
            "WARNING:utils.model:*UNMATCHED* criterion.empty_weight, Model Shape: torch.Size([6]) <-> Ckpt Shape: torch.Size([134])\n",
            "WARNING:trainer.utils_trainer:Load weights from /content/datasets/xdecoder_data/pretrained/pretrained_w.pt...\n",
            "INFO:trainer.default_trainer:***** Running training *****\n",
            "INFO:trainer.default_trainer:  Num of GPUs = 1\n",
            "INFO:trainer.default_trainer:  Num Epochs = 30\n",
            "INFO:trainer.default_trainer:  Num of Mini Batches per Epoch = 125\n",
            "INFO:trainer.default_trainer:  Total train batch size (w. parallel, distributed & accumulation) = 3750\n",
            "INFO:trainer.default_trainer:  Gradient Accumulation steps = 1\n",
            "INFO:trainer.default_trainer:  Total optimization steps = 3750\n",
            "INFO:trainer.default_trainer:Start epoch: 0 training.\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "INFO:trainer.default_trainer:epochs[     0] optim steps[1] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 4.59969/4.59969, loss_mask_bce_0: 1.39762/1.39762, loss_mask_dice_0: 2.61853/2.61853, loss_spatial_bce_0: 0.58731/0.58731, loss_spatial_dice_0: 0.61676/0.61676, loss_spatial_ce_0: 0.69829/0.69829, loss_grounding_bce_0: 0.32296/0.32296, loss_grounding_dice_0: 0.42484/0.42484, loss_grounding_ce_0: 2.99872/2.99872, loss_mask_ce_1: 3.83510/3.83510, loss_mask_bce_1: 1.39016/1.39016, loss_mask_dice_1: 2.54513/2.54513, loss_spatial_bce_1: 0.59665/0.59665, loss_spatial_dice_1: 0.61950/0.61950, loss_spatial_ce_1: 0.71234/0.71234, loss_grounding_bce_1: 0.26692/0.26692, loss_grounding_dice_1: 0.40576/0.40576, loss_grounding_ce_1: 3.35285/3.35285, loss_mask_ce_2: 3.82616/3.82616, loss_mask_bce_2: 1.42903/1.42903, loss_mask_dice_2: 2.53769/2.53769, loss_spatial_bce_2: 0.58424/0.58424, loss_spatial_dice_2: 0.63669/0.63669, loss_spatial_ce_2: 0.73645/0.73645, loss_grounding_bce_2: 0.28495/0.28495, loss_grounding_dice_2: 0.40691/0.40691, loss_grounding_ce_2: 3.69030/3.69030, loss_mask_ce_3: 4.37530/4.37530, loss_mask_bce_3: 1.48633/1.48633, loss_mask_dice_3: 2.52644/2.52644, loss_spatial_bce_3: 0.55821/0.55821, loss_spatial_dice_3: 0.63081/0.63081, loss_spatial_ce_3: 0.78879/0.78879, loss_grounding_bce_3: 0.31140/0.31140, loss_grounding_dice_3: 0.41246/0.41246, loss_grounding_ce_3: 3.64830/3.64830, loss_mask_ce_4: 4.35562/4.35562, loss_mask_bce_4: 1.40912/1.40912, loss_mask_dice_4: 2.66238/2.66238, loss_spatial_bce_4: 0.52577/0.52577, loss_spatial_dice_4: 0.62447/0.62447, loss_spatial_ce_4: 0.91845/0.91845, loss_grounding_bce_4: 0.27667/0.27667, loss_grounding_dice_4: 0.42139/0.42139, loss_grounding_ce_4: 3.85764/3.85764, loss_mask_ce_5: 4.37683/4.37683, loss_mask_bce_5: 1.45012/1.45012, loss_mask_dice_5: 2.82122/2.82122, loss_spatial_bce_5: 0.55049/0.55049, loss_spatial_dice_5: 0.64145/0.64145, loss_spatial_ce_5: 0.77826/0.77826, loss_grounding_bce_5: 0.27962/0.27962, loss_grounding_dice_5: 0.41251/0.41251, loss_grounding_ce_5: 4.92413/4.92413, loss_mask_ce_6: 4.03154/4.03154, loss_mask_bce_6: 1.50303/1.50303, loss_mask_dice_6: 2.78024/2.78024, loss_spatial_bce_6: 0.55328/0.55328, loss_spatial_dice_6: 0.66321/0.66321, loss_spatial_ce_6: 0.76047/0.76047, loss_grounding_bce_6: 0.27096/0.27096, loss_grounding_dice_6: 0.41916/0.41916, loss_grounding_ce_6: 4.59477/4.59477, loss_mask_ce_7: 3.49647/3.49647, loss_mask_bce_7: 1.39742/1.39742, loss_mask_dice_7: 2.68645/2.68645, loss_spatial_bce_7: 0.57258/0.57258, loss_spatial_dice_7: 0.65443/0.65443, loss_spatial_ce_7: 0.51507/0.51507, loss_grounding_bce_7: 0.21167/0.21167, loss_grounding_dice_7: 0.35382/0.35382, loss_grounding_ce_7: 4.89626/4.89626, loss_mask_ce_8: 2.81758/2.81758, loss_mask_bce_8: 1.74861/1.74861, loss_mask_dice_8: 2.94943/2.94943, loss_spatial_bce_8: 0.62773/0.62773, loss_spatial_dice_8: 0.72293/0.72293, loss_spatial_ce_8: 0.69953/0.69953, loss_grounding_bce_8: 0.27238/0.27238, loss_grounding_dice_8: 0.44110/0.44110, loss_grounding_ce_8: 4.97305/4.97305, loss_mask_ce_9: 2.98311/2.98311, loss_mask_bce_9: 1.82000/1.82000, loss_mask_dice_9: 3.13975/3.13975, loss_spatial_bce_9: 0.60062/0.60062, loss_spatial_dice_9: 0.72859/0.72859, loss_spatial_ce_9: 0.93205/0.93205, loss_grounding_bce_9: 0.41518/0.41518, loss_grounding_dice_9: 0.50752/0.50752, loss_grounding_ce_9: 3.54792/3.54792] items per batch[4] items per second[0.46] total items[4] mini batches[     1] memory[7282] epoch remaining[0:17:49]\n",
            "INFO:trainer.default_trainer:epochs[     0] optim steps[2] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 2.18912/3.39440, loss_mask_bce_0: 1.54966/1.47364, loss_mask_dice_0: 2.69941/2.65897, loss_spatial_bce_0: 0.56226/0.57478, loss_spatial_dice_0: 0.68199/0.64937, loss_spatial_ce_0: 0.49214/0.59521, loss_grounding_bce_0: 0.33293/0.32794, loss_grounding_dice_0: 0.43765/0.43125, loss_grounding_ce_0: 2.94824/2.97348, loss_mask_ce_1: 2.21530/3.02520, loss_mask_bce_1: 1.63544/1.51280, loss_mask_dice_1: 2.63326/2.58920, loss_spatial_bce_1: 0.54997/0.57331, loss_spatial_dice_1: 0.66029/0.63989, loss_spatial_ce_1: 0.61393/0.66314, loss_grounding_bce_1: 0.32732/0.29712, loss_grounding_dice_1: 0.45307/0.42942, loss_grounding_ce_1: 2.97699/3.16492, loss_mask_ce_2: 2.33737/3.08177, loss_mask_bce_2: 1.51085/1.46994, loss_mask_dice_2: 2.60974/2.57372, loss_spatial_bce_2: 0.41785/0.50105, loss_spatial_dice_2: 0.63092/0.63380, loss_spatial_ce_2: 0.61986/0.67816, loss_grounding_bce_2: 0.30434/0.29465, loss_grounding_dice_2: 0.42121/0.41406, loss_grounding_ce_2: 2.57008/3.13019, loss_mask_ce_3: 2.35213/3.36371, loss_mask_bce_3: 1.47719/1.48176, loss_mask_dice_3: 2.50855/2.51750, loss_spatial_bce_3: 0.52982/0.54401, loss_spatial_dice_3: 0.66020/0.64550, loss_spatial_ce_3: 0.53824/0.66351, loss_grounding_bce_3: 0.27692/0.29416, loss_grounding_dice_3: 0.43059/0.42152, loss_grounding_ce_3: 2.96522/3.30676, loss_mask_ce_4: 2.21359/3.28460, loss_mask_bce_4: 1.57043/1.48978, loss_mask_dice_4: 2.66611/2.66424, loss_spatial_bce_4: 0.62001/0.57289, loss_spatial_dice_4: 0.69481/0.65964, loss_spatial_ce_4: 0.61091/0.76468, loss_grounding_bce_4: 0.29023/0.28345, loss_grounding_dice_4: 0.41195/0.41667, loss_grounding_ce_4: 2.72766/3.29265, loss_mask_ce_5: 2.45296/3.41489, loss_mask_bce_5: 1.53891/1.49452, loss_mask_dice_5: 2.76765/2.79443, loss_spatial_bce_5: 0.64599/0.59824, loss_spatial_dice_5: 0.69921/0.67033, loss_spatial_ce_5: 0.54383/0.66105, loss_grounding_bce_5: 0.26988/0.27475, loss_grounding_dice_5: 0.46027/0.43639, loss_grounding_ce_5: 3.10208/4.01311, loss_mask_ce_6: 2.49353/3.26253, loss_mask_bce_6: 1.56799/1.53551, loss_mask_dice_6: 2.74230/2.76127, loss_spatial_bce_6: 0.63333/0.59331, loss_spatial_dice_6: 0.64363/0.65342, loss_spatial_ce_6: 0.45440/0.60743, loss_grounding_bce_6: 0.38429/0.32762, loss_grounding_dice_6: 0.43508/0.42712, loss_grounding_ce_6: 3.29018/3.94247, loss_mask_ce_7: 2.53002/3.01325, loss_mask_bce_7: 1.54919/1.47330, loss_mask_dice_7: 2.54953/2.61799, loss_spatial_bce_7: 0.56354/0.56806, loss_spatial_dice_7: 0.65215/0.65329, loss_spatial_ce_7: 0.65982/0.58745, loss_grounding_bce_7: 0.35542/0.28355, loss_grounding_dice_7: 0.44297/0.39840, loss_grounding_ce_7: 2.50556/3.70091, loss_mask_ce_8: 2.93922/2.87840, loss_mask_bce_8: 1.60028/1.67444, loss_mask_dice_8: 2.95081/2.95012, loss_spatial_bce_8: 0.61389/0.62081, loss_spatial_dice_8: 0.72527/0.72410, loss_spatial_ce_8: 0.83564/0.76758, loss_grounding_bce_8: 0.32339/0.29789, loss_grounding_dice_8: 0.51887/0.47999, loss_grounding_ce_8: 3.64652/4.30979, loss_mask_ce_9: 2.45022/2.71666, loss_mask_bce_9: 2.00748/1.91374, loss_mask_dice_9: 3.28014/3.20995, loss_spatial_bce_9: 0.55112/0.57587, loss_spatial_dice_9: 0.74971/0.73915, loss_spatial_ce_9: 0.97369/0.95287, loss_grounding_bce_9: 0.47268/0.44393, loss_grounding_dice_9: 0.59666/0.55209, loss_grounding_ce_9: 3.10493/3.32643] items per batch[4] items per second[1.10] total items[8] mini batches[     2] memory[10887] epoch remaining[0:12:33]\n",
            "INFO:trainer.default_trainer:epochs[     0] optim steps[3] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 1.95531/2.91471, loss_mask_bce_0: 0.81738/1.25489, loss_mask_dice_0: 2.45970/2.59255, loss_spatial_bce_0: 0.41372/0.52109, loss_spatial_dice_0: 0.66232/0.65369, loss_spatial_ce_0: 0.77960/0.65668, loss_grounding_bce_0: 0.13863/0.26484, loss_grounding_dice_0: 0.45195/0.43815, loss_grounding_ce_0: 2.49806/2.81501, loss_mask_ce_1: 2.00245/2.68428, loss_mask_bce_1: 0.71331/1.24630, loss_mask_dice_1: 2.39373/2.52404, loss_spatial_bce_1: 0.49435/0.54699, loss_spatial_dice_1: 0.65608/0.64529, loss_spatial_ce_1: 0.55041/0.62556, loss_grounding_bce_1: 0.12477/0.23967, loss_grounding_dice_1: 0.45169/0.43684, loss_grounding_ce_1: 2.59525/2.97503, loss_mask_ce_2: 2.25552/2.80635, loss_mask_bce_2: 0.86477/1.26822, loss_mask_dice_2: 2.41322/2.52022, loss_spatial_bce_2: 0.50146/0.50118, loss_spatial_dice_2: 0.68571/0.65111, loss_spatial_ce_2: 0.51802/0.62478, loss_grounding_bce_2: 0.11723/0.23551, loss_grounding_dice_2: 0.42977/0.41930, loss_grounding_ce_2: 2.57634/2.94557, loss_mask_ce_3: 2.06758/2.93167, loss_mask_bce_3: 0.86218/1.27523, loss_mask_dice_3: 2.39261/2.47587, loss_spatial_bce_3: 0.51706/0.53503, loss_spatial_dice_3: 0.69455/0.66185, loss_spatial_ce_3: 0.57102/0.63268, loss_grounding_bce_3: 0.13944/0.24259, loss_grounding_dice_3: 0.42610/0.42305, loss_grounding_ce_3: 2.36326/2.99226, loss_mask_ce_4: 2.01367/2.86096, loss_mask_bce_4: 0.85527/1.27827, loss_mask_dice_4: 2.41917/2.58255, loss_spatial_bce_4: 0.45927/0.53502, loss_spatial_dice_4: 0.66231/0.66053, loss_spatial_ce_4: 0.66616/0.73184, loss_grounding_bce_4: 0.12179/0.22956, loss_grounding_dice_4: 0.42202/0.41845, loss_grounding_ce_4: 2.51398/3.03309, loss_mask_ce_5: 2.47744/3.10241, loss_mask_bce_5: 0.75348/1.24751, loss_mask_dice_5: 2.31592/2.63493, loss_spatial_bce_5: 0.47776/0.55808, loss_spatial_dice_5: 0.67025/0.67030, loss_spatial_ce_5: 0.76875/0.69695, loss_grounding_bce_5: 0.12723/0.22558, loss_grounding_dice_5: 0.40811/0.42696, loss_grounding_ce_5: 2.75698/3.59440, loss_mask_ce_6: 2.50960/3.01156, loss_mask_bce_6: 0.88036/1.31712, loss_mask_dice_6: 2.47946/2.66733, loss_spatial_bce_6: 0.45042/0.54568, loss_spatial_dice_6: 0.65866/0.65517, loss_spatial_ce_6: 0.64440/0.61975, loss_grounding_bce_6: 0.11818/0.25781, loss_grounding_dice_6: 0.42179/0.42534, loss_grounding_ce_6: 3.04024/3.64173, loss_mask_ce_7: 2.63136/2.88595, loss_mask_bce_7: 0.80402/1.25021, loss_mask_dice_7: 2.45012/2.56203, loss_spatial_bce_7: 0.51029/0.54880, loss_spatial_dice_7: 0.68457/0.66372, loss_spatial_ce_7: 0.57974/0.58488, loss_grounding_bce_7: 0.12308/0.23006, loss_grounding_dice_7: 0.47609/0.42429, loss_grounding_ce_7: 2.31162/3.23782, loss_mask_ce_8: 2.89295/2.88325, loss_mask_bce_8: 0.80408/1.38432, loss_mask_dice_8: 2.64500/2.84841, loss_spatial_bce_8: 0.65396/0.63186, loss_spatial_dice_8: 0.75400/0.73407, loss_spatial_ce_8: 0.81875/0.78464, loss_grounding_bce_8: 0.12490/0.24023, loss_grounding_dice_8: 0.49600/0.48533, loss_grounding_ce_8: 3.56156/4.06038, loss_mask_ce_9: 2.46118/2.63150, loss_mask_bce_9: 1.37358/1.73369, loss_mask_dice_9: 3.18645/3.20212, loss_spatial_bce_9: 0.52936/0.56037, loss_spatial_dice_9: 0.77319/0.75049, loss_spatial_ce_9: 1.19933/1.03502, loss_grounding_bce_9: 0.25000/0.37928, loss_grounding_dice_9: 0.60814/0.57077, loss_grounding_ce_9: 2.80094/3.15126] items per batch[4] items per second[1.44] total items[12] mini batches[     3] memory[10887] epoch remaining[0:10:10]\n",
            "INFO:trainer.default_trainer:epochs[     0] optim steps[4] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 1.93143/2.66889, loss_mask_bce_0: 1.04483/1.20237, loss_mask_dice_0: 2.01733/2.44874, loss_spatial_bce_0: 0.39877/0.49051, loss_spatial_dice_0: 0.56412/0.63130, loss_spatial_ce_0: 0.35842/0.58211, loss_grounding_bce_0: 0.16839/0.24073, loss_grounding_dice_0: 0.52437/0.45970, loss_grounding_ce_0: 2.63960/2.77116, loss_mask_ce_1: 1.98071/2.50839, loss_mask_bce_1: 1.02882/1.19193, loss_mask_dice_1: 1.81792/2.34751, loss_spatial_bce_1: 0.39185/0.50821, loss_spatial_dice_1: 0.54184/0.61943, loss_spatial_ce_1: 0.42484/0.57538, loss_grounding_bce_1: 0.17040/0.22236, loss_grounding_dice_1: 0.51112/0.45541, loss_grounding_ce_1: 3.01955/2.98616, loss_mask_ce_2: 2.10763/2.63167, loss_mask_bce_2: 1.02924/1.20847, loss_mask_dice_2: 1.88955/2.36255, loss_spatial_bce_2: 0.35110/0.46366, loss_spatial_dice_2: 0.54520/0.62463, loss_spatial_ce_2: 0.48905/0.59085, loss_grounding_bce_2: 0.16512/0.21791, loss_grounding_dice_2: 0.53363/0.44788, loss_grounding_ce_2: 3.10389/2.98515, loss_mask_ce_3: 1.95526/2.68757, loss_mask_bce_3: 1.08831/1.22850, loss_mask_dice_3: 1.87474/2.32559, loss_spatial_bce_3: 0.36618/0.49282, loss_spatial_dice_3: 0.57612/0.64042, loss_spatial_ce_3: 0.46023/0.58957, loss_grounding_bce_3: 0.14446/0.21805, loss_grounding_dice_3: 0.51520/0.44609, loss_grounding_ce_3: 3.00524/2.99551, loss_mask_ce_4: 1.96095/2.63596, loss_mask_bce_4: 1.17775/1.25314, loss_mask_dice_4: 1.98006/2.43193, loss_spatial_bce_4: 0.35572/0.49019, loss_spatial_dice_4: 0.55475/0.63409, loss_spatial_ce_4: 0.57059/0.69153, loss_grounding_bce_4: 0.22450/0.22830, loss_grounding_dice_4: 0.54038/0.44894, loss_grounding_ce_4: 2.23680/2.83402, loss_mask_ce_5: 2.11104/2.85456, loss_mask_bce_5: 0.86798/1.15262, loss_mask_dice_5: 2.07664/2.49536, loss_spatial_bce_5: 0.36403/0.50957, loss_spatial_dice_5: 0.56182/0.64318, loss_spatial_ce_5: 0.54214/0.65825, loss_grounding_bce_5: 0.16817/0.21123, loss_grounding_dice_5: 0.52057/0.45037, loss_grounding_ce_5: 3.11353/3.47418, loss_mask_ce_6: 2.37132/2.85150, loss_mask_bce_6: 0.99541/1.23670, loss_mask_dice_6: 2.01711/2.50478, loss_spatial_bce_6: 0.43657/0.51840, loss_spatial_dice_6: 0.58545/0.63774, loss_spatial_ce_6: 0.40697/0.56656, loss_grounding_bce_6: 0.15257/0.23150, loss_grounding_dice_6: 0.54178/0.45445, loss_grounding_ce_6: 2.98615/3.47784, loss_mask_ce_7: 2.22191/2.71994, loss_mask_bce_7: 0.91218/1.16570, loss_mask_dice_7: 1.83158/2.37942, loss_spatial_bce_7: 0.44929/0.52392, loss_spatial_dice_7: 0.60192/0.64827, loss_spatial_ce_7: 0.38308/0.53443, loss_grounding_bce_7: 0.16477/0.21374, loss_grounding_dice_7: 0.50351/0.44410, loss_grounding_ce_7: 3.70534/3.35470, loss_mask_ce_8: 2.83014/2.86997, loss_mask_bce_8: 1.05708/1.30251, loss_mask_dice_8: 2.18626/2.68287, loss_spatial_bce_8: 0.59344/0.62225, loss_spatial_dice_8: 0.72402/0.73156, loss_spatial_ce_8: 0.70427/0.76455, loss_grounding_bce_8: 0.19941/0.23002, loss_grounding_dice_8: 0.59140/0.51184, loss_grounding_ce_8: 3.86552/4.01166, loss_mask_ce_9: 2.36122/2.56393, loss_mask_bce_9: 1.54807/1.68728, loss_mask_dice_9: 3.00092/3.15182, loss_spatial_bce_9: 0.57525/0.56409, loss_spatial_dice_9: 0.68984/0.73533, loss_spatial_ce_9: 0.89493/1.00000, loss_grounding_bce_9: 0.22250/0.34009, loss_grounding_dice_9: 0.71013/0.60561, loss_grounding_ce_9: 2.45369/2.97687] items per batch[4] items per second[2.47] total items[16] mini batches[     4] memory[10887] epoch remaining[0:08:23]\n",
            "INFO:trainer.default_trainer:epochs[     0] optim steps[5] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 1.44713/2.42454, loss_mask_bce_0: 0.27706/1.01731, loss_mask_dice_0: 2.06895/2.37278, loss_spatial_bce_0: 0.23037/0.43848, loss_spatial_dice_0: 0.44359/0.59375, loss_spatial_ce_0: 0.36015/0.53772, loss_grounding_bce_0: 0.03418/0.19942, loss_grounding_dice_0: 0.24447/0.41666, loss_grounding_ce_0: 2.55191/2.72731, loss_mask_ce_1: 1.24164/2.25504, loss_mask_bce_1: 0.28698/1.01094, loss_mask_dice_1: 1.76366/2.23074, loss_spatial_bce_1: 0.24653/0.45587, loss_spatial_dice_1: 0.46188/0.58792, loss_spatial_ce_1: 0.39852/0.54001, loss_grounding_bce_1: 0.04234/0.18635, loss_grounding_dice_1: 0.32432/0.42919, loss_grounding_ce_1: 2.31069/2.85106, loss_mask_ce_2: 1.31335/2.36801, loss_mask_bce_2: 0.26552/1.01988, loss_mask_dice_2: 1.90223/2.27049, loss_spatial_bce_2: 0.19578/0.41009, loss_spatial_dice_2: 0.45812/0.59133, loss_spatial_ce_2: 0.36035/0.54475, loss_grounding_bce_2: 0.03470/0.18127, loss_grounding_dice_2: 0.22361/0.40303, loss_grounding_ce_2: 2.25962/2.84004, loss_mask_ce_3: 1.52716/2.45549, loss_mask_bce_3: 0.39886/1.06257, loss_mask_dice_3: 2.00510/2.26149, loss_spatial_bce_3: 0.16533/0.42732, loss_spatial_dice_3: 0.44668/0.60167, loss_spatial_ce_3: 0.37315/0.54629, loss_grounding_bce_3: 0.03621/0.18169, loss_grounding_dice_3: 0.24665/0.40620, loss_grounding_ce_3: 2.41899/2.88020, loss_mask_ce_4: 1.70480/2.44973, loss_mask_bce_4: 0.35602/1.07372, loss_mask_dice_4: 1.83555/2.31265, loss_spatial_bce_4: 0.17881/0.42792, loss_spatial_dice_4: 0.43941/0.59515, loss_spatial_ce_4: 0.44920/0.64306, loss_grounding_bce_4: 0.05613/0.19386, loss_grounding_dice_4: 0.27444/0.41404, loss_grounding_ce_4: 2.97286/2.86179, loss_mask_ce_5: 1.67855/2.61936, loss_mask_bce_5: 0.31100/0.98430, loss_mask_dice_5: 1.92228/2.38074, loss_spatial_bce_5: 0.17188/0.44203, loss_spatial_dice_5: 0.41915/0.59838, loss_spatial_ce_5: 0.46965/0.62053, loss_grounding_bce_5: 0.04563/0.17811, loss_grounding_dice_5: 0.30967/0.42223, loss_grounding_ce_5: 3.15987/3.41132, loss_mask_ce_6: 1.90157/2.66151, loss_mask_bce_6: 0.31400/1.05216, loss_mask_dice_6: 1.92136/2.38809, loss_spatial_bce_6: 0.19495/0.45371, loss_spatial_dice_6: 0.45033/0.60026, loss_spatial_ce_6: 0.32668/0.51858, loss_grounding_bce_6: 0.07758/0.20072, loss_grounding_dice_6: 0.30042/0.42365, loss_grounding_ce_6: 3.11895/3.40606, loss_mask_ce_7: 1.80046/2.53605, loss_mask_bce_7: 0.25890/0.98434, loss_mask_dice_7: 2.00022/2.30358, loss_spatial_bce_7: 0.22594/0.46433, loss_spatial_dice_7: 0.53980/0.62657, loss_spatial_ce_7: 0.83932/0.59541, loss_grounding_bce_7: 0.04553/0.18010, loss_grounding_dice_7: 0.34564/0.42441, loss_grounding_ce_7: 3.27541/3.33884, loss_mask_ce_8: 1.94703/2.68539, loss_mask_bce_8: 0.64628/1.17126, loss_mask_dice_8: 2.09708/2.56571, loss_spatial_bce_8: 0.28439/0.55468, loss_spatial_dice_8: 0.63644/0.71253, loss_spatial_ce_8: 1.16882/0.84540, loss_grounding_bce_8: 0.05508/0.19503, loss_grounding_dice_8: 0.34929/0.47933, loss_grounding_ce_8: 3.22318/3.85397, loss_mask_ce_9: 2.27259/2.50566, loss_mask_bce_9: 0.80727/1.51128, loss_mask_dice_9: 2.99388/3.12023, loss_spatial_bce_9: 0.33472/0.51821, loss_spatial_dice_9: 0.71961/0.73219, loss_spatial_ce_9: 0.96516/0.99303, loss_grounding_bce_9: 0.18468/0.30901, loss_grounding_dice_9: 0.53694/0.59188, loss_grounding_ce_9: 2.93895/2.96929] items per batch[4] items per second[1.82] total items[20] mini batches[     5] memory[10887] epoch remaining[0:07:32]\n",
            "INFO:trainer.default_trainer:epochs[     0] optim steps[6] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 1.53288/2.27593, loss_mask_bce_0: 1.35404/1.07343, loss_mask_dice_0: 2.01763/2.31359, loss_spatial_bce_0: 0.40131/0.43229, loss_spatial_dice_0: 0.59490/0.59395, loss_spatial_ce_0: 0.38393/0.51209, loss_grounding_bce_0: 0.15068/0.19129, loss_grounding_dice_0: 0.37451/0.40963, loss_grounding_ce_0: 1.52563/2.52703, loss_mask_ce_1: 1.49122/2.12774, loss_mask_bce_1: 1.20240/1.04285, loss_mask_dice_1: 2.10663/2.21006, loss_spatial_bce_1: 0.34660/0.43766, loss_spatial_dice_1: 0.53678/0.57939, loss_spatial_ce_1: 0.42863/0.52144, loss_grounding_bce_1: 0.14915/0.18015, loss_grounding_dice_1: 0.38911/0.42251, loss_grounding_ce_1: 1.79517/2.67508, loss_mask_ce_2: 1.51982/2.22664, loss_mask_bce_2: 1.32295/1.07039, loss_mask_dice_2: 2.26250/2.26916, loss_spatial_bce_2: 0.33990/0.39839, loss_spatial_dice_2: 0.54314/0.58330, loss_spatial_ce_2: 0.47227/0.53267, loss_grounding_bce_2: 0.18603/0.18206, loss_grounding_dice_2: 0.37634/0.39858, loss_grounding_ce_2: 1.66917/2.64490, loss_mask_ce_3: 1.54874/2.30436, loss_mask_bce_3: 1.19631/1.08486, loss_mask_dice_3: 1.97354/2.21350, loss_spatial_bce_3: 0.35660/0.41553, loss_spatial_dice_3: 0.57005/0.59640, loss_spatial_ce_3: 0.50804/0.53991, loss_grounding_bce_3: 0.14825/0.17611, loss_grounding_dice_3: 0.34188/0.39548, loss_grounding_ce_3: 1.84786/2.70815, loss_mask_ce_4: 1.53074/2.29656, loss_mask_bce_4: 1.19638/1.09416, loss_mask_dice_4: 2.07021/2.27225, loss_spatial_bce_4: 0.37121/0.41846, loss_spatial_dice_4: 0.56710/0.59048, loss_spatial_ce_4: 0.51197/0.62121, loss_grounding_bce_4: 0.14891/0.18637, loss_grounding_dice_4: 0.36251/0.40545, loss_grounding_ce_4: 1.78841/2.68289, loss_mask_ce_5: 1.69483/2.46527, loss_mask_bce_5: 1.20991/1.02190, loss_mask_dice_5: 2.10733/2.33517, loss_spatial_bce_5: 0.35421/0.42739, loss_spatial_dice_5: 0.56903/0.59348, loss_spatial_ce_5: 0.55165/0.60905, loss_grounding_bce_5: 0.14862/0.17319, loss_grounding_dice_5: 0.36485/0.41266, loss_grounding_ce_5: 2.01630/3.17882, loss_mask_ce_6: 1.77598/2.51392, loss_mask_bce_6: 1.20483/1.07760, loss_mask_dice_6: 2.12538/2.34431, loss_spatial_bce_6: 0.37390/0.44041, loss_spatial_dice_6: 0.56516/0.59441, loss_spatial_ce_6: 0.39078/0.49728, loss_grounding_bce_6: 0.14484/0.19140, loss_grounding_dice_6: 0.34755/0.41096, loss_grounding_ce_6: 2.79235/3.30377, loss_mask_ce_7: 1.66486/2.39085, loss_mask_bce_7: 1.18394/1.01761, loss_mask_dice_7: 2.12915/2.27451, loss_spatial_bce_7: 0.38280/0.45074, loss_spatial_dice_7: 0.56980/0.61711, loss_spatial_ce_7: 0.50291/0.57999, loss_grounding_bce_7: 0.14562/0.17435, loss_grounding_dice_7: 0.37175/0.41563, loss_grounding_ce_7: 2.99605/3.28171, loss_mask_ce_8: 2.25623/2.61386, loss_mask_bce_8: 1.25698/1.18555, loss_mask_dice_8: 2.19903/2.50460, loss_spatial_bce_8: 0.51828/0.54861, loss_spatial_dice_8: 0.66188/0.70409, loss_spatial_ce_8: 0.58282/0.80164, loss_grounding_bce_8: 0.16499/0.19003, loss_grounding_dice_8: 0.47720/0.47898, loss_grounding_ce_8: 2.48598/3.62597, loss_mask_ce_9: 2.41467/2.49050, loss_mask_bce_9: 1.75028/1.55111, loss_mask_dice_9: 2.94184/3.09050, loss_spatial_bce_9: 0.47410/0.51086, loss_spatial_dice_9: 0.72999/0.73182, loss_spatial_ce_9: 0.89979/0.97749, loss_grounding_bce_9: 0.30641/0.30857, loss_grounding_dice_9: 0.56329/0.58711, loss_grounding_ce_9: 3.37442/3.03681] items per batch[4] items per second[2.42] total items[24] mini batches[     6] memory[10887] epoch remaining[0:06:46]\n",
            "INFO:trainer.default_trainer:epochs[     0] optim steps[7] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 1.39380/2.14991, loss_mask_bce_0: 0.36616/0.97239, loss_mask_dice_0: 2.23692/2.30264, loss_spatial_bce_0: 0.22736/0.40301, loss_spatial_dice_0: 0.58665/0.59290, loss_spatial_ce_0: 0.72308/0.54223, loss_grounding_bce_0: 0.03919/0.16956, loss_grounding_dice_0: 0.40385/0.40881, loss_grounding_ce_0: 2.22368/2.48369, loss_mask_ce_1: 1.42317/2.02708, loss_mask_bce_1: 0.32915/0.94089, loss_mask_dice_1: 2.00119/2.18022, loss_spatial_bce_1: 0.24299/0.40985, loss_spatial_dice_1: 0.60458/0.58299, loss_spatial_ce_1: 0.35546/0.49773, loss_grounding_bce_1: 0.03623/0.15959, loss_grounding_dice_1: 0.35926/0.41348, loss_grounding_ce_1: 2.55947/2.65856, loss_mask_ce_2: 1.42163/2.11164, loss_mask_bce_2: 0.42698/0.97848, loss_mask_dice_2: 2.07088/2.24083, loss_spatial_bce_2: 0.20048/0.37011, loss_spatial_dice_2: 0.60135/0.58588, loss_spatial_ce_2: 0.36335/0.50848, loss_grounding_bce_2: 0.04623/0.16266, loss_grounding_dice_2: 0.38343/0.39642, loss_grounding_ce_2: 2.46873/2.61973, loss_mask_ce_3: 1.47376/2.18571, loss_mask_bce_3: 0.51530/1.00350, loss_mask_dice_3: 1.97024/2.17875, loss_spatial_bce_3: 0.23093/0.38916, loss_spatial_dice_3: 0.59557/0.59628, loss_spatial_ce_3: 0.38741/0.51813, loss_grounding_bce_3: 0.05282/0.15850, loss_grounding_dice_3: 0.34930/0.38888, loss_grounding_ce_3: 2.56429/2.68759, loss_mask_ce_4: 1.55811/2.19107, loss_mask_bce_4: 0.47067/1.00509, loss_mask_dice_4: 2.11575/2.24989, loss_spatial_bce_4: 0.23181/0.39180, loss_spatial_dice_4: 0.60052/0.59191, loss_spatial_ce_4: 0.24451/0.56740, loss_grounding_bce_4: 0.08068/0.17127, loss_grounding_dice_4: 0.41251/0.40646, loss_grounding_ce_4: 2.72569/2.68901, loss_mask_ce_5: 1.53608/2.33253, loss_mask_bce_5: 0.35224/0.92623, loss_mask_dice_5: 2.14933/2.30862, loss_spatial_bce_5: 0.25017/0.40207, loss_spatial_dice_5: 0.61769/0.59694, loss_spatial_ce_5: 0.28317/0.56249, loss_grounding_bce_5: 0.06103/0.15717, loss_grounding_dice_5: 0.44627/0.41746, loss_grounding_ce_5: 2.61645/3.09848, loss_mask_ce_6: 1.70878/2.39890, loss_mask_bce_6: 0.39405/0.97995, loss_mask_dice_6: 2.18793/2.32197, loss_spatial_bce_6: 0.28033/0.41754, loss_spatial_dice_6: 0.60079/0.59532, loss_spatial_ce_6: 0.28922/0.46756, loss_grounding_bce_6: 0.02782/0.16803, loss_grounding_dice_6: 0.46438/0.41859, loss_grounding_ce_6: 2.63050/3.20759, loss_mask_ce_7: 2.02518/2.33861, loss_mask_bce_7: 0.41836/0.93200, loss_mask_dice_7: 2.09108/2.24830, loss_spatial_bce_7: 0.20867/0.41616, loss_spatial_dice_7: 0.60792/0.61580, loss_spatial_ce_7: 0.45575/0.56224, loss_grounding_bce_7: 0.02763/0.15339, loss_grounding_dice_7: 0.41374/0.41536, loss_grounding_ce_7: 2.52518/3.17363, loss_mask_ce_8: 2.49467/2.59683, loss_mask_bce_8: 0.40324/1.07379, loss_mask_dice_8: 2.41334/2.49156, loss_spatial_bce_8: 0.40411/0.52797, loss_spatial_dice_8: 0.73380/0.70834, loss_spatial_ce_8: 0.72965/0.79135, loss_grounding_bce_8: 0.05545/0.17080, loss_grounding_dice_8: 0.54530/0.48845, loss_grounding_ce_8: 2.35039/3.44374, loss_mask_ce_9: 2.25506/2.45687, loss_mask_bce_9: 0.94311/1.46426, loss_mask_dice_9: 3.20381/3.10668, loss_spatial_bce_9: 0.42192/0.49816, loss_spatial_dice_9: 0.78492/0.73941, loss_spatial_ce_9: 0.86921/0.96202, loss_grounding_bce_9: 0.16188/0.28762, loss_grounding_dice_9: 0.65867/0.59734, loss_grounding_ce_9: 3.67699/3.12826] items per batch[4] items per second[2.02] total items[28] mini batches[     7] memory[10887] epoch remaining[0:06:18]\n",
            "INFO:trainer.default_trainer:epochs[     0] optim steps[8] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 1.30161/2.04387, loss_mask_bce_0: 1.03028/0.97963, loss_mask_dice_0: 1.72152/2.23000, loss_spatial_bce_0: 0.39425/0.40192, loss_spatial_dice_0: 0.52414/0.58431, loss_spatial_ce_0: 0.42795/0.52795, loss_grounding_bce_0: 0.12644/0.16417, loss_grounding_dice_0: 0.32386/0.39819, loss_grounding_ce_0: 1.63328/2.37739, loss_mask_ce_1: 1.27616/1.93322, loss_mask_bce_1: 1.03225/0.95231, loss_mask_dice_1: 1.78896/2.13131, loss_spatial_bce_1: 0.34556/0.40181, loss_spatial_dice_1: 0.48078/0.57022, loss_spatial_ce_1: 0.37614/0.48253, loss_grounding_bce_1: 0.12058/0.15472, loss_grounding_dice_1: 0.32158/0.40199, loss_grounding_ce_1: 1.53436/2.51804, loss_mask_ce_2: 1.53675/2.03978, loss_mask_bce_2: 1.07604/0.99067, loss_mask_dice_2: 1.80994/2.18697, loss_spatial_bce_2: 0.38563/0.37205, loss_spatial_dice_2: 0.47629/0.57218, loss_spatial_ce_2: 0.40888/0.49603, loss_grounding_bce_2: 0.12945/0.15851, loss_grounding_dice_2: 0.34415/0.38988, loss_grounding_ce_2: 1.55575/2.48673, loss_mask_ce_3: 1.46608/2.09575, loss_mask_bce_3: 1.13569/1.02002, loss_mask_dice_3: 1.83995/2.13640, loss_spatial_bce_3: 0.41985/0.39300, loss_spatial_dice_3: 0.49542/0.58368, loss_spatial_ce_3: 0.49708/0.51549, loss_grounding_bce_3: 0.13332/0.15535, loss_grounding_dice_3: 0.35989/0.38526, loss_grounding_ce_3: 1.59635/2.55119, loss_mask_ce_4: 1.60958/2.11838, loss_mask_bce_4: 1.27756/1.03915, loss_mask_dice_4: 1.92106/2.20879, loss_spatial_bce_4: 0.30049/0.38039, loss_spatial_dice_4: 0.44087/0.57303, loss_spatial_ce_4: 0.53770/0.56369, loss_grounding_bce_4: 0.15085/0.16872, loss_grounding_dice_4: 0.36076/0.40075, loss_grounding_ce_4: 1.68944/2.56406, loss_mask_ce_5: 1.53524/2.23287, loss_mask_bce_5: 1.13518/0.95235, loss_mask_dice_5: 1.88594/2.25579, loss_spatial_bce_5: 0.30585/0.39005, loss_spatial_dice_5: 0.46375/0.58029, loss_spatial_ce_5: 0.58069/0.56477, loss_grounding_bce_5: 0.13710/0.15466, loss_grounding_dice_5: 0.35253/0.40935, loss_grounding_ce_5: 2.45602/3.01817, loss_mask_ce_6: 1.47537/2.28346, loss_mask_bce_6: 1.18331/1.00537, loss_mask_dice_6: 1.90499/2.26985, loss_spatial_bce_6: 0.39510/0.41473, loss_spatial_dice_6: 0.48113/0.58104, loss_spatial_ce_6: 0.50942/0.47279, loss_grounding_bce_6: 0.14531/0.16519, loss_grounding_dice_6: 0.33498/0.40814, loss_grounding_ce_6: 2.34234/3.09944, loss_mask_ce_7: 1.56419/2.24181, loss_mask_bce_7: 1.14177/0.95822, loss_mask_dice_7: 1.68318/2.17766, loss_spatial_bce_7: 0.35103/0.40801, loss_spatial_dice_7: 0.51756/0.60352, loss_spatial_ce_7: 0.32510/0.53260, loss_grounding_bce_7: 0.13584/0.15120, loss_grounding_dice_7: 0.31074/0.40228, loss_grounding_ce_7: 1.99722/3.02658, loss_mask_ce_8: 2.09989/2.53471, loss_mask_bce_8: 0.86615/1.04784, loss_mask_dice_8: 1.75791/2.39986, loss_spatial_bce_8: 0.48746/0.52291, loss_spatial_dice_8: 0.63736/0.69946, loss_spatial_ce_8: 0.90781/0.80591, loss_grounding_bce_8: 0.13188/0.16594, loss_grounding_dice_8: 0.34597/0.47064, loss_grounding_ce_8: 2.56404/3.33378, loss_mask_ce_9: 2.12416/2.41528, loss_mask_bce_9: 1.80929/1.50739, loss_mask_dice_9: 2.82946/3.07203, loss_spatial_bce_9: 0.59402/0.51014, loss_spatial_dice_9: 0.69765/0.73419, loss_spatial_ce_9: 0.89178/0.95324, loss_grounding_bce_9: 0.33461/0.29349, loss_grounding_dice_9: 0.55251/0.59173, loss_grounding_ce_9: 3.84814/3.21825] items per batch[4] items per second[1.55] total items[32] mini batches[     8] memory[10887] epoch remaining[0:06:06]\n",
            "INFO:trainer.default_trainer:epochs[     0] optim steps[9] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 1.43744/1.97649, loss_mask_bce_0: 0.43262/0.91885, loss_mask_dice_0: 1.97423/2.20158, loss_spatial_bce_0: 0.12736/0.37141, loss_spatial_dice_0: 0.49926/0.57486, loss_spatial_ce_0: 0.31499/0.50428, loss_grounding_bce_0: 0.07231/0.15397, loss_grounding_dice_0: 0.41717/0.40030, loss_grounding_ce_0: 1.74162/2.30675, loss_mask_ce_1: 1.57604/1.89353, loss_mask_bce_1: 0.46000/0.89761, loss_mask_dice_1: 2.13842/2.13210, loss_spatial_bce_1: 0.11535/0.36998, loss_spatial_dice_1: 0.48574/0.56083, loss_spatial_ce_1: 0.46793/0.48091, loss_grounding_bce_1: 0.07418/0.14577, loss_grounding_dice_1: 0.41806/0.40377, loss_grounding_ce_1: 1.95244/2.45520, loss_mask_ce_2: 1.61397/1.99247, loss_mask_bce_2: 0.61810/0.94927, loss_mask_dice_2: 2.13464/2.18115, loss_spatial_bce_2: 0.10120/0.34196, loss_spatial_dice_2: 0.48231/0.56219, loss_spatial_ce_2: 0.57537/0.50485, loss_grounding_bce_2: 0.06899/0.14856, loss_grounding_dice_2: 0.44361/0.39585, loss_grounding_ce_2: 2.85544/2.52770, loss_mask_ce_3: 1.55031/2.03515, loss_mask_bce_3: 0.62992/0.97668, loss_mask_dice_3: 2.25549/2.14963, loss_spatial_bce_3: 0.11398/0.36200, loss_spatial_dice_3: 0.48499/0.57271, loss_spatial_ce_3: 0.40406/0.50311, loss_grounding_bce_3: 0.09110/0.14821, loss_grounding_dice_3: 0.45695/0.39322, loss_grounding_ce_3: 2.81310/2.58029, loss_mask_ce_4: 1.50963/2.05074, loss_mask_bce_4: 0.54007/0.98370, loss_mask_dice_4: 2.01605/2.18737, loss_spatial_bce_4: 0.13314/0.35291, loss_spatial_dice_4: 0.50363/0.56532, loss_spatial_ce_4: 0.48841/0.55532, loss_grounding_bce_4: 0.05826/0.15645, loss_grounding_dice_4: 0.38873/0.39941, loss_grounding_ce_4: 2.27282/2.53170, loss_mask_ce_5: 1.28691/2.12776, loss_mask_bce_5: 0.61410/0.91477, loss_mask_dice_5: 2.02016/2.22961, loss_spatial_bce_5: 0.14574/0.36290, loss_spatial_dice_5: 0.52959/0.57466, loss_spatial_ce_5: 0.45852/0.55296, loss_grounding_bce_5: 0.07786/0.14613, loss_grounding_dice_5: 0.40133/0.40846, loss_grounding_ce_5: 2.16803/2.92371, loss_mask_ce_6: 1.37891/2.18296, loss_mask_bce_6: 0.53965/0.95363, loss_mask_dice_6: 2.13290/2.25463, loss_spatial_bce_6: 0.16160/0.38661, loss_spatial_dice_6: 0.49800/0.57182, loss_spatial_ce_6: 0.31231/0.45496, loss_grounding_bce_6: 0.08642/0.15644, loss_grounding_dice_6: 0.44159/0.41186, loss_grounding_ce_6: 2.23971/3.00391, loss_mask_ce_7: 1.43498/2.15216, loss_mask_bce_7: 0.49232/0.90645, loss_mask_dice_7: 1.90966/2.14789, loss_spatial_bce_7: 0.14609/0.37891, loss_spatial_dice_7: 0.51418/0.59359, loss_spatial_ce_7: 0.41028/0.51901, loss_grounding_bce_7: 0.05174/0.14015, loss_grounding_dice_7: 0.36877/0.39856, loss_grounding_ce_7: 2.19520/2.93420, loss_mask_ce_8: 2.12379/2.48906, loss_mask_bce_8: 0.64551/1.00313, loss_mask_dice_8: 2.10291/2.36686, loss_spatial_bce_8: 0.35700/0.50447, loss_spatial_dice_8: 0.63829/0.69267, loss_spatial_ce_8: 0.71183/0.79546, loss_grounding_bce_8: 0.05866/0.15402, loss_grounding_dice_8: 0.38725/0.46138, loss_grounding_ce_8: 2.84797/3.27980, loss_mask_ce_9: 1.96633/2.36539, loss_mask_bce_9: 0.73372/1.42142, loss_mask_dice_9: 2.86261/3.04876, loss_spatial_bce_9: 0.35291/0.49267, loss_spatial_dice_9: 0.66331/0.72631, loss_spatial_ce_9: 0.84781/0.94153, loss_grounding_bce_9: 0.13519/0.27590, loss_grounding_dice_9: 0.57433/0.58980, loss_grounding_ce_9: 2.95518/3.18902] items per batch[4] items per second[1.57] total items[36] mini batches[     9] memory[10887] epoch remaining[0:05:55]\n",
            "INFO:trainer.default_trainer:epochs[     0] optim steps[10] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.86827/1.86567, loss_mask_bce_0: 0.39112/0.86608, loss_mask_dice_0: 1.45246/2.12667, loss_spatial_bce_0: 0.25459/0.35973, loss_spatial_dice_0: 0.46504/0.56388, loss_spatial_ce_0: 0.22748/0.47660, loss_grounding_bce_0: 0.08142/0.14671, loss_grounding_dice_0: 0.33839/0.39411, loss_grounding_ce_0: 1.99491/2.27556, loss_mask_ce_1: 1.02422/1.80660, loss_mask_bce_1: 0.40033/0.84788, loss_mask_dice_1: 1.49497/2.06839, loss_spatial_bce_1: 0.24655/0.35764, loss_spatial_dice_1: 0.45806/0.55055, loss_spatial_ce_1: 0.16173/0.44899, loss_grounding_bce_1: 0.06827/0.13802, loss_grounding_dice_1: 0.33648/0.39705, loss_grounding_ce_1: 1.97349/2.40702, loss_mask_ce_2: 1.05998/1.89922, loss_mask_bce_2: 0.52253/0.90660, loss_mask_dice_2: 1.60111/2.12315, loss_spatial_bce_2: 0.27325/0.33509, loss_spatial_dice_2: 0.47978/0.55395, loss_spatial_ce_2: 0.21958/0.47632, loss_grounding_bce_2: 0.07218/0.14092, loss_grounding_dice_2: 0.34569/0.39084, loss_grounding_ce_2: 1.91464/2.46640, loss_mask_ce_3: 1.01226/1.93286, loss_mask_bce_3: 0.40943/0.91995, loss_mask_dice_3: 1.50878/2.08555, loss_spatial_bce_3: 0.27426/0.35322, loss_spatial_dice_3: 0.46986/0.56243, loss_spatial_ce_3: 0.20045/0.47285, loss_grounding_bce_3: 0.05921/0.13931, loss_grounding_dice_3: 0.32441/0.38634, loss_grounding_ce_3: 1.96793/2.51906, loss_mask_ce_4: 1.12194/1.95786, loss_mask_bce_4: 0.34258/0.91958, loss_mask_dice_4: 1.47282/2.11592, loss_spatial_bce_4: 0.28870/0.34649, loss_spatial_dice_4: 0.49663/0.55845, loss_spatial_ce_4: 0.16520/0.51631, loss_grounding_bce_4: 0.05066/0.14587, loss_grounding_dice_4: 0.30649/0.39012, loss_grounding_ce_4: 2.34269/2.51280, loss_mask_ce_5: 1.14751/2.02974, loss_mask_bce_5: 0.39325/0.86262, loss_mask_dice_5: 1.59436/2.16608, loss_spatial_bce_5: 0.28414/0.35502, loss_spatial_dice_5: 0.51660/0.56885, loss_spatial_ce_5: 0.25161/0.52283, loss_grounding_bce_5: 0.08213/0.13973, loss_grounding_dice_5: 0.36692/0.40430, loss_grounding_ce_5: 2.04658/2.83600, loss_mask_ce_6: 1.19762/2.08442, loss_mask_bce_6: 0.48979/0.90724, loss_mask_dice_6: 1.68294/2.19746, loss_spatial_bce_6: 0.32467/0.38041, loss_spatial_dice_6: 0.47894/0.56253, loss_spatial_ce_6: 0.23074/0.43254, loss_grounding_bce_6: 0.06491/0.14729, loss_grounding_dice_6: 0.36690/0.40736, loss_grounding_ce_6: 2.46880/2.95040, loss_mask_ce_7: 1.32145/2.06909, loss_mask_bce_7: 0.43679/0.85949, loss_mask_dice_7: 1.85932/2.11903, loss_spatial_bce_7: 0.30734/0.37176, loss_spatial_dice_7: 0.51833/0.58607, loss_spatial_ce_7: 0.45457/0.51257, loss_grounding_bce_7: 0.08046/0.13418, loss_grounding_dice_7: 0.39043/0.39775, loss_grounding_ce_7: 2.17446/2.85823, loss_mask_ce_8: 1.98034/2.43818, loss_mask_bce_8: 0.41163/0.94398, loss_mask_dice_8: 2.33306/2.36348, loss_spatial_bce_8: 0.61272/0.51530, loss_spatial_dice_8: 0.67611/0.69101, loss_spatial_ce_8: 0.58039/0.77395, loss_grounding_bce_8: 0.08132/0.14675, loss_grounding_dice_8: 0.52308/0.46755, loss_grounding_ce_8: 2.03640/3.15546, loss_mask_ce_9: 1.94234/2.32309, loss_mask_bce_9: 1.58361/1.43764, loss_mask_dice_9: 3.15155/3.05904, loss_spatial_bce_9: 0.58414/0.50182, loss_spatial_dice_9: 0.68023/0.72170, loss_spatial_ce_9: 0.84224/0.93160, loss_grounding_bce_9: 0.25823/0.27414, loss_grounding_dice_9: 0.68421/0.59924, loss_grounding_ce_9: 2.87240/3.15736] items per batch[4] items per second[1.42] total items[40] mini batches[    10] memory[10887] epoch remaining[0:05:49]\n",
            "INFO:trainer.default_trainer:epochs[     0] optim steps[100] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.71318/1.15504, loss_mask_bce_0: 1.49501/0.96532, loss_mask_dice_0: 1.03012/1.74643, loss_spatial_bce_0: 0.26005/0.32002, loss_spatial_dice_0: 0.20301/0.45312, loss_spatial_ce_0: 0.11292/0.39356, loss_grounding_bce_0: 0.09654/0.10789, loss_grounding_dice_0: 0.11278/0.29558, loss_grounding_ce_0: 1.44072/1.79360, loss_mask_ce_1: 0.69166/1.16404, loss_mask_bce_1: 1.39228/0.97919, loss_mask_dice_1: 0.95441/1.75907, loss_spatial_bce_1: 0.28389/0.32280, loss_spatial_dice_1: 0.20856/0.45417, loss_spatial_ce_1: 0.16681/0.40395, loss_grounding_bce_1: 0.09651/0.11151, loss_grounding_dice_1: 0.11593/0.29912, loss_grounding_ce_1: 1.58526/1.81126, loss_mask_ce_2: 0.71743/1.17499, loss_mask_bce_2: 1.36478/0.99358, loss_mask_dice_2: 0.94071/1.75557, loss_spatial_bce_2: 0.30584/0.31214, loss_spatial_dice_2: 0.22758/0.45767, loss_spatial_ce_2: 0.18471/0.41177, loss_grounding_bce_2: 0.09954/0.10689, loss_grounding_dice_2: 0.11516/0.29141, loss_grounding_ce_2: 1.49279/1.85026, loss_mask_ce_3: 0.63851/1.17597, loss_mask_bce_3: 1.23874/1.00276, loss_mask_dice_3: 0.96395/1.74334, loss_spatial_bce_3: 0.29930/0.31117, loss_spatial_dice_3: 0.22173/0.45453, loss_spatial_ce_3: 0.17206/0.40768, loss_grounding_bce_3: 0.09290/0.10400, loss_grounding_dice_3: 0.11659/0.28914, loss_grounding_ce_3: 1.44138/1.84073, loss_mask_ce_4: 0.70920/1.18527, loss_mask_bce_4: 1.26906/1.02107, loss_mask_dice_4: 0.97188/1.77000, loss_spatial_bce_4: 0.32511/0.31447, loss_spatial_dice_4: 0.22434/0.46586, loss_spatial_ce_4: 0.16541/0.43006, loss_grounding_bce_4: 0.10415/0.10615, loss_grounding_dice_4: 0.09563/0.29539, loss_grounding_ce_4: 1.60297/1.85469, loss_mask_ce_5: 0.75253/1.19708, loss_mask_bce_5: 1.24571/0.98233, loss_mask_dice_5: 0.95884/1.77092, loss_spatial_bce_5: 0.36724/0.33290, loss_spatial_dice_5: 0.26400/0.48271, loss_spatial_ce_5: 0.15464/0.41329, loss_grounding_bce_5: 0.09688/0.10293, loss_grounding_dice_5: 0.10241/0.29959, loss_grounding_ce_5: 1.53749/1.83360, loss_mask_ce_6: 0.68147/1.20632, loss_mask_bce_6: 1.29765/1.01884, loss_mask_dice_6: 1.04398/1.78677, loss_spatial_bce_6: 0.35771/0.35150, loss_spatial_dice_6: 0.29032/0.47669, loss_spatial_ce_6: 0.18569/0.42370, loss_grounding_bce_6: 0.10937/0.10568, loss_grounding_dice_6: 0.10818/0.29983, loss_grounding_ce_6: 1.38792/1.91076, loss_mask_ce_7: 0.65501/1.22959, loss_mask_bce_7: 1.22747/0.97865, loss_mask_dice_7: 0.94553/1.80269, loss_spatial_bce_7: 0.34553/0.33340, loss_spatial_dice_7: 0.35154/0.51452, loss_spatial_ce_7: 0.20542/0.52344, loss_grounding_bce_7: 0.09951/0.10446, loss_grounding_dice_7: 0.10389/0.30585, loss_grounding_ce_7: 1.32521/1.85528, loss_mask_ce_8: 0.69750/1.34177, loss_mask_bce_8: 1.54014/1.02702, loss_mask_dice_8: 1.07270/1.92629, loss_spatial_bce_8: 0.46535/0.44318, loss_spatial_dice_8: 0.51596/0.62953, loss_spatial_ce_8: 0.55997/0.77696, loss_grounding_bce_8: 0.14865/0.11025, loss_grounding_dice_8: 0.11799/0.32865, loss_grounding_ce_8: 1.29170/1.99195, loss_mask_ce_9: 0.65813/1.29784, loss_mask_bce_9: 1.61510/1.23465, loss_mask_dice_9: 1.70081/2.68769, loss_spatial_bce_9: 0.63187/0.49973, loss_spatial_dice_9: 0.61389/0.69034, loss_spatial_ce_9: 0.65174/1.11633, loss_grounding_bce_9: 0.20411/0.21466, loss_grounding_dice_9: 0.31517/0.50515, loss_grounding_ce_9: 0.59293/1.75059] items per batch[4] items per second[0.02] total items[400] mini batches[   100] memory[11531] epoch remaining[0:01:02]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:detectron2.data.common:Serializing 93 elements to byte tensors and concatenating them all ...\n",
            "INFO:detectron2.data.common:Serialized dataset takes 0.02 MiB\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0256 s/iter. Inference: 0.1941 s/iter. Eval: 0.0091 s/iter. Total: 0.2289 s/iter. ETA=0:00:08\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 35/47. Dataloading: 0.0091 s/iter. Inference: 0.1985 s/iter. Eval: 0.0106 s/iter. Total: 0.2183 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 43.734601356846625, 'fwIoU': 95.0381111731963, 'IoU-void': 97.45032415563426, 'IoU-liver': 54.64419317134792, 'IoU-right kidney': 1.7183557443546584, 'IoU-left kidney': 22.97952904981659, 'IoU-spleen': 41.88060466307969, 'mACC': 55.06204246737967, 'pACC': 97.01929315060114, 'ACC-void': 98.73899371429292, 'ACC-liver': 72.65808646865544, 'ACC-right kidney': 1.804702961993895, 'ACC-left kidney': 48.91841682672628, 'ACC-spleen': 53.19001236522985})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 43.734601356846625, 'fwIoU': 95.0381111731963, 'IoU-void': 97.45032415563426, 'IoU-liver': 54.64419317134792, 'IoU-right kidney': 1.7183557443546584, 'IoU-left kidney': 22.97952904981659, 'IoU-spleen': 41.88060466307969, 'mACC': 55.06204246737967, 'pACC': 97.01929315060114, 'ACC-void': 98.73899371429292, 'ACC-liver': 72.65808646865544, 'ACC-right kidney': 1.804702961993895, 'ACC-left kidney': 48.91841682672628, 'ACC-spleen': 53.19001236522985})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:44.076373\n",
            "INFO:trainer.default_trainer:PROGRESS: 3.33%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 1 training.\n",
            "INFO:trainer.default_trainer:epochs[     1] optim steps[200] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.98511/0.98730, loss_mask_bce_0: 1.02430/0.93576, loss_mask_dice_0: 1.31217/1.62045, loss_spatial_bce_0: 0.29290/0.29057, loss_spatial_dice_0: 0.31651/0.41685, loss_spatial_ce_0: 0.19015/0.33319, loss_grounding_bce_0: 0.10641/0.09993, loss_grounding_dice_0: 0.22732/0.26830, loss_grounding_ce_0: 1.21110/1.62080, loss_mask_ce_1: 0.99253/0.98853, loss_mask_bce_1: 0.98206/0.93705, loss_mask_dice_1: 1.19734/1.61560, loss_spatial_bce_1: 0.32027/0.29282, loss_spatial_dice_1: 0.32986/0.42037, loss_spatial_ce_1: 0.21319/0.34311, loss_grounding_bce_1: 0.09009/0.10196, loss_grounding_dice_1: 0.18965/0.26735, loss_grounding_ce_1: 1.35964/1.61338, loss_mask_ce_2: 1.04944/0.99122, loss_mask_bce_2: 0.96482/0.94327, loss_mask_dice_2: 1.21151/1.61968, loss_spatial_bce_2: 0.31373/0.28606, loss_spatial_dice_2: 0.32487/0.42478, loss_spatial_ce_2: 0.16740/0.35199, loss_grounding_bce_2: 0.09694/0.09949, loss_grounding_dice_2: 0.18955/0.26434, loss_grounding_ce_2: 1.41591/1.63691, loss_mask_ce_3: 1.02772/0.99452, loss_mask_bce_3: 0.96511/0.94471, loss_mask_dice_3: 1.20734/1.60753, loss_spatial_bce_3: 0.31719/0.28638, loss_spatial_dice_3: 0.33769/0.42243, loss_spatial_ce_3: 0.21181/0.33660, loss_grounding_bce_3: 0.08885/0.09724, loss_grounding_dice_3: 0.18257/0.26125, loss_grounding_ce_3: 1.40730/1.63985, loss_mask_ce_4: 1.13835/1.01554, loss_mask_bce_4: 1.00208/0.95840, loss_mask_dice_4: 1.36883/1.63570, loss_spatial_bce_4: 0.30850/0.29196, loss_spatial_dice_4: 0.37339/0.43824, loss_spatial_ce_4: 0.29908/0.35759, loss_grounding_bce_4: 0.09987/0.09787, loss_grounding_dice_4: 0.21990/0.26779, loss_grounding_ce_4: 1.31050/1.66490, loss_mask_ce_5: 1.32825/1.02494, loss_mask_bce_5: 1.01087/0.94215, loss_mask_dice_5: 1.36792/1.64545, loss_spatial_bce_5: 0.31399/0.30510, loss_spatial_dice_5: 0.34076/0.44626, loss_spatial_ce_5: 0.28276/0.36279, loss_grounding_bce_5: 0.08705/0.09598, loss_grounding_dice_5: 0.19264/0.27002, loss_grounding_ce_5: 1.44437/1.66731, loss_mask_ce_6: 1.17096/1.04500, loss_mask_bce_6: 1.11756/0.96198, loss_mask_dice_6: 1.41096/1.65652, loss_spatial_bce_6: 0.36068/0.31300, loss_spatial_dice_6: 0.36761/0.44353, loss_spatial_ce_6: 0.25721/0.37821, loss_grounding_bce_6: 0.11495/0.09883, loss_grounding_dice_6: 0.19422/0.27306, loss_grounding_ce_6: 1.59721/1.71169, loss_mask_ce_7: 1.18948/1.07592, loss_mask_bce_7: 1.02472/0.94073, loss_mask_dice_7: 1.31802/1.68158, loss_spatial_bce_7: 0.36770/0.31541, loss_spatial_dice_7: 0.39826/0.48856, loss_spatial_ce_7: 0.32249/0.46868, loss_grounding_bce_7: 0.07772/0.09711, loss_grounding_dice_7: 0.14380/0.27557, loss_grounding_ce_7: 1.76682/1.69475, loss_mask_ce_8: 1.21228/1.16408, loss_mask_bce_8: 1.12540/0.98070, loss_mask_dice_8: 1.44606/1.77660, loss_spatial_bce_8: 0.37602/0.41418, loss_spatial_dice_8: 0.59911/0.60572, loss_spatial_ce_8: 0.71216/0.72708, loss_grounding_bce_8: 0.11437/0.10372, loss_grounding_dice_8: 0.17895/0.29354, loss_grounding_ce_8: 1.99876/1.80489, loss_mask_ce_9: 0.65845/0.99112, loss_mask_bce_9: 1.69150/1.16236, loss_mask_dice_9: 2.22698/2.48595, loss_spatial_bce_9: 0.47992/0.49317, loss_spatial_dice_9: 0.66028/0.68084, loss_spatial_ce_9: 1.18798/1.11988, loss_grounding_bce_9: 0.22971/0.18145, loss_grounding_dice_9: 0.37658/0.45375, loss_grounding_ce_9: 1.11268/1.28905] items per batch[4] items per second[0.01] total items[800] mini batches[   200] memory[11531] epoch remaining[0:01:56]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0030 s/iter. Inference: 0.1939 s/iter. Eval: 0.0108 s/iter. Total: 0.2077 s/iter. ETA=0:00:07\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 36/47. Dataloading: 0.0026 s/iter. Inference: 0.1929 s/iter. Eval: 0.0103 s/iter. Total: 0.2059 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 51.06832491196308, 'fwIoU': 95.66721792155377, 'IoU-void': 97.8161175529935, 'IoU-liver': 58.729191966370855, 'IoU-right kidney': 18.40352495132698, 'IoU-left kidney': 30.26561654122208, 'IoU-spleen': 50.12717354790204, 'mACC': 63.95913471300193, 'pACC': 97.43390557283658, 'ACC-void': 99.08223695134618, 'ACC-liver': 68.04157629752767, 'ACC-right kidney': 30.12310891952635, 'ACC-left kidney': 54.57035800875343, 'ACC-spleen': 67.97839338785604})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 51.06832491196308, 'fwIoU': 95.66721792155377, 'IoU-void': 97.8161175529935, 'IoU-liver': 58.729191966370855, 'IoU-right kidney': 18.40352495132698, 'IoU-left kidney': 30.26561654122208, 'IoU-spleen': 50.12717354790204, 'mACC': 63.95913471300193, 'pACC': 97.43390557283658, 'ACC-void': 99.08223695134618, 'ACC-liver': 68.04157629752767, 'ACC-right kidney': 30.12310891952635, 'ACC-left kidney': 54.57035800875343, 'ACC-spleen': 67.97839338785604})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:25.261972\n",
            "INFO:trainer.default_trainer:PROGRESS: 6.67%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 2 training.\n",
            "INFO:trainer.default_trainer:epochs[     2] optim steps[300] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.53652/0.90427, loss_mask_bce_0: 0.27806/0.90782, loss_mask_dice_0: 1.00243/1.56909, loss_spatial_bce_0: 0.05305/0.27150, loss_spatial_dice_0: 0.23962/0.39857, loss_spatial_ce_0: 0.18129/0.30146, loss_grounding_bce_0: 0.01310/0.09386, loss_grounding_dice_0: 0.14145/0.26179, loss_grounding_ce_0: 0.89620/1.53912, loss_mask_ce_1: 0.45960/0.90803, loss_mask_bce_1: 0.30494/0.91548, loss_mask_dice_1: 1.39222/1.56617, loss_spatial_bce_1: 0.06122/0.27228, loss_spatial_dice_1: 0.30129/0.40317, loss_spatial_ce_1: 0.10401/0.31464, loss_grounding_bce_1: 0.01496/0.09596, loss_grounding_dice_1: 0.22491/0.26310, loss_grounding_ce_1: 0.88567/1.52789, loss_mask_ce_2: 0.41512/0.91501, loss_mask_bce_2: 0.27032/0.91657, loss_mask_dice_2: 1.02279/1.57154, loss_spatial_bce_2: 0.05741/0.26609, loss_spatial_dice_2: 0.27201/0.40783, loss_spatial_ce_2: 0.17926/0.32199, loss_grounding_bce_2: 0.01254/0.09436, loss_grounding_dice_2: 0.14564/0.26208, loss_grounding_ce_2: 1.05531/1.53312, loss_mask_ce_3: 0.47263/0.92081, loss_mask_bce_3: 0.29261/0.91621, loss_mask_dice_3: 1.16510/1.55167, loss_spatial_bce_3: 0.05585/0.26830, loss_spatial_dice_3: 0.28176/0.40432, loss_spatial_ce_3: 0.31000/0.30936, loss_grounding_bce_3: 0.01426/0.09266, loss_grounding_dice_3: 0.17095/0.25809, loss_grounding_ce_3: 0.82397/1.54904, loss_mask_ce_4: 0.67386/0.93659, loss_mask_bce_4: 0.36746/0.92959, loss_mask_dice_4: 1.07650/1.57773, loss_spatial_bce_4: 0.08818/0.27411, loss_spatial_dice_4: 0.35205/0.42177, loss_spatial_ce_4: 0.10171/0.33042, loss_grounding_bce_4: 0.01090/0.09345, loss_grounding_dice_4: 0.15354/0.26294, loss_grounding_ce_4: 1.23377/1.58546, loss_mask_ce_5: 0.40060/0.95023, loss_mask_bce_5: 0.51298/0.92258, loss_mask_dice_5: 1.48125/1.59246, loss_spatial_bce_5: 0.09634/0.28476, loss_spatial_dice_5: 0.32664/0.42936, loss_spatial_ce_5: 0.06020/0.33761, loss_grounding_bce_5: 0.01866/0.09218, loss_grounding_dice_5: 0.19465/0.26501, loss_grounding_ce_5: 0.86382/1.58313, loss_mask_ce_6: 0.43750/0.96532, loss_mask_bce_6: 0.50630/0.93382, loss_mask_dice_6: 1.44643/1.59345, loss_spatial_bce_6: 0.08814/0.29338, loss_spatial_dice_6: 0.29360/0.42479, loss_spatial_ce_6: 0.19502/0.34639, loss_grounding_bce_6: 0.01835/0.09524, loss_grounding_dice_6: 0.21620/0.26710, loss_grounding_ce_6: 0.77313/1.60881, loss_mask_ce_7: 0.48080/0.99228, loss_mask_bce_7: 0.42023/0.92359, loss_mask_dice_7: 1.41360/1.62067, loss_spatial_bce_7: 0.10183/0.29766, loss_spatial_dice_7: 0.36317/0.47000, loss_spatial_ce_7: 0.16963/0.44770, loss_grounding_bce_7: 0.01433/0.09315, loss_grounding_dice_7: 0.20055/0.26867, loss_grounding_ce_7: 0.86352/1.62519, loss_mask_ce_8: 0.47555/1.05643, loss_mask_bce_8: 0.36448/0.95706, loss_mask_dice_8: 1.27220/1.70719, loss_spatial_bce_8: 0.09319/0.39226, loss_spatial_dice_8: 0.40738/0.58778, loss_spatial_ce_8: 0.30795/0.69630, loss_grounding_bce_8: 0.01544/0.09970, loss_grounding_dice_8: 0.18174/0.28628, loss_grounding_ce_8: 0.88566/1.68452, loss_mask_ce_9: 0.49748/0.85230, loss_mask_bce_9: 0.29110/1.13322, loss_mask_dice_9: 1.64111/2.38640, loss_spatial_bce_9: 0.27925/0.48763, loss_spatial_dice_9: 0.57814/0.67465, loss_spatial_ce_9: 0.75115/1.10347, loss_grounding_bce_9: 0.04329/0.17088, loss_grounding_dice_9: 0.35413/0.43852, loss_grounding_ce_9: 0.17021/1.05336] items per batch[4] items per second[0.01] total items[1200] mini batches[   300] memory[11531] epoch remaining[0:03:03]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0036 s/iter. Inference: 0.1892 s/iter. Eval: 0.0133 s/iter. Total: 0.2061 s/iter. ETA=0:00:07\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 36/47. Dataloading: 0.0027 s/iter. Inference: 0.1917 s/iter. Eval: 0.0114 s/iter. Total: 0.2058 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 48.31480062114662, 'fwIoU': 95.02582744704911, 'IoU-void': 97.22849242432973, 'IoU-liver': 57.8164174946606, 'IoU-right kidney': 7.017028772753964, 'IoU-left kidney': 30.400306606578827, 'IoU-spleen': 49.11175780740999, 'mACC': 67.7835623635931, 'pACC': 96.84058207657627, 'ACC-void': 97.8307210641614, 'ACC-liver': 86.2522024095551, 'ACC-right kidney': 8.81889235517091, 'ACC-left kidney': 72.4298624012091, 'ACC-spleen': 73.58613358786906})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 48.31480062114662, 'fwIoU': 95.02582744704911, 'IoU-void': 97.22849242432973, 'IoU-liver': 57.8164174946606, 'IoU-right kidney': 7.017028772753964, 'IoU-left kidney': 30.400306606578827, 'IoU-spleen': 49.11175780740999, 'mACC': 67.7835623635931, 'pACC': 96.84058207657627, 'ACC-void': 97.8307210641614, 'ACC-liver': 86.2522024095551, 'ACC-right kidney': 8.81889235517091, 'ACC-left kidney': 72.4298624012091, 'ACC-spleen': 73.58613358786906})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:12.476058\n",
            "INFO:trainer.default_trainer:PROGRESS: 10.00%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 3 training.\n",
            "INFO:trainer.default_trainer:epochs[     3] optim steps[400] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.97506/0.84458, loss_mask_bce_0: 1.08446/0.89763, loss_mask_dice_0: 1.53638/1.51977, loss_spatial_bce_0: 0.39567/0.26226, loss_spatial_dice_0: 0.41070/0.38731, loss_spatial_ce_0: 0.06371/0.28037, loss_grounding_bce_0: 0.07813/0.09176, loss_grounding_dice_0: 0.16639/0.25283, loss_grounding_ce_0: 1.18192/1.46840, loss_mask_ce_1: 1.12555/0.84845, loss_mask_bce_1: 1.07147/0.90507, loss_mask_dice_1: 1.51633/1.52350, loss_spatial_bce_1: 0.41207/0.26319, loss_spatial_dice_1: 0.41227/0.39150, loss_spatial_ce_1: 0.05848/0.29139, loss_grounding_bce_1: 0.08585/0.09324, loss_grounding_dice_1: 0.15629/0.25474, loss_grounding_ce_1: 1.14829/1.45741, loss_mask_ce_2: 0.97978/0.86081, loss_mask_bce_2: 1.18438/0.90530, loss_mask_dice_2: 1.59837/1.52488, loss_spatial_bce_2: 0.35567/0.25848, loss_spatial_dice_2: 0.38298/0.39614, loss_spatial_ce_2: 0.15300/0.30164, loss_grounding_bce_2: 0.09061/0.09189, loss_grounding_dice_2: 0.14844/0.25368, loss_grounding_ce_2: 1.27978/1.46801, loss_mask_ce_3: 1.16432/0.86586, loss_mask_bce_3: 1.11079/0.90180, loss_mask_dice_3: 1.50975/1.50725, loss_spatial_bce_3: 0.35425/0.26015, loss_spatial_dice_3: 0.38625/0.39286, loss_spatial_ce_3: 0.15510/0.29029, loss_grounding_bce_3: 0.09738/0.09061, loss_grounding_dice_3: 0.15241/0.25037, loss_grounding_ce_3: 1.24806/1.48234, loss_mask_ce_4: 1.19081/0.88235, loss_mask_bce_4: 1.09668/0.91467, loss_mask_dice_4: 1.53666/1.53316, loss_spatial_bce_4: 0.35160/0.26656, loss_spatial_dice_4: 0.38313/0.41056, loss_spatial_ce_4: 0.22826/0.31287, loss_grounding_bce_4: 0.09686/0.09060, loss_grounding_dice_4: 0.15292/0.25408, loss_grounding_ce_4: 1.20967/1.51947, loss_mask_ce_5: 0.74609/0.89881, loss_mask_bce_5: 1.26150/0.91142, loss_mask_dice_5: 1.60558/1.54450, loss_spatial_bce_5: 0.35755/0.27641, loss_spatial_dice_5: 0.42784/0.41954, loss_spatial_ce_5: 0.20368/0.32524, loss_grounding_bce_5: 0.11321/0.09024, loss_grounding_dice_5: 0.17227/0.25623, loss_grounding_ce_5: 1.25363/1.53038, loss_mask_ce_6: 0.71272/0.91319, loss_mask_bce_6: 1.28683/0.91873, loss_mask_dice_6: 1.69784/1.54178, loss_spatial_bce_6: 0.40903/0.28506, loss_spatial_dice_6: 0.44460/0.41367, loss_spatial_ce_6: 0.16858/0.33091, loss_grounding_bce_6: 0.12113/0.09290, loss_grounding_dice_6: 0.20419/0.25717, loss_grounding_ce_6: 1.75110/1.55026, loss_mask_ce_7: 0.89631/0.94257, loss_mask_bce_7: 1.17159/0.91251, loss_mask_dice_7: 1.73257/1.57948, loss_spatial_bce_7: 0.45771/0.29189, loss_spatial_dice_7: 0.50792/0.45857, loss_spatial_ce_7: 0.35079/0.42184, loss_grounding_bce_7: 0.09252/0.09102, loss_grounding_dice_7: 0.21709/0.26121, loss_grounding_ce_7: 1.67041/1.58752, loss_mask_ce_8: 0.90184/0.98818, loss_mask_bce_8: 1.38373/0.94875, loss_mask_dice_8: 1.67146/1.66626, loss_spatial_bce_8: 0.41785/0.37896, loss_spatial_dice_8: 0.60288/0.57636, loss_spatial_ce_8: 0.60988/0.67690, loss_grounding_bce_8: 0.13352/0.09758, loss_grounding_dice_8: 0.21222/0.27792, loss_grounding_ce_8: 1.20483/1.63149, loss_mask_ce_9: 0.85166/0.77340, loss_mask_bce_9: 1.45982/1.12228, loss_mask_dice_9: 2.21701/2.31231, loss_spatial_bce_9: 0.58566/0.48594, loss_spatial_dice_9: 0.67235/0.66885, loss_spatial_ce_9: 1.20839/1.10720, loss_grounding_bce_9: 0.16703/0.16896, loss_grounding_dice_9: 0.27980/0.42553, loss_grounding_ce_9: 0.83769/0.96818] items per batch[4] items per second[0.02] total items[1600] mini batches[   400] memory[11531] epoch remaining[0:04:13]\n",
            "INFO:trainer.default_trainer:epochs[     3] optim steps[500] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.29341/0.80739, loss_mask_bce_0: 0.70957/0.87810, loss_mask_dice_0: 1.00776/1.48820, loss_spatial_bce_0: 0.16616/0.25420, loss_spatial_dice_0: 0.26801/0.37909, loss_spatial_ce_0: 0.09492/0.26695, loss_grounding_bce_0: 0.04247/0.08886, loss_grounding_dice_0: 0.13701/0.24813, loss_grounding_ce_0: 0.75621/1.42629, loss_mask_ce_1: 0.42762/0.81428, loss_mask_bce_1: 0.70249/0.88510, loss_mask_dice_1: 1.00931/1.49346, loss_spatial_bce_1: 0.16503/0.25484, loss_spatial_dice_1: 0.27481/0.38375, loss_spatial_ce_1: 0.08469/0.27810, loss_grounding_bce_1: 0.04271/0.09022, loss_grounding_dice_1: 0.14643/0.25015, loss_grounding_ce_1: 0.86610/1.41686, loss_mask_ce_2: 0.22043/0.82335, loss_mask_bce_2: 0.77400/0.88790, loss_mask_dice_2: 1.24223/1.49797, loss_spatial_bce_2: 0.16934/0.25187, loss_spatial_dice_2: 0.29695/0.38883, loss_spatial_ce_2: 0.10034/0.28797, loss_grounding_bce_2: 0.04183/0.08965, loss_grounding_dice_2: 0.16388/0.25006, loss_grounding_ce_2: 0.79675/1.42509, loss_mask_ce_3: 0.30663/0.82842, loss_mask_bce_3: 0.74766/0.88420, loss_mask_dice_3: 1.08061/1.48258, loss_spatial_bce_3: 0.17149/0.25225, loss_spatial_dice_3: 0.29159/0.38503, loss_spatial_ce_3: 0.07086/0.27602, loss_grounding_bce_3: 0.03886/0.08894, loss_grounding_dice_3: 0.13916/0.24746, loss_grounding_ce_3: 0.81023/1.42673, loss_mask_ce_4: 0.40720/0.84408, loss_mask_bce_4: 0.72506/0.89591, loss_mask_dice_4: 0.98769/1.51071, loss_spatial_bce_4: 0.17282/0.25802, loss_spatial_dice_4: 0.29367/0.40276, loss_spatial_ce_4: 0.14915/0.29766, loss_grounding_bce_4: 0.04460/0.08846, loss_grounding_dice_4: 0.15252/0.25115, loss_grounding_ce_4: 0.84211/1.46463, loss_mask_ce_5: 0.48549/0.86138, loss_mask_bce_5: 0.69681/0.89693, loss_mask_dice_5: 0.94202/1.52132, loss_spatial_bce_5: 0.16783/0.26680, loss_spatial_dice_5: 0.31104/0.41140, loss_spatial_ce_5: 0.11500/0.31447, loss_grounding_bce_5: 0.04023/0.08834, loss_grounding_dice_5: 0.14120/0.25317, loss_grounding_ce_5: 0.86186/1.48835, loss_mask_ce_6: 0.46795/0.87634, loss_mask_bce_6: 0.75112/0.90135, loss_mask_dice_6: 1.00924/1.51667, loss_spatial_bce_6: 0.16698/0.27658, loss_spatial_dice_6: 0.26610/0.40731, loss_spatial_ce_6: 0.07161/0.32437, loss_grounding_bce_6: 0.04267/0.09026, loss_grounding_dice_6: 0.16062/0.25323, loss_grounding_ce_6: 0.86424/1.51179, loss_mask_ce_7: 0.69119/0.90284, loss_mask_bce_7: 0.72044/0.89860, loss_mask_dice_7: 1.04183/1.55524, loss_spatial_bce_7: 0.23740/0.28456, loss_spatial_dice_7: 0.35317/0.45131, loss_spatial_ce_7: 0.23853/0.41680, loss_grounding_bce_7: 0.04673/0.08932, loss_grounding_dice_7: 0.17681/0.25770, loss_grounding_ce_7: 0.98172/1.53567, loss_mask_ce_8: 0.40021/0.94440, loss_mask_bce_8: 0.79750/0.93298, loss_mask_dice_8: 1.40849/1.64226, loss_spatial_bce_8: 0.31933/0.36955, loss_spatial_dice_8: 0.50194/0.56936, loss_spatial_ce_8: 0.32358/0.65833, loss_grounding_bce_8: 0.04815/0.09547, loss_grounding_dice_8: 0.19152/0.27567, loss_grounding_ce_8: 1.00401/1.56118, loss_mask_ce_9: 0.36686/0.71986, loss_mask_bce_9: 0.85342/1.10487, loss_mask_dice_9: 1.74774/2.26963, loss_spatial_bce_9: 0.38854/0.47829, loss_spatial_dice_9: 0.53921/0.66464, loss_spatial_ce_9: 0.68419/1.10325, loss_grounding_bce_9: 0.17219/0.16665, loss_grounding_dice_9: 0.38895/0.41742, loss_grounding_ce_9: 0.10985/0.89317] items per batch[4] items per second[0.02] total items[2000] mini batches[   500] memory[11531] epoch remaining[0:00:00]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0020 s/iter. Inference: 0.1914 s/iter. Eval: 0.0115 s/iter. Total: 0.2049 s/iter. ETA=0:00:07\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 36/47. Dataloading: 0.0026 s/iter. Inference: 0.1915 s/iter. Eval: 0.0109 s/iter. Total: 0.2051 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 53.27626225154645, 'fwIoU': 96.0108950784452, 'IoU-void': 98.01039617404476, 'IoU-liver': 62.45784806706411, 'IoU-right kidney': 21.159479528578238, 'IoU-left kidney': 29.17727546597734, 'IoU-spleen': 55.57631202206781, 'mACC': 64.73397026709017, 'pACC': 97.65152623899064, 'ACC-void': 99.03363981260829, 'ACC-liver': 77.05424745775919, 'ACC-right kidney': 31.25691858709872, 'ACC-left kidney': 47.26849082149941, 'ACC-spleen': 69.05655465648523})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 53.27626225154645, 'fwIoU': 96.0108950784452, 'IoU-void': 98.01039617404476, 'IoU-liver': 62.45784806706411, 'IoU-right kidney': 21.159479528578238, 'IoU-left kidney': 29.17727546597734, 'IoU-spleen': 55.57631202206781, 'mACC': 64.73397026709017, 'pACC': 97.65152623899064, 'ACC-void': 99.03363981260829, 'ACC-liver': 77.05424745775919, 'ACC-right kidney': 31.25691858709872, 'ACC-left kidney': 47.26849082149941, 'ACC-spleen': 69.05655465648523})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:38.774520\n",
            "INFO:trainer.default_trainer:PROGRESS: 13.33%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 4 training.\n",
            "INFO:trainer.default_trainer:epochs[     4] optim steps[600] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.14080/0.77010, loss_mask_bce_0: 0.80413/0.86360, loss_mask_dice_0: 0.71197/1.45984, loss_spatial_bce_0: 0.21915/0.24766, loss_spatial_dice_0: 0.16682/0.37109, loss_spatial_ce_0: 0.03462/0.25434, loss_grounding_bce_0: 0.08241/0.08736, loss_grounding_dice_0: 0.10983/0.24390, loss_grounding_ce_0: 0.24897/1.36206, loss_mask_ce_1: 0.12982/0.77522, loss_mask_bce_1: 0.79666/0.86876, loss_mask_dice_1: 0.71857/1.46829, loss_spatial_bce_1: 0.21490/0.24870, loss_spatial_dice_1: 0.16653/0.37624, loss_spatial_ce_1: 0.04195/0.26615, loss_grounding_bce_1: 0.07928/0.08869, loss_grounding_dice_1: 0.11639/0.24581, loss_grounding_ce_1: 0.47556/1.35217, loss_mask_ce_2: 0.17111/0.78442, loss_mask_bce_2: 0.84195/0.87128, loss_mask_dice_2: 0.79945/1.46976, loss_spatial_bce_2: 0.21971/0.24613, loss_spatial_dice_2: 0.17068/0.38142, loss_spatial_ce_2: 0.07262/0.27738, loss_grounding_bce_2: 0.09239/0.08799, loss_grounding_dice_2: 0.12306/0.24580, loss_grounding_ce_2: 0.52170/1.35821, loss_mask_ce_3: 0.16368/0.79013, loss_mask_bce_3: 0.84402/0.86890, loss_mask_dice_3: 0.82951/1.45700, loss_spatial_bce_3: 0.23492/0.24611, loss_spatial_dice_3: 0.18929/0.37708, loss_spatial_ce_3: 0.05609/0.26409, loss_grounding_bce_3: 0.08581/0.08771, loss_grounding_dice_3: 0.12940/0.24294, loss_grounding_ce_3: 0.51274/1.36195, loss_mask_ce_4: 0.16143/0.80891, loss_mask_bce_4: 0.84754/0.88199, loss_mask_dice_4: 0.83887/1.48478, loss_spatial_bce_4: 0.21436/0.25275, loss_spatial_dice_4: 0.17075/0.39530, loss_spatial_ce_4: 0.00809/0.27940, loss_grounding_bce_4: 0.08122/0.08749, loss_grounding_dice_4: 0.12337/0.24706, loss_grounding_ce_4: 0.54505/1.40135, loss_mask_ce_5: 0.26707/0.82838, loss_mask_bce_5: 0.89535/0.88496, loss_mask_dice_5: 0.84873/1.49254, loss_spatial_bce_5: 0.24096/0.26073, loss_spatial_dice_5: 0.20647/0.40279, loss_spatial_ce_5: 0.01018/0.29681, loss_grounding_bce_5: 0.09850/0.08782, loss_grounding_dice_5: 0.13350/0.24898, loss_grounding_ce_5: 0.59910/1.42177, loss_mask_ce_6: 0.25127/0.84500, loss_mask_bce_6: 0.77897/0.88782, loss_mask_dice_6: 0.73104/1.48741, loss_spatial_bce_6: 0.24367/0.27062, loss_spatial_dice_6: 0.20831/0.39993, loss_spatial_ce_6: 0.00881/0.30877, loss_grounding_bce_6: 0.08192/0.08903, loss_grounding_dice_6: 0.11417/0.24857, loss_grounding_ce_6: 0.61202/1.45537, loss_mask_ce_7: 0.29571/0.87123, loss_mask_bce_7: 0.73211/0.88632, loss_mask_dice_7: 0.72082/1.53249, loss_spatial_bce_7: 0.25379/0.27828, loss_spatial_dice_7: 0.19851/0.44378, loss_spatial_ce_7: 0.07308/0.40588, loss_grounding_bce_7: 0.07263/0.08813, loss_grounding_dice_7: 0.10967/0.25420, loss_grounding_ce_7: 0.51355/1.48666, loss_mask_ce_8: 0.41543/0.90768, loss_mask_bce_8: 0.73592/0.92468, loss_mask_dice_8: 0.70839/1.61964, loss_spatial_bce_8: 0.37024/0.36241, loss_spatial_dice_8: 0.43110/0.56176, loss_spatial_ce_8: 0.20428/0.64678, loss_grounding_bce_8: 0.08182/0.09455, loss_grounding_dice_8: 0.10527/0.27130, loss_grounding_ce_8: 0.78470/1.50564, loss_mask_ce_9: 0.46988/0.68206, loss_mask_bce_9: 1.03102/1.08926, loss_mask_dice_9: 1.10773/2.22669, loss_spatial_bce_9: 0.60203/0.47353, loss_spatial_dice_9: 0.54439/0.65932, loss_spatial_ce_9: 1.06427/1.10130, loss_grounding_bce_9: 0.18651/0.16669, loss_grounding_dice_9: 0.20928/0.41090, loss_grounding_ce_9: 0.54695/0.83016] items per batch[4] items per second[0.01] total items[2400] mini batches[   600] memory[11531] epoch remaining[0:00:59]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0025 s/iter. Inference: 0.1937 s/iter. Eval: 0.0116 s/iter. Total: 0.2078 s/iter. ETA=0:00:07\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 35/47. Dataloading: 0.0052 s/iter. Inference: 0.1971 s/iter. Eval: 0.0122 s/iter. Total: 0.2146 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 51.694310324261686, 'fwIoU': 94.6307380507085, 'IoU-void': 96.86666224972353, 'IoU-liver': 53.07592374700422, 'IoU-right kidney': 24.314230196583136, 'IoU-left kidney': 30.647050056422593, 'IoU-spleen': 53.56768537157497, 'mACC': 67.0085130160992, 'pACC': 96.6230023678876, 'ACC-void': 97.52548236417708, 'ACC-liver': 90.02367996121163, 'ACC-right kidney': 32.32028445875684, 'ACC-left kidney': 45.322585723731855, 'ACC-spleen': 69.8505325726186})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 51.694310324261686, 'fwIoU': 94.6307380507085, 'IoU-void': 96.86666224972353, 'IoU-liver': 53.07592374700422, 'IoU-right kidney': 24.314230196583136, 'IoU-left kidney': 30.647050056422593, 'IoU-spleen': 53.56768537157497, 'mACC': 67.0085130160992, 'pACC': 96.6230023678876, 'ACC-void': 97.52548236417708, 'ACC-liver': 90.02367996121163, 'ACC-right kidney': 32.32028445875684, 'ACC-left kidney': 45.322585723731855, 'ACC-spleen': 69.8505325726186})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:13.718039\n",
            "INFO:trainer.default_trainer:PROGRESS: 16.67%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 5 training.\n",
            "INFO:trainer.default_trainer:epochs[     5] optim steps[700] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.64210/0.74333, loss_mask_bce_0: 0.76797/0.85606, loss_mask_dice_0: 1.64155/1.44277, loss_spatial_bce_0: 0.22694/0.24328, loss_spatial_dice_0: 0.33686/0.36448, loss_spatial_ce_0: 0.21018/0.24452, loss_grounding_bce_0: 0.03381/0.08669, loss_grounding_dice_0: 0.11815/0.24016, loss_grounding_ce_0: 1.18576/1.31623, loss_mask_ce_1: 0.87695/0.74847, loss_mask_bce_1: 0.77519/0.86023, loss_mask_dice_1: 1.48184/1.45101, loss_spatial_bce_1: 0.23180/0.24352, loss_spatial_dice_1: 0.34760/0.36942, loss_spatial_ce_1: 0.28907/0.25929, loss_grounding_bce_1: 0.03018/0.08764, loss_grounding_dice_1: 0.09546/0.24182, loss_grounding_ce_1: 1.37401/1.31115, loss_mask_ce_2: 1.03591/0.76182, loss_mask_bce_2: 0.82425/0.86401, loss_mask_dice_2: 1.48906/1.45043, loss_spatial_bce_2: 0.24517/0.24142, loss_spatial_dice_2: 0.35159/0.37425, loss_spatial_ce_2: 0.41122/0.26921, loss_grounding_bce_2: 0.03531/0.08749, loss_grounding_dice_2: 0.11703/0.24109, loss_grounding_ce_2: 1.78024/1.31607, loss_mask_ce_3: 1.12695/0.76463, loss_mask_bce_3: 0.84102/0.86268, loss_mask_dice_3: 1.52851/1.43636, loss_spatial_bce_3: 0.24533/0.24197, loss_spatial_dice_3: 0.33874/0.37064, loss_spatial_ce_3: 0.33103/0.25420, loss_grounding_bce_3: 0.03569/0.08695, loss_grounding_dice_3: 0.11251/0.23830, loss_grounding_ce_3: 1.93030/1.32346, loss_mask_ce_4: 1.14078/0.78835, loss_mask_bce_4: 0.89172/0.87466, loss_mask_dice_4: 1.43393/1.46678, loss_spatial_bce_4: 0.24498/0.24870, loss_spatial_dice_4: 0.38152/0.38890, loss_spatial_ce_4: 0.30910/0.27119, loss_grounding_bce_4: 0.03870/0.08665, loss_grounding_dice_4: 0.10974/0.24267, loss_grounding_ce_4: 1.88139/1.36913, loss_mask_ce_5: 0.89559/0.81093, loss_mask_bce_5: 0.80355/0.87832, loss_mask_dice_5: 1.50064/1.47320, loss_spatial_bce_5: 0.24587/0.25645, loss_spatial_dice_5: 0.38970/0.39644, loss_spatial_ce_5: 0.38501/0.28636, loss_grounding_bce_5: 0.03589/0.08705, loss_grounding_dice_5: 0.11858/0.24391, loss_grounding_ce_5: 2.09965/1.39003, loss_mask_ce_6: 1.18577/0.82854, loss_mask_bce_6: 0.80479/0.88090, loss_mask_dice_6: 1.48468/1.46835, loss_spatial_bce_6: 0.23843/0.26648, loss_spatial_dice_6: 0.37880/0.39412, loss_spatial_ce_6: 0.29468/0.29839, loss_grounding_bce_6: 0.03871/0.08807, loss_grounding_dice_6: 0.13574/0.24395, loss_grounding_ce_6: 2.12395/1.41985, loss_mask_ce_7: 0.89885/0.85395, loss_mask_bce_7: 1.01023/0.88354, loss_mask_dice_7: 1.63508/1.51561, loss_spatial_bce_7: 0.28242/0.27724, loss_spatial_dice_7: 0.43273/0.43864, loss_spatial_ce_7: 0.38830/0.39872, loss_grounding_bce_7: 0.05667/0.08757, loss_grounding_dice_7: 0.13556/0.25002, loss_grounding_ce_7: 1.87265/1.44843, loss_mask_ce_8: 1.20468/0.88727, loss_mask_bce_8: 0.95666/0.92096, loss_mask_dice_8: 1.72480/1.60630, loss_spatial_bce_8: 0.31975/0.35697, loss_spatial_dice_8: 0.61272/0.55547, loss_spatial_ce_8: 0.60692/0.63422, loss_grounding_bce_8: 0.05810/0.09348, loss_grounding_dice_8: 0.13958/0.26616, loss_grounding_ce_8: 1.88427/1.48088, loss_mask_ce_9: 0.67504/0.65479, loss_mask_bce_9: 1.16290/1.07880, loss_mask_dice_9: 2.38664/2.19497, loss_spatial_bce_9: 0.43422/0.47314, loss_spatial_dice_9: 0.74802/0.65552, loss_spatial_ce_9: 1.50235/1.10493, loss_grounding_bce_9: 0.10233/0.16630, loss_grounding_dice_9: 0.26946/0.40307, loss_grounding_ce_9: 0.84918/0.79743] items per batch[4] items per second[0.02] total items[2800] mini batches[   700] memory[11531] epoch remaining[0:01:58]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0087 s/iter. Inference: 0.1995 s/iter. Eval: 0.0145 s/iter. Total: 0.2227 s/iter. ETA=0:00:08\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 34/47. Dataloading: 0.0075 s/iter. Inference: 0.2006 s/iter. Eval: 0.0131 s/iter. Total: 0.2213 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 52.7885216443453, 'fwIoU': 95.93357679293514, 'IoU-void': 97.90598319132422, 'IoU-liver': 63.43086850457729, 'IoU-right kidney': 11.642616576764054, 'IoU-left kidney': 33.654921491731656, 'IoU-spleen': 57.30821845732926, 'mACC': 67.72249265268992, 'pACC': 97.54231998374809, 'ACC-void': 98.65008743190675, 'ACC-liver': 83.74740797499534, 'ACC-right kidney': 14.263191439401563, 'ACC-left kidney': 71.06646934727165, 'ACC-spleen': 70.88530706987439})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 52.7885216443453, 'fwIoU': 95.93357679293514, 'IoU-void': 97.90598319132422, 'IoU-liver': 63.43086850457729, 'IoU-right kidney': 11.642616576764054, 'IoU-left kidney': 33.654921491731656, 'IoU-spleen': 57.30821845732926, 'mACC': 67.72249265268992, 'pACC': 97.54231998374809, 'ACC-void': 98.65008743190675, 'ACC-liver': 83.74740797499534, 'ACC-right kidney': 14.263191439401563, 'ACC-left kidney': 71.06646934727165, 'ACC-spleen': 70.88530706987439})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:11.284581\n",
            "INFO:trainer.default_trainer:PROGRESS: 20.00%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 6 training.\n",
            "INFO:trainer.default_trainer:epochs[     6] optim steps[800] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.31700/0.71871, loss_mask_bce_0: 0.24249/0.84615, loss_mask_dice_0: 0.65037/1.42641, loss_spatial_bce_0: 0.06823/0.23912, loss_spatial_dice_0: 0.15269/0.35910, loss_spatial_ce_0: 0.03770/0.23562, loss_grounding_bce_0: 0.01557/0.08518, loss_grounding_dice_0: 0.09996/0.23623, loss_grounding_ce_0: 1.35264/1.27300, loss_mask_ce_1: 0.34739/0.72713, loss_mask_bce_1: 0.22617/0.84960, loss_mask_dice_1: 0.52021/1.43390, loss_spatial_bce_1: 0.06740/0.23966, loss_spatial_dice_1: 0.15786/0.36385, loss_spatial_ce_1: 0.05770/0.24973, loss_grounding_bce_1: 0.01715/0.08593, loss_grounding_dice_1: 0.09821/0.23765, loss_grounding_ce_1: 1.47110/1.27168, loss_mask_ce_2: 0.41194/0.73891, loss_mask_bce_2: 0.19858/0.85387, loss_mask_dice_2: 0.50543/1.43362, loss_spatial_bce_2: 0.06347/0.23780, loss_spatial_dice_2: 0.14725/0.36823, loss_spatial_ce_2: 0.08204/0.25996, loss_grounding_bce_2: 0.01695/0.08624, loss_grounding_dice_2: 0.09969/0.23733, loss_grounding_ce_2: 1.30926/1.26711, loss_mask_ce_3: 0.45837/0.74373, loss_mask_bce_3: 0.19468/0.85168, loss_mask_dice_3: 0.46803/1.42076, loss_spatial_bce_3: 0.08661/0.23783, loss_spatial_dice_3: 0.17975/0.36432, loss_spatial_ce_3: 0.04757/0.24872, loss_grounding_bce_3: 0.01774/0.08558, loss_grounding_dice_3: 0.09713/0.23450, loss_grounding_ce_3: 1.36154/1.28021, loss_mask_ce_4: 0.38942/0.76789, loss_mask_bce_4: 0.20425/0.86375, loss_mask_dice_4: 0.41781/1.44791, loss_spatial_bce_4: 0.11733/0.24440, loss_spatial_dice_4: 0.19684/0.38323, loss_spatial_ce_4: 0.04999/0.26643, loss_grounding_bce_4: 0.01765/0.08502, loss_grounding_dice_4: 0.08948/0.23824, loss_grounding_ce_4: 1.31024/1.33086, loss_mask_ce_5: 0.38489/0.79128, loss_mask_bce_5: 0.22761/0.87044, loss_mask_dice_5: 0.59414/1.45649, loss_spatial_bce_5: 0.12993/0.25330, loss_spatial_dice_5: 0.21162/0.39153, loss_spatial_ce_5: 0.07039/0.27936, loss_grounding_bce_5: 0.02191/0.08576, loss_grounding_dice_5: 0.12049/0.23979, loss_grounding_ce_5: 1.28557/1.35456, loss_mask_ce_6: 0.36332/0.80874, loss_mask_bce_6: 0.23704/0.87171, loss_mask_dice_6: 0.49363/1.45254, loss_spatial_bce_6: 0.17606/0.26278, loss_spatial_dice_6: 0.21858/0.38917, loss_spatial_ce_6: 0.08966/0.29317, loss_grounding_bce_6: 0.01688/0.08647, loss_grounding_dice_6: 0.13306/0.23980, loss_grounding_ce_6: 1.04695/1.38061, loss_mask_ce_7: 0.49953/0.83314, loss_mask_bce_7: 0.27625/0.87972, loss_mask_dice_7: 0.75623/1.50121, loss_spatial_bce_7: 0.12803/0.27376, loss_spatial_dice_7: 0.22012/0.43324, loss_spatial_ce_7: 0.24770/0.39297, loss_grounding_bce_7: 0.01871/0.08627, loss_grounding_dice_7: 0.18741/0.24630, loss_grounding_ce_7: 1.38576/1.41319, loss_mask_ce_8: 0.61273/0.86816, loss_mask_bce_8: 0.32153/0.91769, loss_mask_dice_8: 0.63432/1.59315, loss_spatial_bce_8: 0.17846/0.35217, loss_spatial_dice_8: 0.36854/0.55031, loss_spatial_ce_8: 0.21738/0.62278, loss_grounding_bce_8: 0.02216/0.09224, loss_grounding_dice_8: 0.12627/0.26221, loss_grounding_ce_8: 0.77401/1.45022, loss_mask_ce_9: 0.26244/0.62860, loss_mask_bce_9: 0.61284/1.06992, loss_mask_dice_9: 1.24621/2.16660, loss_spatial_bce_9: 0.48137/0.47144, loss_spatial_dice_9: 0.59850/0.65215, loss_spatial_ce_9: 1.86352/1.10231, loss_grounding_bce_9: 0.15173/0.16623, loss_grounding_dice_9: 0.34737/0.39775, loss_grounding_ce_9: 0.10538/0.75810] items per batch[4] items per second[0.02] total items[3200] mini batches[   800] memory[11531] epoch remaining[0:03:03]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0020 s/iter. Inference: 0.1911 s/iter. Eval: 0.0134 s/iter. Total: 0.2065 s/iter. ETA=0:00:07\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 35/47. Dataloading: 0.0054 s/iter. Inference: 0.1958 s/iter. Eval: 0.0121 s/iter. Total: 0.2134 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 55.46814732290457, 'fwIoU': 96.1978009350438, 'IoU-void': 98.07238929288722, 'IoU-liver': 64.90059046115593, 'IoU-right kidney': 12.040497983930305, 'IoU-left kidney': 40.7949394143096, 'IoU-spleen': 61.532319462239805, 'mACC': 68.39626552191336, 'pACC': 97.78571654812666, 'ACC-void': 98.91206823696737, 'ACC-liver': 82.93700783992865, 'ACC-right kidney': 13.723122337392239, 'ACC-left kidney': 74.52375704524702, 'ACC-spleen': 71.88537215003146})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 55.46814732290457, 'fwIoU': 96.1978009350438, 'IoU-void': 98.07238929288722, 'IoU-liver': 64.90059046115593, 'IoU-right kidney': 12.040497983930305, 'IoU-left kidney': 40.7949394143096, 'IoU-spleen': 61.532319462239805, 'mACC': 68.39626552191336, 'pACC': 97.78571654812666, 'ACC-void': 98.91206823696737, 'ACC-liver': 82.93700783992865, 'ACC-right kidney': 13.723122337392239, 'ACC-left kidney': 74.52375704524702, 'ACC-spleen': 71.88537215003146})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:25.007911\n",
            "INFO:trainer.default_trainer:PROGRESS: 23.33%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 7 training.\n",
            "INFO:trainer.default_trainer:epochs[     7] optim steps[900] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.53801/0.70042, loss_mask_bce_0: 1.09595/0.82934, loss_mask_dice_0: 1.67279/1.40801, loss_spatial_bce_0: 0.35807/0.23438, loss_spatial_dice_0: 0.40842/0.35410, loss_spatial_ce_0: 0.19306/0.22947, loss_grounding_bce_0: 0.13508/0.08386, loss_grounding_dice_0: 0.38029/0.23391, loss_grounding_ce_0: 0.59729/1.23620, loss_mask_ce_1: 0.56060/0.71083, loss_mask_bce_1: 1.00691/0.83245, loss_mask_dice_1: 1.56568/1.41587, loss_spatial_bce_1: 0.34912/0.23532, loss_spatial_dice_1: 0.41122/0.35922, loss_spatial_ce_1: 0.19105/0.24301, loss_grounding_bce_1: 0.12415/0.08474, loss_grounding_dice_1: 0.35249/0.23558, loss_grounding_ce_1: 0.88423/1.24082, loss_mask_ce_2: 0.52910/0.72183, loss_mask_bce_2: 1.07200/0.83773, loss_mask_dice_2: 1.64780/1.41517, loss_spatial_bce_2: 0.33928/0.23388, loss_spatial_dice_2: 0.41193/0.36330, loss_spatial_ce_2: 0.23118/0.25326, loss_grounding_bce_2: 0.14171/0.08515, loss_grounding_dice_2: 0.38318/0.23548, loss_grounding_ce_2: 0.48489/1.23344, loss_mask_ce_3: 0.51016/0.72533, loss_mask_bce_3: 1.01857/0.83573, loss_mask_dice_3: 1.58022/1.40505, loss_spatial_bce_3: 0.30117/0.23438, loss_spatial_dice_3: 0.40441/0.35951, loss_spatial_ce_3: 0.21791/0.24239, loss_grounding_bce_3: 0.11205/0.08471, loss_grounding_dice_3: 0.32739/0.23285, loss_grounding_ce_3: 0.88295/1.24683, loss_mask_ce_4: 0.81157/0.75030, loss_mask_bce_4: 1.00896/0.84684, loss_mask_dice_4: 1.48087/1.43001, loss_spatial_bce_4: 0.31171/0.24044, loss_spatial_dice_4: 0.40331/0.37779, loss_spatial_ce_4: 0.24101/0.26044, loss_grounding_bce_4: 0.09893/0.08396, loss_grounding_dice_4: 0.28910/0.23634, loss_grounding_ce_4: 1.17040/1.30341, loss_mask_ce_5: 0.60710/0.77691, loss_mask_bce_5: 1.18878/0.85678, loss_mask_dice_5: 1.65571/1.43875, loss_spatial_bce_5: 0.37153/0.24902, loss_spatial_dice_5: 0.41725/0.38651, loss_spatial_ce_5: 0.21731/0.27114, loss_grounding_bce_5: 0.11027/0.08483, loss_grounding_dice_5: 0.31310/0.23750, loss_grounding_ce_5: 0.99773/1.33554, loss_mask_ce_6: 0.59836/0.79356, loss_mask_bce_6: 1.09945/0.85642, loss_mask_dice_6: 1.62512/1.43629, loss_spatial_bce_6: 0.34937/0.25867, loss_spatial_dice_6: 0.41457/0.38488, loss_spatial_ce_6: 0.27698/0.28726, loss_grounding_bce_6: 0.11038/0.08529, loss_grounding_dice_6: 0.33960/0.23814, loss_grounding_ce_6: 0.75686/1.35832, loss_mask_ce_7: 1.07691/0.81904, loss_mask_bce_7: 1.13597/0.86783, loss_mask_dice_7: 1.60350/1.48476, loss_spatial_bce_7: 0.42192/0.26956, loss_spatial_dice_7: 0.42528/0.42829, loss_spatial_ce_7: 0.39141/0.38526, loss_grounding_bce_7: 0.09746/0.08553, loss_grounding_dice_7: 0.31302/0.24451, loss_grounding_ce_7: 1.30120/1.39208, loss_mask_ce_8: 0.88421/0.85234, loss_mask_bce_8: 1.33606/0.90615, loss_mask_dice_8: 1.90087/1.57715, loss_spatial_bce_8: 0.42631/0.34719, loss_spatial_dice_8: 0.61109/0.54557, loss_spatial_ce_8: 0.55719/0.61145, loss_grounding_bce_8: 0.09085/0.09137, loss_grounding_dice_8: 0.32087/0.25995, loss_grounding_ce_8: 1.50587/1.43144, loss_mask_ce_9: 0.35414/0.60755, loss_mask_bce_9: 1.69965/1.05231, loss_mask_dice_9: 2.52184/2.13952, loss_spatial_bce_9: 0.54995/0.46580, loss_spatial_dice_9: 0.70945/0.64811, loss_spatial_ce_9: 1.48927/1.09566, loss_grounding_bce_9: 0.29230/0.16506, loss_grounding_dice_9: 0.50153/0.39356, loss_grounding_ce_9: 0.16972/0.72939] items per batch[4] items per second[0.02] total items[3600] mini batches[   900] memory[11531] epoch remaining[0:04:04]\n",
            "INFO:trainer.default_trainer:epochs[     7] optim steps[1000] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.24728/0.68784, loss_mask_bce_0: 0.20036/0.81603, loss_mask_dice_0: 0.96511/1.38744, loss_spatial_bce_0: 0.06619/0.23023, loss_spatial_dice_0: 0.22180/0.34963, loss_spatial_ce_0: 0.25018/0.22338, loss_grounding_bce_0: 0.02928/0.08299, loss_grounding_dice_0: 0.08455/0.23177, loss_grounding_ce_0: 0.02003/1.21801, loss_mask_ce_1: 0.17430/0.69772, loss_mask_bce_1: 0.20072/0.81798, loss_mask_dice_1: 0.95765/1.39659, loss_spatial_bce_1: 0.07244/0.23160, loss_spatial_dice_1: 0.22291/0.35495, loss_spatial_ce_1: 0.26867/0.23744, loss_grounding_bce_1: 0.03065/0.08367, loss_grounding_dice_1: 0.10176/0.23350, loss_grounding_ce_1: 0.03593/1.22172, loss_mask_ce_2: 0.22323/0.70709, loss_mask_bce_2: 0.20872/0.82387, loss_mask_dice_2: 0.89662/1.39661, loss_spatial_bce_2: 0.08299/0.23017, loss_spatial_dice_2: 0.20696/0.35901, loss_spatial_ce_2: 0.17211/0.24780, loss_grounding_bce_2: 0.02595/0.08419, loss_grounding_dice_2: 0.07105/0.23369, loss_grounding_ce_2: 0.45545/1.21601, loss_mask_ce_3: 0.21952/0.71092, loss_mask_bce_3: 0.19255/0.82222, loss_mask_dice_3: 0.91259/1.38835, loss_spatial_bce_3: 0.07447/0.23060, loss_spatial_dice_3: 0.22089/0.35528, loss_spatial_ce_3: 0.19525/0.23742, loss_grounding_bce_3: 0.02488/0.08389, loss_grounding_dice_3: 0.07273/0.23135, loss_grounding_ce_3: 0.16589/1.22826, loss_mask_ce_4: 0.25062/0.73673, loss_mask_bce_4: 0.19299/0.83358, loss_mask_dice_4: 0.84688/1.41386, loss_spatial_bce_4: 0.08094/0.23712, loss_spatial_dice_4: 0.22489/0.37348, loss_spatial_ce_4: 0.12574/0.25335, loss_grounding_bce_4: 0.02246/0.08317, loss_grounding_dice_4: 0.05968/0.23479, loss_grounding_ce_4: 0.17965/1.28222, loss_mask_ce_5: 0.34031/0.76291, loss_mask_bce_5: 0.16982/0.84424, loss_mask_dice_5: 0.82960/1.42002, loss_spatial_bce_5: 0.08776/0.24578, loss_spatial_dice_5: 0.22592/0.38210, loss_spatial_ce_5: 0.20739/0.26494, loss_grounding_bce_5: 0.02346/0.08411, loss_grounding_dice_5: 0.05112/0.23580, loss_grounding_ce_5: 0.37621/1.31172, loss_mask_ce_6: 0.40898/0.77827, loss_mask_bce_6: 0.19529/0.84378, loss_mask_dice_6: 0.91179/1.42194, loss_spatial_bce_6: 0.08161/0.25463, loss_spatial_dice_6: 0.22255/0.38107, loss_spatial_ce_6: 0.17734/0.28319, loss_grounding_bce_6: 0.02535/0.08461, loss_grounding_dice_6: 0.07132/0.23701, loss_grounding_ce_6: 0.46507/1.33168, loss_mask_ce_7: 0.39308/0.80624, loss_mask_bce_7: 0.19733/0.85734, loss_mask_dice_7: 0.96731/1.47075, loss_spatial_bce_7: 0.08513/0.26613, loss_spatial_dice_7: 0.26246/0.42398, loss_spatial_ce_7: 0.23254/0.37994, loss_grounding_bce_7: 0.02460/0.08489, loss_grounding_dice_7: 0.06120/0.24325, loss_grounding_ce_7: 0.99544/1.37961, loss_mask_ce_8: 0.45061/0.84512, loss_mask_bce_8: 0.20420/0.89565, loss_mask_dice_8: 1.04927/1.55959, loss_spatial_bce_8: 0.10438/0.34148, loss_spatial_dice_8: 0.41220/0.54196, loss_spatial_ce_8: 0.32653/0.60550, loss_grounding_bce_8: 0.02498/0.09085, loss_grounding_dice_8: 0.08824/0.25821, loss_grounding_ce_8: 0.57800/1.41696, loss_mask_ce_9: 0.45802/0.58999, loss_mask_bce_9: 0.20641/1.03778, loss_mask_dice_9: 1.15113/2.11744, loss_spatial_bce_9: 0.33294/0.46204, loss_spatial_dice_9: 0.55965/0.64568, loss_spatial_ce_9: 1.01597/1.09913, loss_grounding_bce_9: 0.05053/0.16411, loss_grounding_dice_9: 0.14913/0.39095, loss_grounding_ce_9: 0.38237/0.69857] items per batch[4] items per second[0.02] total items[4000] mini batches[  1000] memory[11531] epoch remaining[0:00:00]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0128 s/iter. Inference: 0.1975 s/iter. Eval: 0.0147 s/iter. Total: 0.2251 s/iter. ETA=0:00:08\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 34/47. Dataloading: 0.0090 s/iter. Inference: 0.2005 s/iter. Eval: 0.0142 s/iter. Total: 0.2238 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 51.589822188513935, 'fwIoU': 95.4796266422177, 'IoU-void': 97.54193312305749, 'IoU-liver': 60.65919758389342, 'IoU-right kidney': 10.258280158006684, 'IoU-left kidney': 35.910605035102385, 'IoU-spleen': 53.57909504250967, 'mACC': 70.34642834081609, 'pACC': 97.19891894135327, 'ACC-void': 98.10044591990162, 'ACC-liver': 88.34010831309497, 'ACC-right kidney': 11.324678809835294, 'ACC-left kidney': 85.20104537296514, 'ACC-spleen': 68.7658632882834})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 51.589822188513935, 'fwIoU': 95.4796266422177, 'IoU-void': 97.54193312305749, 'IoU-liver': 60.65919758389342, 'IoU-right kidney': 10.258280158006684, 'IoU-left kidney': 35.910605035102385, 'IoU-spleen': 53.57909504250967, 'mACC': 70.34642834081609, 'pACC': 97.19891894135327, 'ACC-void': 98.10044591990162, 'ACC-liver': 88.34010831309497, 'ACC-right kidney': 11.324678809835294, 'ACC-left kidney': 85.20104537296514, 'ACC-spleen': 68.7658632882834})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:07.253533\n",
            "INFO:trainer.default_trainer:PROGRESS: 26.67%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 8 training.\n",
            "INFO:trainer.default_trainer:epochs[     8] optim steps[1100] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.06216/0.67304, loss_mask_bce_0: 0.89937/0.80596, loss_mask_dice_0: 0.75429/1.37085, loss_spatial_bce_0: 0.22947/0.22678, loss_spatial_dice_0: 0.14627/0.34463, loss_spatial_ce_0: 0.00972/0.21704, loss_grounding_bce_0: 0.05340/0.08228, loss_grounding_dice_0: 0.07384/0.22929, loss_grounding_ce_0: 0.00872/1.19064, loss_mask_ce_1: 0.04898/0.68227, loss_mask_bce_1: 0.91591/0.80790, loss_mask_dice_1: 0.82075/1.38260, loss_spatial_bce_1: 0.26383/0.22819, loss_spatial_dice_1: 0.16102/0.34996, loss_spatial_ce_1: 0.02189/0.23138, loss_grounding_bce_1: 0.05466/0.08292, loss_grounding_dice_1: 0.07976/0.23114, loss_grounding_ce_1: 0.00729/1.19244, loss_mask_ce_2: 0.06461/0.69089, loss_mask_bce_2: 0.87699/0.81357, loss_mask_dice_2: 0.76338/1.38109, loss_spatial_bce_2: 0.27516/0.22708, loss_spatial_dice_2: 0.15921/0.35396, loss_spatial_ce_2: 0.05452/0.24137, loss_grounding_bce_2: 0.05709/0.08332, loss_grounding_dice_2: 0.07987/0.23113, loss_grounding_ce_2: 0.01192/1.19054, loss_mask_ce_3: 0.10890/0.69603, loss_mask_bce_3: 0.86917/0.81136, loss_mask_dice_3: 0.72742/1.37210, loss_spatial_bce_3: 0.24998/0.22775, loss_spatial_dice_3: 0.17753/0.35058, loss_spatial_ce_3: 0.10445/0.23247, loss_grounding_bce_3: 0.05599/0.08304, loss_grounding_dice_3: 0.07363/0.22873, loss_grounding_ce_3: 0.03168/1.20391, loss_mask_ce_4: 0.08815/0.72269, loss_mask_bce_4: 0.87958/0.82296, loss_mask_dice_4: 0.73504/1.39596, loss_spatial_bce_4: 0.25002/0.23376, loss_spatial_dice_4: 0.17244/0.36915, loss_spatial_ce_4: 0.06643/0.25050, loss_grounding_bce_4: 0.06283/0.08237, loss_grounding_dice_4: 0.08162/0.23233, loss_grounding_ce_4: 0.09967/1.26134, loss_mask_ce_5: 0.16431/0.74989, loss_mask_bce_5: 0.90086/0.83375, loss_mask_dice_5: 0.70978/1.40336, loss_spatial_bce_5: 0.25469/0.24222, loss_spatial_dice_5: 0.17299/0.37734, loss_spatial_ce_5: 0.09993/0.26213, loss_grounding_bce_5: 0.06241/0.08336, loss_grounding_dice_5: 0.07810/0.23332, loss_grounding_ce_5: 0.20476/1.28999, loss_mask_ce_6: 0.22420/0.76254, loss_mask_bce_6: 0.88176/0.83493, loss_mask_dice_6: 0.71023/1.40595, loss_spatial_bce_6: 0.25735/0.25181, loss_spatial_dice_6: 0.17328/0.37671, loss_spatial_ce_6: 0.08471/0.27750, loss_grounding_bce_6: 0.06739/0.08393, loss_grounding_dice_6: 0.08960/0.23453, loss_grounding_ce_6: 0.19821/1.30251, loss_mask_ce_7: 0.14330/0.79277, loss_mask_bce_7: 0.84255/0.85012, loss_mask_dice_7: 0.70606/1.45757, loss_spatial_bce_7: 0.30770/0.26293, loss_spatial_dice_7: 0.21716/0.41963, loss_spatial_ce_7: 0.09247/0.37347, loss_grounding_bce_7: 0.06330/0.08417, loss_grounding_dice_7: 0.08896/0.24120, loss_grounding_ce_7: 0.14704/1.35817, loss_mask_ce_8: 0.32887/0.83444, loss_mask_bce_8: 0.94136/0.89007, loss_mask_dice_8: 0.84732/1.54583, loss_spatial_bce_8: 0.48073/0.33810, loss_spatial_dice_8: 0.41929/0.53803, loss_spatial_ce_8: 0.48076/0.59819, loss_grounding_bce_8: 0.07071/0.09015, loss_grounding_dice_8: 0.11085/0.25535, loss_grounding_ce_8: 0.50158/1.40267, loss_mask_ce_9: 0.35941/0.57068, loss_mask_bce_9: 1.05302/1.02915, loss_mask_dice_9: 0.98961/2.10168, loss_spatial_bce_9: 0.45744/0.46001, loss_spatial_dice_9: 0.45427/0.64236, loss_spatial_ce_9: 0.57143/1.10282, loss_grounding_bce_9: 0.24520/0.16345, loss_grounding_dice_9: 0.19907/0.38722, loss_grounding_ce_9: 0.40591/0.67333] items per batch[4] items per second[0.02] total items[4400] mini batches[  1100] memory[11531] epoch remaining[0:00:59]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0020 s/iter. Inference: 0.1925 s/iter. Eval: 0.0107 s/iter. Total: 0.2052 s/iter. ETA=0:00:07\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 36/47. Dataloading: 0.0026 s/iter. Inference: 0.1920 s/iter. Eval: 0.0105 s/iter. Total: 0.2052 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 55.83211947294255, 'fwIoU': 96.23510553299896, 'IoU-void': 98.08057425797429, 'IoU-liver': 65.85374209085708, 'IoU-right kidney': 18.154260693836306, 'IoU-left kidney': 36.89747135519558, 'IoU-spleen': 60.17454896684949, 'mACC': 70.36179690500515, 'pACC': 97.76234348787222, 'ACC-void': 98.82940300096993, 'ACC-liver': 83.21882968177076, 'ACC-right kidney': 23.504746570058032, 'ACC-left kidney': 70.57212128845367, 'ACC-spleen': 75.68388398377334})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 55.83211947294255, 'fwIoU': 96.23510553299896, 'IoU-void': 98.08057425797429, 'IoU-liver': 65.85374209085708, 'IoU-right kidney': 18.154260693836306, 'IoU-left kidney': 36.89747135519558, 'IoU-spleen': 60.17454896684949, 'mACC': 70.36179690500515, 'pACC': 97.76234348787222, 'ACC-void': 98.82940300096993, 'ACC-liver': 83.21882968177076, 'ACC-right kidney': 23.504746570058032, 'ACC-left kidney': 70.57212128845367, 'ACC-spleen': 75.68388398377334})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:20.149239\n",
            "INFO:trainer.default_trainer:PROGRESS: 30.00%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 9 training.\n",
            "INFO:trainer.default_trainer:epochs[     9] optim steps[1200] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.50571/0.65871, loss_mask_bce_0: 0.54763/0.79547, loss_mask_dice_0: 1.06402/1.35835, loss_spatial_bce_0: 0.14126/0.22301, loss_spatial_dice_0: 0.30307/0.34129, loss_spatial_ce_0: 0.22270/0.21005, loss_grounding_bce_0: 0.05827/0.08153, loss_grounding_dice_0: 0.20359/0.22784, loss_grounding_ce_0: 1.08761/1.16879, loss_mask_ce_1: 0.55507/0.66826, loss_mask_bce_1: 0.54739/0.79786, loss_mask_dice_1: 1.09568/1.36990, loss_spatial_bce_1: 0.13496/0.22477, loss_spatial_dice_1: 0.28169/0.34674, loss_spatial_ce_1: 0.20989/0.22474, loss_grounding_bce_1: 0.06549/0.08214, loss_grounding_dice_1: 0.21086/0.22972, loss_grounding_ce_1: 1.32335/1.17951, loss_mask_ce_2: 0.57168/0.67701, loss_mask_bce_2: 0.52777/0.80283, loss_mask_dice_2: 1.03514/1.36873, loss_spatial_bce_2: 0.13829/0.22385, loss_spatial_dice_2: 0.26157/0.35043, loss_spatial_ce_2: 0.20194/0.23542, loss_grounding_bce_2: 0.05164/0.08243, loss_grounding_dice_2: 0.18395/0.22948, loss_grounding_ce_2: 1.24814/1.17517, loss_mask_ce_3: 0.58851/0.68322, loss_mask_bce_3: 0.53772/0.80014, loss_mask_dice_3: 0.99767/1.35960, loss_spatial_bce_3: 0.12621/0.22446, loss_spatial_dice_3: 0.25461/0.34703, loss_spatial_ce_3: 0.17636/0.22753, loss_grounding_bce_3: 0.06131/0.08216, loss_grounding_dice_3: 0.19429/0.22721, loss_grounding_ce_3: 1.19938/1.18719, loss_mask_ce_4: 0.55083/0.71092, loss_mask_bce_4: 0.56025/0.81215, loss_mask_dice_4: 1.09247/1.38507, loss_spatial_bce_4: 0.13297/0.23034, loss_spatial_dice_4: 0.28676/0.36584, loss_spatial_ce_4: 0.26926/0.24639, loss_grounding_bce_4: 0.05707/0.08163, loss_grounding_dice_4: 0.18687/0.23122, loss_grounding_ce_4: 1.18282/1.24683, loss_mask_ce_5: 0.54745/0.73645, loss_mask_bce_5: 0.60734/0.82344, loss_mask_dice_5: 1.00006/1.39282, loss_spatial_bce_5: 0.12989/0.23867, loss_spatial_dice_5: 0.27266/0.37379, loss_spatial_ce_5: 0.17944/0.25742, loss_grounding_bce_5: 0.07280/0.08274, loss_grounding_dice_5: 0.19466/0.23209, loss_grounding_ce_5: 1.31546/1.27529, loss_mask_ce_6: 0.58748/0.74833, loss_mask_bce_6: 0.58779/0.82470, loss_mask_dice_6: 1.01804/1.39699, loss_spatial_bce_6: 0.16801/0.24813, loss_spatial_dice_6: 0.31589/0.37265, loss_spatial_ce_6: 0.19526/0.27184, loss_grounding_bce_6: 0.06641/0.08311, loss_grounding_dice_6: 0.21106/0.23297, loss_grounding_ce_6: 1.50075/1.28573, loss_mask_ce_7: 0.56377/0.77880, loss_mask_bce_7: 0.61791/0.84150, loss_mask_dice_7: 1.10878/1.44810, loss_spatial_bce_7: 0.23190/0.26093, loss_spatial_dice_7: 0.41182/0.41712, loss_spatial_ce_7: 0.28686/0.36834, loss_grounding_bce_7: 0.07450/0.08364, loss_grounding_dice_7: 0.20131/0.23987, loss_grounding_ce_7: 0.94811/1.34017, loss_mask_ce_8: 0.74225/0.82425, loss_mask_bce_8: 0.67219/0.88311, loss_mask_dice_8: 1.12363/1.53548, loss_spatial_bce_8: 0.29723/0.33380, loss_spatial_dice_8: 0.56066/0.53513, loss_spatial_ce_8: 0.41440/0.59302, loss_grounding_bce_8: 0.06537/0.08930, loss_grounding_dice_8: 0.19170/0.25349, loss_grounding_ce_8: 1.10952/1.39303, loss_mask_ce_9: 0.31717/0.55164, loss_mask_bce_9: 0.89561/1.02020, loss_mask_dice_9: 1.67396/2.09062, loss_spatial_bce_9: 0.36795/0.45798, loss_spatial_dice_9: 0.66322/0.64041, loss_spatial_ce_9: 0.94680/1.10064, loss_grounding_bce_9: 0.13203/0.16271, loss_grounding_dice_9: 0.31069/0.38528, loss_grounding_ce_9: 0.50359/0.65092] items per batch[4] items per second[0.02] total items[4800] mini batches[  1200] memory[11531] epoch remaining[0:02:01]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0033 s/iter. Inference: 0.1899 s/iter. Eval: 0.0105 s/iter. Total: 0.2038 s/iter. ETA=0:00:07\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 35/47. Dataloading: 0.0043 s/iter. Inference: 0.1944 s/iter. Eval: 0.0121 s/iter. Total: 0.2109 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 53.887542101884975, 'fwIoU': 96.28281000023588, 'IoU-void': 98.08685852673608, 'IoU-liver': 69.2942958665801, 'IoU-right kidney': 17.911110124925113, 'IoU-left kidney': 28.954600549641775, 'IoU-spleen': 55.190845441541825, 'mACC': 70.25534929948105, 'pACC': 97.66251649220607, 'ACC-void': 98.67455061603135, 'ACC-liver': 85.9422416741343, 'ACC-right kidney': 27.077253362852638, 'ACC-left kidney': 75.96901665669574, 'ACC-spleen': 63.61368418769118})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 53.887542101884975, 'fwIoU': 96.28281000023588, 'IoU-void': 98.08685852673608, 'IoU-liver': 69.2942958665801, 'IoU-right kidney': 17.911110124925113, 'IoU-left kidney': 28.954600549641775, 'IoU-spleen': 55.190845441541825, 'mACC': 70.25534929948105, 'pACC': 97.66251649220607, 'ACC-void': 98.67455061603135, 'ACC-liver': 85.9422416741343, 'ACC-right kidney': 27.077253362852638, 'ACC-left kidney': 75.96901665669574, 'ACC-spleen': 63.61368418769118})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:09.361608\n",
            "INFO:trainer.default_trainer:PROGRESS: 33.33%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 10 training.\n",
            "INFO:trainer.default_trainer:epochs[    10] optim steps[1300] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.19669/0.64613, loss_mask_bce_0: 0.45596/0.78535, loss_mask_dice_0: 0.42429/1.34898, loss_spatial_bce_0: 0.15444/0.22006, loss_spatial_dice_0: 0.17120/0.33867, loss_spatial_ce_0: 0.08882/0.20516, loss_grounding_bce_0: 0.05862/0.08067, loss_grounding_dice_0: 0.03651/0.22626, loss_grounding_ce_0: 0.00325/1.15004, loss_mask_ce_1: 0.26734/0.65567, loss_mask_bce_1: 0.47796/0.78805, loss_mask_dice_1: 0.45009/1.36101, loss_spatial_bce_1: 0.17183/0.22192, loss_spatial_dice_1: 0.17535/0.34443, loss_spatial_ce_1: 0.08535/0.21902, loss_grounding_bce_1: 0.05379/0.08127, loss_grounding_dice_1: 0.03892/0.22808, loss_grounding_ce_1: 0.00109/1.16128, loss_mask_ce_2: 0.25350/0.66549, loss_mask_bce_2: 0.47508/0.79298, loss_mask_dice_2: 0.45497/1.35876, loss_spatial_bce_2: 0.17707/0.22142, loss_spatial_dice_2: 0.17918/0.34809, loss_spatial_ce_2: 0.08009/0.23073, loss_grounding_bce_2: 0.05399/0.08146, loss_grounding_dice_2: 0.03826/0.22772, loss_grounding_ce_2: 0.00089/1.16075, loss_mask_ce_3: 0.27375/0.67111, loss_mask_bce_3: 0.47428/0.79065, loss_mask_dice_3: 0.43655/1.34995, loss_spatial_bce_3: 0.17909/0.22222, loss_spatial_dice_3: 0.17246/0.34469, loss_spatial_ce_3: 0.08202/0.22403, loss_grounding_bce_3: 0.05559/0.08117, loss_grounding_dice_3: 0.03484/0.22539, loss_grounding_ce_3: 0.00089/1.17087, loss_mask_ce_4: 0.28829/0.69978, loss_mask_bce_4: 0.51534/0.80213, loss_mask_dice_4: 0.46611/1.37571, loss_spatial_bce_4: 0.20704/0.22766, loss_spatial_dice_4: 0.19635/0.36330, loss_spatial_ce_4: 0.10279/0.24414, loss_grounding_bce_4: 0.05431/0.08080, loss_grounding_dice_4: 0.03181/0.22955, loss_grounding_ce_4: 0.00115/1.23247, loss_mask_ce_5: 0.36869/0.72808, loss_mask_bce_5: 0.47317/0.81382, loss_mask_dice_5: 0.43268/1.38374, loss_spatial_bce_5: 0.23052/0.23597, loss_spatial_dice_5: 0.24272/0.37154, loss_spatial_ce_5: 0.08028/0.25407, loss_grounding_bce_5: 0.05063/0.08196, loss_grounding_dice_5: 0.03307/0.23047, loss_grounding_ce_5: 0.00089/1.26156, loss_mask_ce_6: 0.38930/0.73941, loss_mask_bce_6: 0.51732/0.81572, loss_mask_dice_6: 0.47305/1.38802, loss_spatial_bce_6: 0.22947/0.24578, loss_spatial_dice_6: 0.24691/0.37072, loss_spatial_ce_6: 0.07336/0.26672, loss_grounding_bce_6: 0.05499/0.08239, loss_grounding_dice_6: 0.03607/0.23145, loss_grounding_ce_6: 0.00033/1.27160, loss_mask_ce_7: 0.44618/0.77213, loss_mask_bce_7: 0.49652/0.83332, loss_mask_dice_7: 0.59858/1.44113, loss_spatial_bce_7: 0.21805/0.25965, loss_spatial_dice_7: 0.27421/0.41517, loss_spatial_ce_7: 0.09950/0.36437, loss_grounding_bce_7: 0.05657/0.08294, loss_grounding_dice_7: 0.04032/0.23859, loss_grounding_ce_7: 0.00014/1.32717, loss_mask_ce_8: 0.42677/0.81871, loss_mask_bce_8: 0.54023/0.87757, loss_mask_dice_8: 0.63441/1.52933, loss_spatial_bce_8: 0.37306/0.33065, loss_spatial_dice_8: 0.43501/0.53244, loss_spatial_ce_8: 0.45775/0.58732, loss_grounding_bce_8: 0.05730/0.08877, loss_grounding_dice_8: 0.03736/0.25259, loss_grounding_ce_8: 0.02603/1.38079, loss_mask_ce_9: 0.20559/0.53578, loss_mask_bce_9: 0.69693/1.01349, loss_mask_dice_9: 1.10643/2.08367, loss_spatial_bce_9: 0.52519/0.45523, loss_spatial_dice_9: 0.51093/0.63820, loss_spatial_ce_9: 1.00398/1.09694, loss_grounding_bce_9: 0.18934/0.16238, loss_grounding_dice_9: 0.09938/0.38344, loss_grounding_ce_9: 0.02263/0.63319] items per batch[4] items per second[0.02] total items[5200] mini batches[  1300] memory[11531] epoch remaining[0:02:52]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0013 s/iter. Inference: 0.1923 s/iter. Eval: 0.0102 s/iter. Total: 0.2037 s/iter. ETA=0:00:07\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 36/47. Dataloading: 0.0024 s/iter. Inference: 0.1917 s/iter. Eval: 0.0099 s/iter. Total: 0.2041 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 55.149597370215794, 'fwIoU': 96.06221934482497, 'IoU-void': 97.9108178072702, 'IoU-liver': 65.85785458010629, 'IoU-right kidney': 16.357653102175135, 'IoU-left kidney': 34.17385619239299, 'IoU-spleen': 61.44780516913437, 'mACC': 72.6990289167371, 'pACC': 97.56477832727526, 'ACC-void': 98.40561304825303, 'ACC-liver': 89.15786785109763, 'ACC-right kidney': 20.509208010465933, 'ACC-left kidney': 83.93841115904152, 'ACC-spleen': 71.48404451482743})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 55.149597370215794, 'fwIoU': 96.06221934482497, 'IoU-void': 97.9108178072702, 'IoU-liver': 65.85785458010629, 'IoU-right kidney': 16.357653102175135, 'IoU-left kidney': 34.17385619239299, 'IoU-spleen': 61.44780516913437, 'mACC': 72.6990289167371, 'pACC': 97.56477832727526, 'ACC-void': 98.40561304825303, 'ACC-liver': 89.15786785109763, 'ACC-right kidney': 20.509208010465933, 'ACC-left kidney': 83.93841115904152, 'ACC-spleen': 71.48404451482743})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:07.087360\n",
            "INFO:trainer.default_trainer:PROGRESS: 36.67%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 11 training.\n",
            "INFO:trainer.default_trainer:epochs[    11] optim steps[1400] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.56972/0.63530, loss_mask_bce_0: 0.56290/0.77788, loss_mask_dice_0: 1.03648/1.33612, loss_spatial_bce_0: 0.20431/0.21789, loss_spatial_dice_0: 0.35079/0.33564, loss_spatial_ce_0: 0.13038/0.19929, loss_grounding_bce_0: 0.14872/0.08018, loss_grounding_dice_0: 0.29046/0.22394, loss_grounding_ce_0: 0.36972/1.13125, loss_mask_ce_1: 0.57226/0.64475, loss_mask_bce_1: 0.60533/0.78034, loss_mask_dice_1: 1.14960/1.34779, loss_spatial_bce_1: 0.18332/0.22005, loss_spatial_dice_1: 0.32239/0.34168, loss_spatial_ce_1: 0.20074/0.21389, loss_grounding_bce_1: 0.14748/0.08065, loss_grounding_dice_1: 0.30575/0.22582, loss_grounding_ce_1: 0.36472/1.14456, loss_mask_ce_2: 0.60999/0.65418, loss_mask_bce_2: 0.63678/0.78548, loss_mask_dice_2: 1.24821/1.34556, loss_spatial_bce_2: 0.18024/0.21967, loss_spatial_dice_2: 0.32213/0.34484, loss_spatial_ce_2: 0.14359/0.22615, loss_grounding_bce_2: 0.16097/0.08088, loss_grounding_dice_2: 0.37859/0.22559, loss_grounding_ce_2: 0.00360/1.14251, loss_mask_ce_3: 0.64102/0.66044, loss_mask_bce_3: 0.62756/0.78362, loss_mask_dice_3: 1.15375/1.33717, loss_spatial_bce_3: 0.18621/0.22010, loss_spatial_dice_3: 0.30946/0.34121, loss_spatial_ce_3: 0.24878/0.22075, loss_grounding_bce_3: 0.14608/0.08070, loss_grounding_dice_3: 0.29465/0.22339, loss_grounding_ce_3: 0.41128/1.15376, loss_mask_ce_4: 0.83347/0.68866, loss_mask_bce_4: 0.70680/0.79578, loss_mask_dice_4: 1.24000/1.36393, loss_spatial_bce_4: 0.20776/0.22557, loss_spatial_dice_4: 0.38609/0.35991, loss_spatial_ce_4: 0.27969/0.24146, loss_grounding_bce_4: 0.14946/0.08050, loss_grounding_dice_4: 0.31657/0.22743, loss_grounding_ce_4: 0.35199/1.21580, loss_mask_ce_5: 0.75702/0.71769, loss_mask_bce_5: 0.79889/0.80736, loss_mask_dice_5: 1.20954/1.37163, loss_spatial_bce_5: 0.21206/0.23332, loss_spatial_dice_5: 0.36391/0.36824, loss_spatial_ce_5: 0.20681/0.25213, loss_grounding_bce_5: 0.14626/0.08146, loss_grounding_dice_5: 0.37751/0.22833, loss_grounding_ce_5: 0.03573/1.24387, loss_mask_ce_6: 0.86996/0.72668, loss_mask_bce_6: 0.71026/0.80955, loss_mask_dice_6: 1.18768/1.37672, loss_spatial_bce_6: 0.24482/0.24335, loss_spatial_dice_6: 0.37189/0.36750, loss_spatial_ce_6: 0.18023/0.26279, loss_grounding_bce_6: 0.14269/0.08195, loss_grounding_dice_6: 0.30911/0.22950, loss_grounding_ce_6: 0.30205/1.24914, loss_mask_ce_7: 0.88029/0.76252, loss_mask_bce_7: 0.74905/0.82795, loss_mask_dice_7: 1.29092/1.42854, loss_spatial_bce_7: 0.28086/0.25821, loss_spatial_dice_7: 0.41124/0.41187, loss_spatial_ce_7: 0.40285/0.36058, loss_grounding_bce_7: 0.16612/0.08260, loss_grounding_dice_7: 0.31491/0.23621, loss_grounding_ce_7: 0.25208/1.31293, loss_mask_ce_8: 1.06319/0.81276, loss_mask_bce_8: 0.94377/0.87450, loss_mask_dice_8: 1.61641/1.51894, loss_spatial_bce_8: 0.30085/0.32808, loss_spatial_dice_8: 0.50655/0.52883, loss_spatial_ce_8: 0.54482/0.58238, loss_grounding_bce_8: 0.16501/0.08830, loss_grounding_dice_8: 0.35951/0.25045, loss_grounding_ce_8: 0.30316/1.37505, loss_mask_ce_9: 0.29124/0.52066, loss_mask_bce_9: 1.08802/1.00899, loss_mask_dice_9: 2.21612/2.07327, loss_spatial_bce_9: 0.51213/0.45354, loss_spatial_dice_9: 0.71820/0.63609, loss_spatial_ce_9: 2.05055/1.09462, loss_grounding_bce_9: 0.20245/0.16217, loss_grounding_dice_9: 0.39857/0.38097, loss_grounding_ce_9: 0.05470/0.61663] items per batch[4] items per second[0.02] total items[5600] mini batches[  1400] memory[11531] epoch remaining[0:03:51]\n",
            "INFO:trainer.default_trainer:epochs[    11] optim steps[1500] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.26892/0.62575, loss_mask_bce_0: 0.25939/0.76993, loss_mask_dice_0: 0.46774/1.32286, loss_spatial_bce_0: 0.11235/0.21498, loss_spatial_dice_0: 0.18469/0.33224, loss_spatial_ce_0: 0.05082/0.19362, loss_grounding_bce_0: 0.03564/0.07922, loss_grounding_dice_0: 0.09128/0.22131, loss_grounding_ce_0: 0.39017/1.11623, loss_mask_ce_1: 0.46320/0.63613, loss_mask_bce_1: 0.26906/0.77256, loss_mask_dice_1: 0.51194/1.33401, loss_spatial_bce_1: 0.11417/0.21752, loss_spatial_dice_1: 0.21604/0.33852, loss_spatial_ce_1: 0.11382/0.20887, loss_grounding_bce_1: 0.03487/0.07971, loss_grounding_dice_1: 0.08280/0.22312, loss_grounding_ce_1: 0.40343/1.13231, loss_mask_ce_2: 0.29150/0.64604, loss_mask_bce_2: 0.26878/0.77767, loss_mask_dice_2: 0.50024/1.33184, loss_spatial_bce_2: 0.11992/0.21723, loss_spatial_dice_2: 0.20394/0.34157, loss_spatial_ce_2: 0.09252/0.22119, loss_grounding_bce_2: 0.03298/0.07994, loss_grounding_dice_2: 0.07639/0.22293, loss_grounding_ce_2: 0.42961/1.13236, loss_mask_ce_3: 0.35207/0.65234, loss_mask_bce_3: 0.26416/0.77524, loss_mask_dice_3: 0.47691/1.32378, loss_spatial_bce_3: 0.10828/0.21743, loss_spatial_dice_3: 0.18932/0.33776, loss_spatial_ce_3: 0.13060/0.21737, loss_grounding_bce_3: 0.03378/0.07973, loss_grounding_dice_3: 0.08901/0.22067, loss_grounding_ce_3: 0.48984/1.14402, loss_mask_ce_4: 0.28160/0.68166, loss_mask_bce_4: 0.28320/0.78865, loss_mask_dice_4: 0.52929/1.35038, loss_spatial_bce_4: 0.10637/0.22263, loss_spatial_dice_4: 0.18811/0.35625, loss_spatial_ce_4: 0.16878/0.23862, loss_grounding_bce_4: 0.03841/0.07968, loss_grounding_dice_4: 0.08976/0.22501, loss_grounding_ce_4: 0.37927/1.20301, loss_mask_ce_5: 0.47323/0.71055, loss_mask_bce_5: 0.29684/0.80056, loss_mask_dice_5: 0.45960/1.35908, loss_spatial_bce_5: 0.14607/0.23087, loss_spatial_dice_5: 0.20194/0.36548, loss_spatial_ce_5: 0.02420/0.24709, loss_grounding_bce_5: 0.03945/0.08068, loss_grounding_dice_5: 0.08224/0.22601, loss_grounding_ce_5: 0.46163/1.23124, loss_mask_ce_6: 0.51214/0.72050, loss_mask_bce_6: 0.27409/0.80199, loss_mask_dice_6: 0.56177/1.36233, loss_spatial_bce_6: 0.10215/0.24084, loss_spatial_dice_6: 0.16965/0.36407, loss_spatial_ce_6: 0.04552/0.25765, loss_grounding_bce_6: 0.03718/0.08112, loss_grounding_dice_6: 0.11169/0.22700, loss_grounding_ce_6: 0.61384/1.23606, loss_mask_ce_7: 0.51611/0.75624, loss_mask_bce_7: 0.31426/0.82115, loss_mask_dice_7: 0.61006/1.41622, loss_spatial_bce_7: 0.11813/0.25586, loss_spatial_dice_7: 0.28261/0.40882, loss_spatial_ce_7: 0.13149/0.35459, loss_grounding_bce_7: 0.04339/0.08187, loss_grounding_dice_7: 0.10931/0.23381, loss_grounding_ce_7: 0.53726/1.30412, loss_mask_ce_8: 0.59490/0.80882, loss_mask_bce_8: 0.28830/0.86912, loss_mask_dice_8: 0.67936/1.50672, loss_spatial_bce_8: 0.31944/0.32528, loss_spatial_dice_8: 0.41949/0.52614, loss_spatial_ce_8: 0.39054/0.57811, loss_grounding_bce_8: 0.03719/0.08755, loss_grounding_dice_8: 0.11680/0.24785, loss_grounding_ce_8: 0.58196/1.37028, loss_mask_ce_9: 0.34922/0.50844, loss_mask_bce_9: 0.40298/1.00166, loss_mask_dice_9: 1.29783/2.06068, loss_spatial_bce_9: 0.34929/0.45137, loss_spatial_dice_9: 0.52373/0.63462, loss_spatial_ce_9: 0.63216/1.09282, loss_grounding_bce_9: 0.13170/0.16101, loss_grounding_dice_9: 0.27063/0.37757, loss_grounding_ce_9: 0.08046/0.60339] items per batch[4] items per second[0.02] total items[6000] mini batches[  1500] memory[11531] epoch remaining[0:00:00]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0005 s/iter. Inference: 0.1921 s/iter. Eval: 0.0103 s/iter. Total: 0.2029 s/iter. ETA=0:00:07\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 35/47. Dataloading: 0.0037 s/iter. Inference: 0.1939 s/iter. Eval: 0.0121 s/iter. Total: 0.2099 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 54.980879475341624, 'fwIoU': 96.49288184375455, 'IoU-void': 98.23805106507174, 'IoU-liver': 70.41838037157851, 'IoU-right kidney': 10.020631645770512, 'IoU-left kidney': 35.85241277666252, 'IoU-spleen': 60.374921517624905, 'mACC': 71.4596313759855, 'pACC': 97.89590578253879, 'ACC-void': 98.86783696783627, 'ACC-liver': 85.97860578275909, 'ACC-right kidney': 10.590050652443729, 'ACC-left kidney': 88.85040460971693, 'ACC-spleen': 73.0112588671714})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 54.980879475341624, 'fwIoU': 96.49288184375455, 'IoU-void': 98.23805106507174, 'IoU-liver': 70.41838037157851, 'IoU-right kidney': 10.020631645770512, 'IoU-left kidney': 35.85241277666252, 'IoU-spleen': 60.374921517624905, 'mACC': 71.4596313759855, 'pACC': 97.89590578253879, 'ACC-void': 98.86783696783627, 'ACC-liver': 85.97860578275909, 'ACC-right kidney': 10.590050652443729, 'ACC-left kidney': 88.85040460971693, 'ACC-spleen': 73.0112588671714})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:05.969826\n",
            "INFO:trainer.default_trainer:PROGRESS: 40.00%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 12 training.\n",
            "INFO:trainer.default_trainer:epochs[    12] optim steps[1600] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.53779/0.61592, loss_mask_bce_0: 0.12344/0.76563, loss_mask_dice_0: 0.93111/1.31267, loss_spatial_bce_0: 0.03712/0.21267, loss_spatial_dice_0: 0.24284/0.32998, loss_spatial_ce_0: 0.02242/0.19037, loss_grounding_bce_0: 0.00860/0.07881, loss_grounding_dice_0: 0.21027/0.21979, loss_grounding_ce_0: 0.85431/1.09827, loss_mask_ce_1: 0.55482/0.62639, loss_mask_bce_1: 0.13054/0.76823, loss_mask_dice_1: 1.07590/1.32463, loss_spatial_bce_1: 0.03027/0.21542, loss_spatial_dice_1: 0.20419/0.33635, loss_spatial_ce_1: 0.02252/0.20516, loss_grounding_bce_1: 0.01145/0.07934, loss_grounding_dice_1: 0.23785/0.22157, loss_grounding_ce_1: 0.97942/1.11280, loss_mask_ce_2: 0.48766/0.63565, loss_mask_bce_2: 0.12709/0.77351, loss_mask_dice_2: 1.02120/1.32180, loss_spatial_bce_2: 0.04008/0.21537, loss_spatial_dice_2: 0.21573/0.33917, loss_spatial_ce_2: 0.06013/0.21784, loss_grounding_bce_2: 0.00787/0.07963, loss_grounding_dice_2: 0.19359/0.22135, loss_grounding_ce_2: 1.21780/1.11457, loss_mask_ce_3: 0.45112/0.64292, loss_mask_bce_3: 0.12539/0.77109, loss_mask_dice_3: 1.04425/1.31362, loss_spatial_bce_3: 0.04097/0.21605, loss_spatial_dice_3: 0.20002/0.33510, loss_spatial_ce_3: 0.06964/0.21423, loss_grounding_bce_3: 0.00761/0.07944, loss_grounding_dice_3: 0.18943/0.21912, loss_grounding_ce_3: 1.16329/1.12733, loss_mask_ce_4: 0.66262/0.67236, loss_mask_bce_4: 0.12389/0.78378, loss_mask_dice_4: 1.15967/1.34010, loss_spatial_bce_4: 0.04804/0.22117, loss_spatial_dice_4: 0.22431/0.35363, loss_spatial_ce_4: 0.12512/0.23727, loss_grounding_bce_4: 0.00651/0.07938, loss_grounding_dice_4: 0.21001/0.22354, loss_grounding_ce_4: 1.13850/1.18522, loss_mask_ce_5: 0.70487/0.70098, loss_mask_bce_5: 0.12844/0.79647, loss_mask_dice_5: 1.03500/1.34849, loss_spatial_bce_5: 0.05533/0.22918, loss_spatial_dice_5: 0.26256/0.36284, loss_spatial_ce_5: 0.13845/0.24329, loss_grounding_bce_5: 0.00724/0.08041, loss_grounding_dice_5: 0.20917/0.22443, loss_grounding_ce_5: 1.39913/1.21387, loss_mask_ce_6: 0.62184/0.71095, loss_mask_bce_6: 0.14002/0.79793, loss_mask_dice_6: 0.92915/1.35169, loss_spatial_bce_6: 0.06244/0.23949, loss_spatial_dice_6: 0.30175/0.36151, loss_spatial_ce_6: 0.10878/0.25490, loss_grounding_bce_6: 0.01274/0.08075, loss_grounding_dice_6: 0.20930/0.22556, loss_grounding_ce_6: 1.42891/1.21663, loss_mask_ce_7: 0.60061/0.74879, loss_mask_bce_7: 0.13697/0.81709, loss_mask_dice_7: 1.17235/1.40535, loss_spatial_bce_7: 0.04896/0.25437, loss_spatial_dice_7: 0.30676/0.40603, loss_spatial_ce_7: 0.13510/0.35041, loss_grounding_bce_7: 0.01054/0.08154, loss_grounding_dice_7: 0.21249/0.23220, loss_grounding_ce_7: 1.80929/1.29119, loss_mask_ce_8: 0.64483/0.80298, loss_mask_bce_8: 0.14540/0.86724, loss_mask_dice_8: 1.15941/1.49607, loss_spatial_bce_8: 0.11320/0.32262, loss_spatial_dice_8: 0.41922/0.52282, loss_spatial_ce_8: 0.19205/0.57295, loss_grounding_bce_8: 0.00940/0.08742, loss_grounding_dice_8: 0.22350/0.24642, loss_grounding_ce_8: 1.99771/1.35488, loss_mask_ce_9: 0.62214/0.49665, loss_mask_bce_9: 0.15638/0.99789, loss_mask_dice_9: 1.37597/2.05005, loss_spatial_bce_9: 0.16140/0.44981, loss_spatial_dice_9: 0.42809/0.63283, loss_spatial_ce_9: 0.80448/1.09480, loss_grounding_bce_9: 0.01211/0.16073, loss_grounding_dice_9: 0.32101/0.37568, loss_grounding_ce_9: 1.28091/0.58958] items per batch[4] items per second[0.02] total items[6400] mini batches[  1600] memory[11531] epoch remaining[0:01:00]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0020 s/iter. Inference: 0.1923 s/iter. Eval: 0.0109 s/iter. Total: 0.2052 s/iter. ETA=0:00:07\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 36/47. Dataloading: 0.0026 s/iter. Inference: 0.1920 s/iter. Eval: 0.0115 s/iter. Total: 0.2062 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 58.41870199304143, 'fwIoU': 96.86309889275744, 'IoU-void': 98.50432621832582, 'IoU-liver': 71.91860013967563, 'IoU-right kidney': 21.278855149048066, 'IoU-left kidney': 37.42956038994699, 'IoU-spleen': 62.962168068210644, 'mACC': 69.67076095582112, 'pACC': 98.18869705018874, 'ACC-void': 99.39248566563337, 'ACC-liver': 80.24476508352922, 'ACC-right kidney': 28.23118982925766, 'ACC-left kidney': 70.48080858969111, 'ACC-spleen': 70.00455561099422})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 58.41870199304143, 'fwIoU': 96.86309889275744, 'IoU-void': 98.50432621832582, 'IoU-liver': 71.91860013967563, 'IoU-right kidney': 21.278855149048066, 'IoU-left kidney': 37.42956038994699, 'IoU-spleen': 62.962168068210644, 'mACC': 69.67076095582112, 'pACC': 98.18869705018874, 'ACC-void': 99.39248566563337, 'ACC-liver': 80.24476508352922, 'ACC-right kidney': 28.23118982925766, 'ACC-left kidney': 70.48080858969111, 'ACC-spleen': 70.00455561099422})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:37.827575\n",
            "INFO:trainer.default_trainer:PROGRESS: 43.33%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 13 training.\n",
            "INFO:trainer.default_trainer:epochs[    13] optim steps[1700] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.57714/0.60577, loss_mask_bce_0: 0.75000/0.75740, loss_mask_dice_0: 0.96092/1.29850, loss_spatial_bce_0: 0.22360/0.21044, loss_spatial_dice_0: 0.33464/0.32699, loss_spatial_ce_0: 0.41979/0.18585, loss_grounding_bce_0: 0.08237/0.07816, loss_grounding_dice_0: 0.15674/0.21771, loss_grounding_ce_0: 0.49284/1.08014, loss_mask_ce_1: 0.53239/0.61550, loss_mask_bce_1: 0.76979/0.76009, loss_mask_dice_1: 1.00509/1.31114, loss_spatial_bce_1: 0.21849/0.21349, loss_spatial_dice_1: 0.34316/0.33331, loss_spatial_ce_1: 0.37427/0.20048, loss_grounding_bce_1: 0.07331/0.07869, loss_grounding_dice_1: 0.14273/0.21954, loss_grounding_ce_1: 0.45797/1.09421, loss_mask_ce_2: 0.42273/0.62606, loss_mask_bce_2: 0.78010/0.76492, loss_mask_dice_2: 0.91124/1.30807, loss_spatial_bce_2: 0.21710/0.21357, loss_spatial_dice_2: 0.32388/0.33604, loss_spatial_ce_2: 0.35011/0.21403, loss_grounding_bce_2: 0.07548/0.07888, loss_grounding_dice_2: 0.13615/0.21921, loss_grounding_ce_2: 0.40332/1.09776, loss_mask_ce_3: 0.42561/0.63289, loss_mask_bce_3: 0.76086/0.76260, loss_mask_dice_3: 0.95076/1.29932, loss_spatial_bce_3: 0.26232/0.21445, loss_spatial_dice_3: 0.32048/0.33182, loss_spatial_ce_3: 0.28828/0.21057, loss_grounding_bce_3: 0.06854/0.07874, loss_grounding_dice_3: 0.13196/0.21709, loss_grounding_ce_3: 0.41112/1.10975, loss_mask_ce_4: 0.34550/0.66215, loss_mask_bce_4: 0.84812/0.77553, loss_mask_dice_4: 1.04728/1.32688, loss_spatial_bce_4: 0.21850/0.21914, loss_spatial_dice_4: 0.31452/0.35026, loss_spatial_ce_4: 0.32721/0.23552, loss_grounding_bce_4: 0.08936/0.07882, loss_grounding_dice_4: 0.15172/0.22151, loss_grounding_ce_4: 0.46781/1.16991, loss_mask_ce_5: 0.45638/0.69102, loss_mask_bce_5: 0.84228/0.78865, loss_mask_dice_5: 1.03051/1.33522, loss_spatial_bce_5: 0.27867/0.22732, loss_spatial_dice_5: 0.33977/0.35946, loss_spatial_ce_5: 0.20543/0.23944, loss_grounding_bce_5: 0.08142/0.07997, loss_grounding_dice_5: 0.15016/0.22254, loss_grounding_ce_5: 0.63802/1.19709, loss_mask_ce_6: 0.73807/0.70179, loss_mask_bce_6: 0.75307/0.78948, loss_mask_dice_6: 0.92671/1.33819, loss_spatial_bce_6: 0.24614/0.23765, loss_spatial_dice_6: 0.30418/0.35821, loss_spatial_ce_6: 0.20118/0.25101, loss_grounding_bce_6: 0.06561/0.08013, loss_grounding_dice_6: 0.14275/0.22371, loss_grounding_ce_6: 0.68169/1.19682, loss_mask_ce_7: 0.85187/0.74243, loss_mask_bce_7: 0.84031/0.80935, loss_mask_dice_7: 1.08876/1.39145, loss_spatial_bce_7: 0.40992/0.25260, loss_spatial_dice_7: 0.38794/0.40285, loss_spatial_ce_7: 0.32333/0.34549, loss_grounding_bce_7: 0.08864/0.08104, loss_grounding_dice_7: 0.19493/0.23031, loss_grounding_ce_7: 0.99276/1.27487, loss_mask_ce_8: 0.82989/0.79901, loss_mask_bce_8: 1.20507/0.86149, loss_mask_dice_8: 1.33079/1.48354, loss_spatial_bce_8: 0.59846/0.31996, loss_spatial_dice_8: 0.50466/0.51951, loss_spatial_ce_8: 0.88932/0.56651, loss_grounding_bce_8: 0.10144/0.08692, loss_grounding_dice_8: 0.22096/0.24458, loss_grounding_ce_8: 1.03552/1.34402, loss_mask_ce_9: 0.16719/0.48660, loss_mask_bce_9: 1.49271/0.99201, loss_mask_dice_9: 2.12047/2.03686, loss_spatial_bce_9: 0.59256/0.44812, loss_spatial_dice_9: 0.61956/0.63103, loss_spatial_ce_9: 1.28962/1.09536, loss_grounding_bce_9: 0.17092/0.15997, loss_grounding_dice_9: 0.34397/0.37286, loss_grounding_ce_9: 0.19747/0.57945] items per batch[4] items per second[0.01] total items[6800] mini batches[  1700] memory[11531] epoch remaining[0:02:03]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0088 s/iter. Inference: 0.2018 s/iter. Eval: 0.0138 s/iter. Total: 0.2244 s/iter. ETA=0:00:08\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 36/47. Dataloading: 0.0038 s/iter. Inference: 0.1944 s/iter. Eval: 0.0117 s/iter. Total: 0.2100 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 57.98385091781579, 'fwIoU': 96.75565044256425, 'IoU-void': 98.43545517247847, 'IoU-liver': 71.2962383195642, 'IoU-right kidney': 25.933241726783464, 'IoU-left kidney': 38.682387527561026, 'IoU-spleen': 55.57193184269178, 'mACC': 71.66182361616882, 'pACC': 98.1033963022508, 'ACC-void': 99.16203932202768, 'ACC-liver': 83.79762507738195, 'ACC-right kidney': 30.38811177082285, 'ACC-left kidney': 77.88973204445983, 'ACC-spleen': 67.07160986615182})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 57.98385091781579, 'fwIoU': 96.75565044256425, 'IoU-void': 98.43545517247847, 'IoU-liver': 71.2962383195642, 'IoU-right kidney': 25.933241726783464, 'IoU-left kidney': 38.682387527561026, 'IoU-spleen': 55.57193184269178, 'mACC': 71.66182361616882, 'pACC': 98.1033963022508, 'ACC-void': 99.16203932202768, 'ACC-liver': 83.79762507738195, 'ACC-right kidney': 30.38811177082285, 'ACC-left kidney': 77.88973204445983, 'ACC-spleen': 67.07160986615182})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:11.653349\n",
            "INFO:trainer.default_trainer:PROGRESS: 46.67%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 14 training.\n",
            "INFO:trainer.default_trainer:epochs[    14] optim steps[1800] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.44673/0.59871, loss_mask_bce_0: 0.16473/0.75182, loss_mask_dice_0: 0.81724/1.29232, loss_spatial_bce_0: 0.08046/0.20817, loss_spatial_dice_0: 0.22188/0.32474, loss_spatial_ce_0: 0.03346/0.18269, loss_grounding_bce_0: 0.01271/0.07767, loss_grounding_dice_0: 0.09068/0.21649, loss_grounding_ce_0: 0.60984/1.06165, loss_mask_ce_1: 0.51081/0.60850, loss_mask_bce_1: 0.16850/0.75392, loss_mask_dice_1: 0.82807/1.30430, loss_spatial_bce_1: 0.06808/0.21148, loss_spatial_dice_1: 0.23824/0.33134, loss_spatial_ce_1: 0.03881/0.19706, loss_grounding_bce_1: 0.01506/0.07814, loss_grounding_dice_1: 0.10831/0.21836, loss_grounding_ce_1: 0.93380/1.07828, loss_mask_ce_2: 0.45207/0.61882, loss_mask_bce_2: 0.17097/0.75914, loss_mask_dice_2: 0.93440/1.30110, loss_spatial_bce_2: 0.07493/0.21161, loss_spatial_dice_2: 0.21434/0.33381, loss_spatial_ce_2: 0.13061/0.21082, loss_grounding_bce_2: 0.01510/0.07845, loss_grounding_dice_2: 0.09119/0.21794, loss_grounding_ce_2: 0.96438/1.08367, loss_mask_ce_3: 0.44774/0.62631, loss_mask_bce_3: 0.18831/0.75695, loss_mask_dice_3: 0.88742/1.29254, loss_spatial_bce_3: 0.06616/0.21292, loss_spatial_dice_3: 0.20459/0.32958, loss_spatial_ce_3: 0.12810/0.20738, loss_grounding_bce_3: 0.01450/0.07827, loss_grounding_dice_3: 0.10889/0.21580, loss_grounding_ce_3: 0.80077/1.09532, loss_mask_ce_4: 0.59048/0.65401, loss_mask_bce_4: 0.19596/0.76986, loss_mask_dice_4: 0.97771/1.31991, loss_spatial_bce_4: 0.05557/0.21793, loss_spatial_dice_4: 0.23673/0.34793, loss_spatial_ce_4: 0.13810/0.23406, loss_grounding_bce_4: 0.01497/0.07835, loss_grounding_dice_4: 0.15435/0.22016, loss_grounding_ce_4: 0.75893/1.15395, loss_mask_ce_5: 0.54136/0.68322, loss_mask_bce_5: 0.18166/0.78325, loss_mask_dice_5: 0.90958/1.32868, loss_spatial_bce_5: 0.06184/0.22580, loss_spatial_dice_5: 0.25793/0.35697, loss_spatial_ce_5: 0.26163/0.23675, loss_grounding_bce_5: 0.01658/0.07967, loss_grounding_dice_5: 0.09570/0.22129, loss_grounding_ce_5: 1.01280/1.18132, loss_mask_ce_6: 0.49878/0.69458, loss_mask_bce_6: 0.20914/0.78471, loss_mask_dice_6: 1.08122/1.33155, loss_spatial_bce_6: 0.07048/0.23552, loss_spatial_dice_6: 0.25369/0.35571, loss_spatial_ce_6: 0.08897/0.24763, loss_grounding_bce_6: 0.01416/0.07984, loss_grounding_dice_6: 0.14472/0.22258, loss_grounding_ce_6: 1.18501/1.18358, loss_mask_ce_7: 0.56308/0.73610, loss_mask_bce_7: 0.28542/0.80448, loss_mask_dice_7: 1.04717/1.38493, loss_spatial_bce_7: 0.07878/0.25107, loss_spatial_dice_7: 0.32621/0.40103, loss_spatial_ce_7: 0.26855/0.34261, loss_grounding_bce_7: 0.01836/0.08076, loss_grounding_dice_7: 0.12820/0.22921, loss_grounding_ce_7: 1.35316/1.26195, loss_mask_ce_8: 0.57541/0.79370, loss_mask_bce_8: 0.26580/0.85816, loss_mask_dice_8: 0.91134/1.47877, loss_spatial_bce_8: 0.11289/0.31815, loss_spatial_dice_8: 0.39956/0.51763, loss_spatial_ce_8: 0.25572/0.56107, loss_grounding_bce_8: 0.01504/0.08654, loss_grounding_dice_8: 0.09553/0.24333, loss_grounding_ce_8: 1.32585/1.33547, loss_mask_ce_9: 0.33011/0.47648, loss_mask_bce_9: 0.34030/0.98863, loss_mask_dice_9: 1.63287/2.03194, loss_spatial_bce_9: 0.36454/0.44728, loss_spatial_dice_9: 0.57166/0.62971, loss_spatial_ce_9: 0.84268/1.09700, loss_grounding_bce_9: 0.02269/0.15996, loss_grounding_dice_9: 0.24417/0.37170, loss_grounding_ce_9: 0.54910/0.56849] items per batch[4] items per second[0.02] total items[7200] mini batches[  1800] memory[11531] epoch remaining[0:03:12]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0023 s/iter. Inference: 0.1970 s/iter. Eval: 0.0137 s/iter. Total: 0.2131 s/iter. ETA=0:00:07\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 35/47. Dataloading: 0.0043 s/iter. Inference: 0.1982 s/iter. Eval: 0.0129 s/iter. Total: 0.2156 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 56.58711194780322, 'fwIoU': 96.78689439878055, 'IoU-void': 98.45037319444234, 'IoU-liver': 72.62096001650627, 'IoU-right kidney': 14.468213152423678, 'IoU-left kidney': 37.5951458677574, 'IoU-spleen': 59.80086750788644, 'mACC': 71.07529156259021, 'pACC': 98.08167519310079, 'ACC-void': 99.10783096727059, 'ACC-liver': 85.32664926384325, 'ACC-right kidney': 17.741773170977158, 'ACC-left kidney': 87.40199628451778, 'ACC-spleen': 65.79820812634229})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 56.58711194780322, 'fwIoU': 96.78689439878055, 'IoU-void': 98.45037319444234, 'IoU-liver': 72.62096001650627, 'IoU-right kidney': 14.468213152423678, 'IoU-left kidney': 37.5951458677574, 'IoU-spleen': 59.80086750788644, 'mACC': 71.07529156259021, 'pACC': 98.08167519310079, 'ACC-void': 99.10783096727059, 'ACC-liver': 85.32664926384325, 'ACC-right kidney': 17.741773170977158, 'ACC-left kidney': 87.40199628451778, 'ACC-spleen': 65.79820812634229})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:17.006103\n",
            "INFO:trainer.default_trainer:PROGRESS: 50.00%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 15 training.\n",
            "INFO:trainer.default_trainer:epochs[    15] optim steps[1900] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 1.19569/0.59033, loss_mask_bce_0: 0.12859/0.74496, loss_mask_dice_0: 1.30398/1.28219, loss_spatial_bce_0: 0.04646/0.20633, loss_spatial_dice_0: 0.32476/0.32192, loss_spatial_ce_0: 0.13932/0.17892, loss_grounding_bce_0: 0.01882/0.07696, loss_grounding_dice_0: 0.13593/0.21474, loss_grounding_ce_0: 0.62310/1.04780, loss_mask_ce_1: 1.06322/0.59973, loss_mask_bce_1: 0.12514/0.74758, loss_mask_dice_1: 1.28047/1.29469, loss_spatial_bce_1: 0.04601/0.20955, loss_spatial_dice_1: 0.36135/0.32852, loss_spatial_ce_1: 0.13038/0.19325, loss_grounding_bce_1: 0.01931/0.07743, loss_grounding_dice_1: 0.16999/0.21671, loss_grounding_ce_1: 0.49950/1.06694, loss_mask_ce_2: 1.01479/0.61034, loss_mask_bce_2: 0.11893/0.75223, loss_mask_dice_2: 1.28090/1.29170, loss_spatial_bce_2: 0.05425/0.20979, loss_spatial_dice_2: 0.35566/0.33092, loss_spatial_ce_2: 0.21380/0.20755, loss_grounding_bce_2: 0.01707/0.07761, loss_grounding_dice_2: 0.13963/0.21615, loss_grounding_ce_2: 0.39945/1.07222, loss_mask_ce_3: 0.96514/0.61736, loss_mask_bce_3: 0.13249/0.74998, loss_mask_dice_3: 1.34417/1.28321, loss_spatial_bce_3: 0.05141/0.21118, loss_spatial_dice_3: 0.33659/0.32671, loss_spatial_ce_3: 0.12106/0.20477, loss_grounding_bce_3: 0.02215/0.07738, loss_grounding_dice_3: 0.14482/0.21393, loss_grounding_ce_3: 0.40998/1.08501, loss_mask_ce_4: 1.13714/0.64605, loss_mask_bce_4: 0.13085/0.76312, loss_mask_dice_4: 1.23266/1.30979, loss_spatial_bce_4: 0.06821/0.21677, loss_spatial_dice_4: 0.37722/0.34510, loss_spatial_ce_4: 0.15719/0.23205, loss_grounding_bce_4: 0.02107/0.07763, loss_grounding_dice_4: 0.16493/0.21855, loss_grounding_ce_4: 0.61844/1.14253, loss_mask_ce_5: 1.00656/0.67454, loss_mask_bce_5: 0.11541/0.77686, loss_mask_dice_5: 1.17884/1.31877, loss_spatial_bce_5: 0.06986/0.22431, loss_spatial_dice_5: 0.41126/0.35429, loss_spatial_ce_5: 0.16392/0.23524, loss_grounding_bce_5: 0.01576/0.07902, loss_grounding_dice_5: 0.12755/0.21972, loss_grounding_ce_5: 0.57265/1.16777, loss_mask_ce_6: 0.80073/0.68583, loss_mask_bce_6: 0.14298/0.77806, loss_mask_dice_6: 1.35231/1.32125, loss_spatial_bce_6: 0.06634/0.23372, loss_spatial_dice_6: 0.38683/0.35309, loss_spatial_ce_6: 0.20215/0.24537, loss_grounding_bce_6: 0.02350/0.07914, loss_grounding_dice_6: 0.15306/0.22083, loss_grounding_ce_6: 0.64166/1.17242, loss_mask_ce_7: 1.00845/0.73000, loss_mask_bce_7: 0.13955/0.79803, loss_mask_dice_7: 1.27104/1.37487, loss_spatial_bce_7: 0.05608/0.24927, loss_spatial_dice_7: 0.41728/0.39873, loss_spatial_ce_7: 0.27529/0.34075, loss_grounding_bce_7: 0.02265/0.08018, loss_grounding_dice_7: 0.14671/0.22747, loss_grounding_ce_7: 1.26625/1.25409, loss_mask_ce_8: 1.17425/0.79130, loss_mask_bce_8: 0.12400/0.85308, loss_mask_dice_8: 1.30667/1.47085, loss_spatial_bce_8: 0.06194/0.31731, loss_spatial_dice_8: 0.46642/0.51561, loss_spatial_ce_8: 0.50751/0.55844, loss_grounding_bce_8: 0.02220/0.08574, loss_grounding_dice_8: 0.16505/0.24163, loss_grounding_ce_8: 1.44542/1.33595, loss_mask_ce_9: 0.47606/0.46624, loss_mask_bce_9: 0.17168/0.98447, loss_mask_dice_9: 2.62434/2.02594, loss_spatial_bce_9: 0.11178/0.44518, loss_spatial_dice_9: 0.58520/0.62832, loss_spatial_ce_9: 1.42295/1.09517, loss_grounding_bce_9: 0.02563/0.15971, loss_grounding_dice_9: 0.35782/0.37067, loss_grounding_ce_9: 0.88819/0.56000] items per batch[4] items per second[0.02] total items[7600] mini batches[  1900] memory[11531] epoch remaining[0:04:02]\n",
            "INFO:trainer.default_trainer:epochs[    15] optim steps[2000] learning rate[default: 1.00000e-04] train loss[loss_mask_ce_0: 0.22783/0.58272, loss_mask_bce_0: 0.50607/0.73815, loss_mask_dice_0: 0.74892/1.27291, loss_spatial_bce_0: 0.19536/0.20439, loss_spatial_dice_0: 0.22178/0.31944, loss_spatial_ce_0: 0.07654/0.17498, loss_grounding_bce_0: 0.04732/0.07634, loss_grounding_dice_0: 0.11955/0.21368, loss_grounding_ce_0: 0.61763/1.03176, loss_mask_ce_1: 0.22683/0.59193, loss_mask_bce_1: 0.49275/0.74107, loss_mask_dice_1: 0.73907/1.28612, loss_spatial_bce_1: 0.19684/0.20759, loss_spatial_dice_1: 0.22244/0.32627, loss_spatial_ce_1: 0.08481/0.18941, loss_grounding_bce_1: 0.04743/0.07687, loss_grounding_dice_1: 0.12298/0.21548, loss_grounding_ce_1: 0.47680/1.05367, loss_mask_ce_2: 0.20325/0.60290, loss_mask_bce_2: 0.50586/0.74589, loss_mask_dice_2: 0.74023/1.28291, loss_spatial_bce_2: 0.19025/0.20789, loss_spatial_dice_2: 0.21722/0.32858, loss_spatial_ce_2: 0.05053/0.20406, loss_grounding_bce_2: 0.04994/0.07707, loss_grounding_dice_2: 0.13091/0.21491, loss_grounding_ce_2: 0.49681/1.05973, loss_mask_ce_3: 0.25449/0.60970, loss_mask_bce_3: 0.52379/0.74355, loss_mask_dice_3: 0.74652/1.27444, loss_spatial_bce_3: 0.19148/0.20931, loss_spatial_dice_3: 0.21189/0.32433, loss_spatial_ce_3: 0.00783/0.20217, loss_grounding_bce_3: 0.04826/0.07682, loss_grounding_dice_3: 0.11796/0.21272, loss_grounding_ce_3: 0.60017/1.06996, loss_mask_ce_4: 0.23844/0.63934, loss_mask_bce_4: 0.57584/0.75679, loss_mask_dice_4: 0.74444/1.30121, loss_spatial_bce_4: 0.23503/0.21538, loss_spatial_dice_4: 0.23982/0.34282, loss_spatial_ce_4: 0.05860/0.23063, loss_grounding_bce_4: 0.04880/0.07715, loss_grounding_dice_4: 0.10178/0.21738, loss_grounding_ce_4: 0.55051/1.12759, loss_mask_ce_5: 0.29915/0.66796, loss_mask_bce_5: 0.60081/0.77055, loss_mask_dice_5: 0.74143/1.31090, loss_spatial_bce_5: 0.23895/0.22254, loss_spatial_dice_5: 0.23546/0.35159, loss_spatial_ce_5: 0.16736/0.23276, loss_grounding_bce_5: 0.05173/0.07852, loss_grounding_dice_5: 0.10963/0.21852, loss_grounding_ce_5: 0.57355/1.15561, loss_mask_ce_6: 0.31656/0.68008, loss_mask_bce_6: 0.58341/0.77183, loss_mask_dice_6: 0.74259/1.31233, loss_spatial_bce_6: 0.21602/0.23186, loss_spatial_dice_6: 0.23428/0.35025, loss_spatial_ce_6: 0.06012/0.24242, loss_grounding_bce_6: 0.05342/0.07870, loss_grounding_dice_6: 0.13364/0.21954, loss_grounding_ce_6: 0.56574/1.16089, loss_mask_ce_7: 0.32670/0.72443, loss_mask_bce_7: 0.63689/0.79252, loss_mask_dice_7: 0.73644/1.36764, loss_spatial_bce_7: 0.20874/0.24730, loss_spatial_dice_7: 0.22802/0.39558, loss_spatial_ce_7: 0.08995/0.33726, loss_grounding_bce_7: 0.05256/0.07980, loss_grounding_dice_7: 0.11879/0.22639, loss_grounding_ce_7: 0.63735/1.24360, loss_mask_ce_8: 0.44895/0.78764, loss_mask_bce_8: 0.78059/0.84928, loss_mask_dice_8: 0.97917/1.46439, loss_spatial_bce_8: 0.34905/0.31491, loss_spatial_dice_8: 0.42895/0.51292, loss_spatial_ce_8: 0.35103/0.55341, loss_grounding_bce_8: 0.05909/0.08519, loss_grounding_dice_8: 0.13807/0.24062, loss_grounding_ce_8: 0.90414/1.33036, loss_mask_ce_9: 0.26537/0.45742, loss_mask_bce_9: 1.10354/0.97952, loss_mask_dice_9: 1.65039/2.01910, loss_spatial_bce_9: 0.72631/0.44452, loss_spatial_dice_9: 0.62865/0.62743, loss_spatial_ce_9: 1.54639/1.09393, loss_grounding_bce_9: 0.16342/0.15944, loss_grounding_dice_9: 0.26107/0.36948, loss_grounding_ce_9: 0.61264/0.54985] items per batch[4] items per second[0.02] total items[8000] mini batches[  2000] memory[11531] epoch remaining[0:00:00]\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/47. Dataloading: 0.0116 s/iter. Inference: 0.1987 s/iter. Eval: 0.0137 s/iter. Total: 0.2240 s/iter. ETA=0:00:08\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 36/47. Dataloading: 0.0058 s/iter. Inference: 0.1938 s/iter. Eval: 0.0111 s/iter. Total: 0.2108 s/iter. ETA=0:00:02\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 62.027059984799116, 'fwIoU': 97.01225876618257, 'IoU-void': 98.54737554967193, 'IoU-liver': 72.83769656881164, 'IoU-right kidney': 27.22136781645272, 'IoU-left kidney': 45.17436021747411, 'IoU-spleen': 66.3544997715852, 'mACC': 74.17980021952498, 'pACC': 98.3086614663428, 'ACC-void': 99.27525133094609, 'ACC-liver': 84.75737780144331, 'ACC-right kidney': 29.093287712589312, 'ACC-left kidney': 82.14994174879561, 'ACC-spleen': 75.62314250385057})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 62.027059984799116, 'fwIoU': 97.01225876618257, 'IoU-void': 98.54737554967193, 'IoU-liver': 72.83769656881164, 'IoU-right kidney': 27.22136781645272, 'IoU-left kidney': 45.17436021747411, 'IoU-spleen': 66.3544997715852, 'mACC': 74.17980021952498, 'pACC': 98.3086614663428, 'ACC-void': 99.27525133094609, 'ACC-liver': 84.75737780144331, 'ACC-right kidney': 29.093287712589312, 'ACC-left kidney': 82.14994174879561, 'ACC-spleen': 75.62314250385057})])}\n",
            "INFO:trainer.default_trainer:This epoch takes 0:05:35.917059\n",
            "INFO:trainer.default_trainer:PROGRESS: 53.33%\n",
            "INFO:trainer.default_trainer:Config files are at ['configs/seem/focall_unicl_lang_v1.yaml']\n",
            "INFO:trainer.default_trainer:Start epoch: 16 training.\n",
            "Traceback (most recent call last):\n",
            "  File \"/content/entry.py\", line 75, in <module>\n",
            "    main()\n",
            "  File \"/content/entry.py\", line 68, in main\n",
            "    trainer.train()\n",
            "  File \"/content/trainer/default_trainer.py\", line 338, in train\n",
            "    self.train_step(batch)\n",
            "  File \"/content/trainer/default_trainer.py\", line 228, in train_step\n",
            "    self.pipeline.forward_step(self,\n",
            "  File \"/content/pipeline/XDecoderPipeline.py\", line 104, in forward_step\n",
            "    trainer.backward_loss(loss, model_names=['default'])\n",
            "  File \"/content/trainer/default_trainer.py\", line 182, in backward_loss\n",
            "    backward(loss)\n",
            "  File \"/content/trainer/default_trainer.py\", line 177, in backward\n",
            "    loss_tensor.backward()\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/_tensor.py\", line 492, in backward\n",
            "    torch.autograd.backward(\n",
            "  File \"/usr/local/lib/python3.10/dist-packages/torch/autograd/__init__.py\", line 251, in backward\n",
            "    Variable._execution_engine.run_backward(  # Calls into the C++ engine to run the backward pass\n",
            "KeyboardInterrupt\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Waiting for W&B process to finish... \u001b[31m(failed 255).\u001b[0m Press Control-C to abort syncing.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run history:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_0 ██▄▄▂▄▂▄▂▃▃▃▁▃▂▃▂▁▄▁▂▂▂▄▂▁▃▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_1 ▇█▄▅▂▄▂▃▂▂▃▃▁▃▂▃▁▁▃▁▂▂▂▄▂▁▂▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_2 ██▄▅▂▅▂▄▂▃▃▃▁▃▂▃▂▁▄▁▂▂▂▅▂▁▃▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_3 █▇▄▄▂▄▂▄▃▂▃▃▁▃▂▃▂▁▃▁▂▂▂▄▂▁▂▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_4 ██▄▆▂▅▃▅▂▂▃▃▁▃▂▃▂▁▃▁▂▂▂▅▂▁▃▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_5 ██▄▅▂▅▂▄▃▃▃▃▁▄▂▃▂▁▄▁▂▃▂▅▂▁▃▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_6 ▆█▃▄▂▃▁▃▂▂▃▃▁▃▂▂▁▁▃▁▂▂▂▃▁▁▂▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_7 ▅█▃▄▂▄▁▄▂▂▃▂▁▃▂▂▂▁▃▁▂▂▂▄▂▁▃▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_8 ▇█▄▅▂▄▂▄▂▃▄▃▁▄▂▃▂▁▃▁▂▂▂▄▂▁▃▁▁▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_9 ▇█▅▄▄▅▃▆▃▅▄▄▁▃▃▄▂▃▅▂▅▃▄▄▃▁▃▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_0 ██▇▇▇▅▆▅▅▆▄▄▃▄▃▂▄▄▂▁▁▄▁▂▂▃▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_1 █▇▆▇▆▅▆▄▅▅▄▄▃▃▃▂▄▄▃▁▁▄▁▂▂▃▂▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_2 █▆▆▇▅▄▆▄▆▅▄▄▃▃▃▂▄▃▂▂▁▃▁▁▂▃▂▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_3 █▇▆▇▆▅▆▄▆▅▄▄▃▃▃▂▅▄▃▁▁▃▁▂▂▃▂▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_4 █▆▆▅▆▄▆▄▅▅▄▃▃▃▃▂▄▃▃▁▁▃▁▂▂▃▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_5 █▅▅▅▅▄▅▄▄▄▃▃▂▃▂▂▄▃▂▂▁▃▁▁▂▃▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_6 █▆▆▆▆▅▅▅▄▅▃▃▂▄▂▂▄▃▂▂▁▃▁▁▂▃▂▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_7 █▅▄▆▆▅▅▄▄▄▃▄▂▃▂▂▄▃▃▂▁▂▁▁▂▄▂▃▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_8 █▆▆▆▆▄▄▅▅▄▃▄▂▃▂▂▄▂▃▂▂▃▁▁▂▄▂▃▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_9 ▇▇▆▅▆▇██▆▆▂▃▁▂▁▂▃▁▁▂▂▂▁▁▁▃▁▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_0 ▇▇▇█▄▆▆▅▆▅▂▄▃▃▂▂▂▂▆▂▂▃▁▅▂▃▃▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_1 ▆▇▇█▅▆▆▅▇▅▂▃▄▃▃▂▂▂▆▂▂▄▁▅▂▄▃▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_2 ▆▆▇█▄▆▆▅▇▅▂▃▃▃▃▂▂▂▆▁▂▃▁▆▂▃▂▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_3 ▇▇▇█▄▅▆▆▇▅▂▃▃▃▃▂▂▂▅▂▂▃▁▅▂▃▂▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_4 ▆▆▆█▄▆▆▆▆▅▂▄▃▃▃▂▂▂▅▁▂▃▁▅▂▃▃▃▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_5 ▆▇▆█▅▆▇▆▆▆▂▃▃▃▃▂▂▂▅▁▂▃▁▆▂▄▃▂▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_6 ▆▇▆█▅▅▇▅▇▆▂▃▃▃▃▂▂▂▅▁▂▃▁▅▂▃▂▃▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_7 ▆▇██▆▆▇▅▆▆▂▃▃▄▃▂▂▃▅▁▂▃▁▅▂▄▃▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_8 ▆▇▇█▅▇▇▅▅▇▂▃▃▃▃▂▂▂▅▂▂▃▁▅▂▃▃▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_9 ▆▇▇█▆▆▇▆▆█▃▄▄▃▄▂▃▄▆▂▂▃▁▄▃▄▄▃▄▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_0 ▇█▄▆▂▇▂▅▃▂█▅▂▆▄▄▄▂▆▁▅▃▃▃▂▁▄▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_1 ▇█▄▅▂▆▂▅▃▂▇▅▂▅▄▄▄▁▅▁▅▃▃▃▂▁▄▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_2 ██▅▆▂▇▃▆▄▃▇▅▂▆▄▅▅▁▆▁▅▃▃▄▂▁▄▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_3 ██▅▆▂▇▃▆▄▂▇▅▂▆▄▅▅▁▆▁▅▃▃▄▂▁▄▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_4 ▇█▅▆▂▆▃▇▃▂▇▅▂▆▄▅▅▁▅▁▅▃▃▄▂▁▅▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_5 ██▄▅▂▆▂▆▃▂▇▅▃▇▄▅▄▂▆▁▅▃▃▄▂▁▅▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_6 ██▅▅▂▆▂▆▃▃▇▆▃▇▄▄▄▁▆▁▅▃▃▄▂▁▄▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_7 ▇█▄▅▂▆▂▆▃▂▆▅▂▆▄▄▅▂▆▁▄▃▃▄▂▁▄▂▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_8 █▇▄▅▃▆▂▄▃▂▇▅▂▆▄▄▅▂▆▁▅▃▃▅▂▁▆▂▁▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_9 ▇█▆▆▃▇▄▇▃▆▇▇▂▆▄▄▅▃▇▁▄▄▃▅▂▁▆▂▁▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_0 █▄▄▄▃▃▃▃▃▂▂▂▂▂▁▁▂▁▂▁▁▂▁▂▁▂▂▂▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_1 █▅▅▅▃▄▄▃▄▃▂▃▂▃▂▁▃▂▂▁▁▂▁▂▂▂▂▂▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_2 █▅▅▅▃▄▄▄▄▃▂▃▂▃▁▁▃▂▂▁▁▂▁▂▁▂▂▂▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_3 █▅▄▄▃▃▃▃▃▂▂▃▂▃▁▁▃▂▂▁▁▂▁▂▁▂▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_4 █▄▄▄▄▃▃▃▃▃▂▃▂▃▂▁▃▁▂▁▁▂▁▂▁▂▁▂▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_5 █▅▅▄▄▄▃▃▃▃▂▃▁▂▂▁▂▁▂▁▁▂▁▂▂▂▁▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_6 █▅▅▅▄▄▄▃▃▃▂▃▁▂▁▁▃▁▂▁▁▂▁▂▂▂▂▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_7 █▆▆▅▄▄▅▄▄▃▂▃▂▃▂▁▃▂▃▂▁▂▂▃▂▂▂▂▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_8 ████▅▆▇▆▆▅▂▃▁▃▁▁▃▂▂▁▁▂▁▃▂▂▂▂▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_9 █▇▇▆▆▇▆▆▅▅▂▂▂▃▁▂▂▁▁▂▁▁▁▁▁▂▁▁▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_0 ██▇▆▆▆▇▅▆▄▃▄▃▄▃▂▅▂▅▃▂▃▁▃▁▃▃▂▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_1 ██▇▅▅▆▆▅▆▄▃▃▄▄▃▂▄▁▅▃▂▃▁▃▁▃▃▂▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_2 ██▇▆▆▇▆▅▆▅▃▃▃▅▄▂▄▁▅▂▂▃▁▄▁▃▂▃▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_3 ███▆▆▆▆▆▇▅▃▄▃▅▃▂▅▁▅▃▂▃▁▃▁▃▃▃▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_4 ██▇▆▅▆▆▆▆▄▃▄▃▄▃▂▄▁▄▂▂▃▁▄▁▃▃▃▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_5 ██▇▆▅▆▆▅▆▄▃▄▄▄▂▂▄▁▅▂▂▃▁▃▁▃▃▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_6 ██▇▆▅▆▆▅▆▅▃▄▄▅▃▂▄▁▄▂▂▃▁▃▁▂▂▃▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_7 ██▇▅▆▆▆▅▅▅▂▃▄▅▂▁▄▂▄▂▁▃▁▃▁▃▃▃▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_8 ██▇▆▅▆▆▄▅▆▂▃▃▄▃▁▄▁▅▂▂▂▁▄▁▃▃▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_9 ███▇▇▇█▇▇█▃▅▃▅▃▁▅▂▆▁▁▃▁▅▂▂▄▃▆▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_0 ██▆▆▃▆▃▆▂▄▄▄▁▆▃▃▃▁▅▁▃▂▂▃▂▁▃▂▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_1 █▇▇▅▄▅▄▅▂▄▄▅▁▆▃▃▃▁▅▂▄▂▃▃▂▁▃▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_2 █▆▇▅▃▅▃▅▂▄▄▅▁▅▃▃▄▁▅▂▄▂▃▃▂▁▃▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_3 ██▇▅▃▅▄▆▂▄▄▅▁▅▃▄▄▂▅▁▄▂▃▃▂▁▄▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_4 ▇█▆▅▃▅▃▄▂▄▄▄▁▅▃▃▃▂▄▁▃▂▃▃▂▁▃▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_5 ▇█▆▅▂▅▃▄▂▄▅▄▁▅▂▃▃▂▅▁▃▂▃▃▂▁▄▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_6 ▇█▆▆▃▅▄▅▂▄▅▅▁▅▂▃▃▂▅▁▃▂▃▃▁▁▃▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_7 ██▇▆▃▅▃▅▂▄▅▅▂▆▄▄▄▂▆▁▄▃▃▄▂▁▆▁▁▃\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_8 ███▇▄▆▅▆▄█▆▅▁▅▄▅▄▂▅▂▆▄▅▄▄▂▇▂▁▄\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_9 ▇▆▆▆▄▅▅▆▄▆▇▅▃▆▄▇▅▅▆▄▅▄▆▆▄▂▆▄▁█\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_0 ▇▅█▄▄▄▇▅▄▃▂▃▃▁▂▁▃▁▃▃▁▃▂▂▁▁▅▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_1 █▇▆▅▅▅▄▅▆▂▂▃▂▁▂▁▄▁▃▄▁▃▂▃▂▁▅▁▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_2 █▇▆▅▄▅▄▅▆▃▂▂▂▂▂▁▅▁▃▂▁▃▁▂▁▁▄▂▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_3 █▆▆▅▄▅▄▅▅▃▂▃▄▂▂▁▄▁▃▃▂▃▂▃▂▂▄▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_4 █▆▆▅▄▅▃▅▅▂▂▃▂▃▂▁▃▁▃▂▁▃▂▃▂▂▃▂▂▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_5 █▆█▆▅▆▃▆▅▃▂▃▁▃▂▁▄▂▃▃▂▃▂▃▁▂▃▃▂▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_6 █▅▇▅▄▅▄▆▄▃▃▃▃▂▂▁▄▂▃▃▂▃▂▃▁▂▃▂▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_7 ▅▆▆▄█▅▄▃▄▄▂▃▂▄▃▁▄▃▄▂▁▃▁▄▂▂▃▃▃▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_8 ▅▆▅▅█▄▅▆▅▄▄▅▂▄▂▁▄▁▄▂▃▃▃▄▂▁▆▁▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_9 ▃▃▄▃▃▃▂▃▂▂▁▄▂▄▂▃▅▇▅▃▁▃▃█▁▂▄▂▅▆\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_0 ▇██▆▅▇▇▆▆▅▂▃▂▄▃▁▃▁▄▂▁▃▁▄▂▂▃▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_1 ▇██▆▅▆▇▅▆▅▂▃▃▅▃▁▄▁▅▂▁▃▁▃▂▂▄▂▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_2 ▇▇█▆▅▆▇▅▅▅▂▃▃▄▃▁▄▁▄▂▁▂▁▃▂▂▃▂▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_3 ▇██▆▅▆▇▅▅▅▂▃▂▄▃▁▃▁▄▂▁▂▁▃▁▁▃▁▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_4 ▇██▆▅▆▇▅▅▅▂▄▃▄▃▁▄▁▄▂▁▃▁▄▁▂▃▂▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_5 ▇██▆▄▆▇▅▆▆▂▃▃▄▃▁▄▂▄▂▁▂▂▄▁▂▃▂▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_6 ███▇▅▇▇▅▆▅▃▄▃▅▂▂▄▂▄▂▁▃▂▄▁▃▃▂▄▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_7 ███▇▆▆▇▆▆▆▃▄▃▅▃▁▄▁▄▂▁▄▂▄▂▃▄▃▄▁\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_8 ▇▇█▇▆▆█▆▆▇▄▅▂▅▃▂▅▁▅▂▂▄▂▄▂▂▃▂▃▂\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_9 ▇▇█▆▇▇█▆▆▆▅▆▄▆▃▃▇▄▇▄▂▆▃▇▃▁▅▄▄▅\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Run summary:\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_0 0.04732\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_1 0.04743\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_2 0.04994\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_3 0.04826\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_4 0.0488\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_5 0.05173\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_6 0.05342\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_7 0.05256\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_8 0.05909\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:  loss_grounding_bce_9 0.16342\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_0 0.61763\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_1 0.4768\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_2 0.49681\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_3 0.60017\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_4 0.55051\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_5 0.57355\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_6 0.56574\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_7 0.63735\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_8 0.90414\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_grounding_ce_9 0.61264\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_0 0.11955\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_1 0.12298\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_2 0.13091\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_3 0.11796\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_4 0.10178\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_5 0.10963\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_6 0.13364\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_7 0.11879\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_8 0.13807\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: loss_grounding_dice_9 0.26107\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_0 0.50607\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_1 0.49275\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_2 0.50586\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_3 0.52379\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_4 0.57584\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_5 0.60081\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_6 0.58341\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_7 0.63689\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_8 0.78059\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:       loss_mask_bce_9 1.10354\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_0 0.22783\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_1 0.22683\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_2 0.20325\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_3 0.25449\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_4 0.23844\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_5 0.29915\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_6 0.31656\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_7 0.3267\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_8 0.44895\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:        loss_mask_ce_9 0.26537\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_0 0.74892\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_1 0.73907\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_2 0.74023\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_3 0.74652\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_4 0.74444\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_5 0.74143\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_6 0.74259\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_7 0.73644\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_8 0.97917\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:      loss_mask_dice_9 1.65039\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_0 0.19536\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_1 0.19684\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_2 0.19025\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_3 0.19148\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_4 0.23503\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_5 0.23895\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_6 0.21602\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_7 0.20874\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_8 0.34905\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:    loss_spatial_bce_9 0.72631\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_0 0.07654\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_1 0.08481\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_2 0.05053\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_3 0.00783\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_4 0.0586\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_5 0.16736\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_6 0.06012\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_7 0.08995\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_8 0.35103\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:     loss_spatial_ce_9 1.54639\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_0 0.22178\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_1 0.22244\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_2 0.21722\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_3 0.21189\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_4 0.23982\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_5 0.23546\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_6 0.23428\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_7 0.22802\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_8 0.42895\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m:   loss_spatial_dice_9 0.62865\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run \u001b[33m/content/data/output/test/focall_unicl_lang_v1.yaml_conf~/run_2\u001b[0m at: \u001b[34m\u001b[4mhttps://wandb.ai/msdk/xdecoder/runs/ygm0nv0l\u001b[0m\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Find logs at: \u001b[35m\u001b[1m./data/output/test/focall_unicl_lang_v1.yaml_conf~/run_2/wandb/wandb/run-20240611_182401-ygm0nv0l/logs\u001b[0m\n",
            "^C\n"
          ]
        }
      ],
      "source": [
        "!python entry.py train \\\n",
        "--conf_files configs/seem/focall_unicl_lang_v1.yaml \\\n",
        "--overrides \\\n",
        "COCO.INPUT.IMAGE_SIZE 256 \\\n",
        "MODEL.DECODER.HIDDEN_DIM 512 \\\n",
        "MODEL.ENCODER.CONVS_DIM 512 \\\n",
        "MODEL.ENCODER.MASK_DIM 512 \\\n",
        "MODEL.DECODER.LVIS.ENABLED False \\\n",
        "TEST.BATCH_SIZE_TOTAL 1 \\\n",
        "TRAIN.BATCH_SIZE_TOTAL 4 \\\n",
        "TRAIN.BATCH_SIZE_PER_GPU 2 \\\n",
        "SOLVER.BASE_LR 0.0001 \\\n",
        "SOLVER.FIX_PARAM.backbone True \\\n",
        "SOLVER.FIX_PARAM.lang_encoder True \\\n",
        "SOLVER.FIX_PARAM.pixel_decoder True \\\n",
        "MODEL.DECODER.COST_SPATIAL.CLASS_WEIGHT 5.0 \\\n",
        "MODEL.DECODER.COST_SPATIAL.MASK_WEIGHT 2.0 \\\n",
        "MODEL.DECODER.COST_SPATIAL.DICE_WEIGHT 2.0 \\\n",
        "MODEL.DECODER.TOP_SPATIAL_LAYERS 10 \\\n",
        "MODEL.DECODER.SPATIAL.ENABLED True \\\n",
        "MODEL.DECODER.GROUNDING.ENABLED True \\\n",
        "FIND_UNUSED_PARAMETERS True \\\n",
        "ATTENTION_ARCH.SPATIAL_MEMORIES 32 \\\n",
        "MODEL.DECODER.SPATIAL.MAX_ITER 5 \\\n",
        "ATTENTION_ARCH.QUERY_NUMBER 3 \\\n",
        "STROKE_SAMPLER.MAX_CANDIDATE 10 \\\n",
        "MODEL.ENCODER.NUM_CLASSES 5 \\\n",
        "SOLVER.MAX_NUM_EPOCHS 30 \\\n",
        "LOG_EVERY 100 \\\n",
        "MODEL.DECODER.TEST.PANOPTIC_ON False \\\n",
        "MODEL.DECODER.TEST.INSTANCE_ON False \\\n",
        "WANDB True \\\n",
        "WEIGHT True \\\n",
        "EVAL_AT_START False \\\n",
        "MODEL.BACKBONE.FOCAL.FINE_TUNE True \\\n",
        "MODEL.DECODER.FINE_TUNE False \\\n",
        "RESUME_FROM /content/datasets/xdecoder_data/pretrained/pretrained_w.pt"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Gi3snj27_BJV"
      },
      "outputs": [],
      "source": [
        "!cp  /content/drive/My\\ Drive/SEEM\\ weights/BH_SEEM_mri_24e_best.pt /content/data/output/test/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bR7oFkqfU3bB"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ybqtJFx9ln2q"
      },
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Edu3dkJG4q6r",
        "outputId": "d68c7458-aa4f-4de1-e07b-6dddc3024ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:utils.arguments:Overrided COCO.INPUT.IMAGE_SIZE from 1024 to 256\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.HIDDEN_DIM from 512 to 512\n",
            "WARNING:utils.arguments:Overrided MODEL.ENCODER.CONVS_DIM from 512 to 512\n",
            "WARNING:utils.arguments:Overrided MODEL.ENCODER.MASK_DIM from 512 to 512\n",
            "WARNING:utils.arguments:Overrided VOC.TEST.BATCH_SIZE_TOTAL from 8 to 1\n",
            "WARNING:utils.arguments:Overrided TEST.BATCH_SIZE_TOTAL from 8 to 1\n",
            "WARNING:utils.arguments:Overrided REF.TEST.BATCH_SIZE_TOTAL from 1 to 1\n",
            "WARNING:utils.arguments:Overrided MODEL.ENCODER.NUM_CLASSES from 133 to 2\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.TEST.PANOPTIC_ON from True to False\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.TEST.INSTANCE_ON from True to False\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "Deformable Transformer Encoder is not available.\n",
            "INFO:trainer.distributed_trainer:Setting SAVE_DIR as /content/data/output/test\n",
            "INFO:trainer.distributed_trainer:Using CUDA\n",
            "WARNING:trainer.utils.mpi_adapter:----------------\n",
            "WARNING:trainer.utils.mpi_adapter:MPI Adapter data\n",
            "WARNING:trainer.utils.mpi_adapter:----------------\n",
            "WARNING:trainer.utils.mpi_adapter:environment info: no MPI\n",
            "WARNING:trainer.utils.mpi_adapter:init method url: tcp://127.0.0.1:36873\n",
            "WARNING:trainer.utils.mpi_adapter:world size: 1\n",
            "WARNING:trainer.utils.mpi_adapter:local size: 1\n",
            "WARNING:trainer.utils.mpi_adapter:rank: 0\n",
            "WARNING:trainer.utils.mpi_adapter:local rank: 0\n",
            "WARNING:trainer.utils.mpi_adapter:master address: 127.0.0.1\n",
            "WARNING:trainer.utils.mpi_adapter:master port: 36873\n",
            "WARNING:trainer.utils.mpi_adapter:----------------\n",
            "INFO:trainer.distributed_trainer:Save config file to /content/data/output/test/conf_copy.yaml\n",
            "INFO:trainer.distributed_trainer:Base learning rate: 0.0001\n",
            "INFO:trainer.distributed_trainer:Number of GPUs: 1\n",
            "INFO:trainer.distributed_trainer:Gradient accumulation steps: 1\n",
            "INFO:trainer.default_trainer:Imported base_dir at base_path ./\n",
            "INFO:trainer.default_trainer:-----------------------------------------------\n",
            "INFO:trainer.default_trainer:Evaluating model ... \n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:pipeline.XDecoderPipeline:GeneralizedSEEM(\n",
            "  (backbone): D2FocalNet(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 192, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
            "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): FocalModulationBlock(\n",
            "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=192, out_features=389, bias=True)\n",
            "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (adopter_layer): Adapter_Layer(\n",
            "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (channel): Sequential(\n",
            "                (0): Linear(in_features=192, out_features=48, bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): Linear(in_features=48, out_features=192, bias=False)\n",
            "                (3): Sigmoid()\n",
            "              )\n",
            "              (spatial): Sequential(\n",
            "                (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): ConvTranspose2d(192, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (3): ReLU()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "          (1): FocalModulationBlock(\n",
            "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=192, out_features=389, bias=True)\n",
            "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (adopter_layer): Adapter_Layer(\n",
            "              (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (channel): Sequential(\n",
            "                (0): Linear(in_features=192, out_features=48, bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): Linear(in_features=48, out_features=192, bias=False)\n",
            "                (3): Sigmoid()\n",
            "              )\n",
            "              (spatial): Sequential(\n",
            "                (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): ConvTranspose2d(192, 192, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (3): ReLU()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchEmbed(\n",
            "          (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0-1): 2 x FocalModulationBlock(\n",
            "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=384, out_features=773, bias=True)\n",
            "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(384, 384, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=384, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (adopter_layer): Adapter_Layer(\n",
            "              (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (channel): Sequential(\n",
            "                (0): Linear(in_features=384, out_features=96, bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): Linear(in_features=96, out_features=384, bias=False)\n",
            "                (3): Sigmoid()\n",
            "              )\n",
            "              (spatial): Sequential(\n",
            "                (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): ConvTranspose2d(384, 384, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (3): ReLU()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchEmbed(\n",
            "          (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0-17): 18 x FocalModulationBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=768, out_features=1541, bias=True)\n",
            "              (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(768, 768, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=768, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (adopter_layer): Adapter_Layer(\n",
            "              (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (channel): Sequential(\n",
            "                (0): Linear(in_features=768, out_features=192, bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): Linear(in_features=192, out_features=768, bias=False)\n",
            "                (3): Sigmoid()\n",
            "              )\n",
            "              (spatial): Sequential(\n",
            "                (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): ConvTranspose2d(768, 768, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (3): ReLU()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchEmbed(\n",
            "          (proj): Conv2d(768, 1536, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (3): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0-1): 2 x FocalModulationBlock(\n",
            "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=1536, out_features=3077, bias=True)\n",
            "              (h): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(1536, 1536, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1536, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(1536, 1536, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=1536, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "            (adopter_layer): Adapter_Layer(\n",
            "              (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "              (avg_pool): AdaptiveAvgPool2d(output_size=1)\n",
            "              (channel): Sequential(\n",
            "                (0): Linear(in_features=1536, out_features=384, bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): Linear(in_features=384, out_features=1536, bias=False)\n",
            "                (3): Sigmoid()\n",
            "              )\n",
            "              (spatial): Sequential(\n",
            "                (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (1): ReLU()\n",
            "                (2): ConvTranspose2d(1536, 1536, kernel_size=(4, 4), stride=(2, 2), padding=(1, 1), bias=False)\n",
            "                (3): ReLU()\n",
            "              )\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (sem_seg_head): XdecoderHead(\n",
            "    (pixel_decoder): TransformerEncoderPixelDecoder(\n",
            "      (adapter_1): Conv2d(\n",
            "        192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (adapter_2): Conv2d(\n",
            "        384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_2): Conv2d(\n",
            "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (adapter_3): Conv2d(\n",
            "        768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_3): Conv2d(\n",
            "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (mask_features): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (input_proj): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (transformer): TransformerEncoderOnly(\n",
            "        (encoder): TransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x TransformerEncoderLayer(\n",
            "              (self_attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "              )\n",
            "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 256\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (layer_4): Conv2d(\n",
            "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): SEEMDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 256\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(101, 512)\n",
            "      (query_embed): Embedding(101, 512)\n",
            "      (level_embed): Embedding(3, 512)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (lang_encoder): LanguageEncoder(\n",
            "        (lang_encoder): Transformer(\n",
            "          (token_embedding): Embedding(49408, 512)\n",
            "          (resblocks): ModuleList(\n",
            "            (0-11): 12 x ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm()\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm()\n",
            "              (drop_path): Identity()\n",
            "            )\n",
            "          )\n",
            "          (ln_final): LayerNorm()\n",
            "        )\n",
            "      )\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (mask_sptial_embed): ParameterList(\n",
            "          (0): Parameter containing: [torch.float32 of size 512x512]\n",
            "          (1): Parameter containing: [torch.float32 of size 512x512]\n",
            "          (2): Parameter containing: [torch.float32 of size 512x512]\n",
            "      )\n",
            "      (spatial_embed): Embedding(32, 512)\n",
            "      (spatial_featured): Embedding(32, 512)\n",
            "      (pn_indicator): Embedding(2, 512)\n",
            "      (attention_data): AttentionDataStruct()\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: []\n",
            "      weight_dict: {'loss_mask_ce_0': 2.0, 'loss_mask_dice_0': 5.0, 'loss_mask_bce_0': 5.0, 'loss_spatial_ce_0': 0.4, 'loss_spatial_dice_0': 1.0, 'loss_spatial_bce_0': 1.0, 'loss_grounding_ce_0': 0.4, 'loss_grounding_dice_0': 1.0, 'loss_grounding_bce_0': 1.0, 'loss_openimage_ce_0': 0.4, 'loss_openimage_dice_0': 1.0, 'loss_openimage_bce_0': 1.0, 'loss_mask_ce_1': 2.0, 'loss_mask_dice_1': 5.0, 'loss_mask_bce_1': 5.0, 'loss_spatial_ce_1': 0.4, 'loss_spatial_dice_1': 1.0, 'loss_spatial_bce_1': 1.0, 'loss_grounding_ce_1': 0.4, 'loss_grounding_dice_1': 1.0, 'loss_grounding_bce_1': 1.0, 'loss_openimage_ce_1': 0.4, 'loss_openimage_dice_1': 1.0, 'loss_openimage_bce_1': 1.0, 'loss_mask_ce_2': 2.0, 'loss_mask_dice_2': 5.0, 'loss_mask_bce_2': 5.0, 'loss_spatial_ce_2': 0.4, 'loss_spatial_dice_2': 1.0, 'loss_spatial_bce_2': 1.0, 'loss_grounding_ce_2': 0.4, 'loss_grounding_dice_2': 1.0, 'loss_grounding_bce_2': 1.0, 'loss_openimage_ce_2': 0.4, 'loss_openimage_dice_2': 1.0, 'loss_openimage_bce_2': 1.0, 'loss_mask_ce_3': 2.0, 'loss_mask_dice_3': 5.0, 'loss_mask_bce_3': 5.0, 'loss_spatial_ce_3': 0.4, 'loss_spatial_dice_3': 1.0, 'loss_spatial_bce_3': 1.0, 'loss_grounding_ce_3': 0.4, 'loss_grounding_dice_3': 1.0, 'loss_grounding_bce_3': 1.0, 'loss_openimage_ce_3': 0.4, 'loss_openimage_dice_3': 1.0, 'loss_openimage_bce_3': 1.0, 'loss_mask_ce_4': 2.0, 'loss_mask_dice_4': 5.0, 'loss_mask_bce_4': 5.0, 'loss_spatial_ce_4': 0.4, 'loss_spatial_dice_4': 1.0, 'loss_spatial_bce_4': 1.0, 'loss_grounding_ce_4': 0.4, 'loss_grounding_dice_4': 1.0, 'loss_grounding_bce_4': 1.0, 'loss_openimage_ce_4': 0.4, 'loss_openimage_dice_4': 1.0, 'loss_openimage_bce_4': 1.0, 'loss_mask_ce_5': 2.0, 'loss_mask_dice_5': 5.0, 'loss_mask_bce_5': 5.0, 'loss_spatial_ce_5': 0.4, 'loss_spatial_dice_5': 1.0, 'loss_spatial_bce_5': 1.0, 'loss_grounding_ce_5': 0.4, 'loss_grounding_dice_5': 1.0, 'loss_grounding_bce_5': 1.0, 'loss_openimage_ce_5': 0.4, 'loss_openimage_dice_5': 1.0, 'loss_openimage_bce_5': 1.0, 'loss_mask_ce_6': 2.0, 'loss_mask_dice_6': 5.0, 'loss_mask_bce_6': 5.0, 'loss_spatial_ce_6': 0.4, 'loss_spatial_dice_6': 1.0, 'loss_spatial_bce_6': 1.0, 'loss_grounding_ce_6': 0.4, 'loss_grounding_dice_6': 1.0, 'loss_grounding_bce_6': 1.0, 'loss_openimage_ce_6': 0.4, 'loss_openimage_dice_6': 1.0, 'loss_openimage_bce_6': 1.0, 'loss_mask_ce_7': 2.0, 'loss_mask_dice_7': 5.0, 'loss_mask_bce_7': 5.0, 'loss_spatial_ce_7': 0.4, 'loss_spatial_dice_7': 1.0, 'loss_spatial_bce_7': 1.0, 'loss_grounding_ce_7': 0.4, 'loss_grounding_dice_7': 1.0, 'loss_grounding_bce_7': 1.0, 'loss_openimage_ce_7': 0.4, 'loss_openimage_dice_7': 1.0, 'loss_openimage_bce_7': 1.0, 'loss_mask_ce_8': 2.0, 'loss_mask_dice_8': 5.0, 'loss_mask_bce_8': 5.0, 'loss_spatial_ce_8': 0.4, 'loss_spatial_dice_8': 1.0, 'loss_spatial_bce_8': 1.0, 'loss_grounding_ce_8': 0.4, 'loss_grounding_dice_8': 1.0, 'loss_grounding_bce_8': 1.0, 'loss_openimage_ce_8': 0.4, 'loss_openimage_dice_8': 1.0, 'loss_openimage_bce_8': 1.0, 'loss_mask_ce_9': 2.0, 'loss_mask_dice_9': 5.0, 'loss_mask_bce_9': 5.0, 'loss_spatial_ce_9': 0.4, 'loss_spatial_dice_9': 1.0, 'loss_spatial_bce_9': 1.0, 'loss_grounding_ce_9': 0.4, 'loss_grounding_dice_9': 1.0, 'loss_grounding_bce_9': 1.0, 'loss_openimage_ce_9': 0.4, 'loss_openimage_dice_9': 1.0, 'loss_openimage_bce_9': 1.0}\n",
            "      num_classes: 2\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.adopter_layer.channel.0.weight, Model Shape: torch.Size([48, 192]) <-> Ckpt Shape: torch.Size([48, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.adopter_layer.channel.2.weight, Model Shape: torch.Size([192, 48]) <-> Ckpt Shape: torch.Size([192, 48])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.adopter_layer.norm.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.adopter_layer.norm.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.adopter_layer.spatial.0.weight, Model Shape: torch.Size([192, 192, 3, 3]) <-> Ckpt Shape: torch.Size([192, 192, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.adopter_layer.spatial.2.weight, Model Shape: torch.Size([192, 192, 4, 4]) <-> Ckpt Shape: torch.Size([192, 192, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.bias, Model Shape: torch.Size([389]) <-> Ckpt Shape: torch.Size([389])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.weight, Model Shape: torch.Size([389, 192]) <-> Ckpt Shape: torch.Size([389, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([192, 1, 9, 9]) <-> Ckpt Shape: torch.Size([192, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.adopter_layer.channel.0.weight, Model Shape: torch.Size([48, 192]) <-> Ckpt Shape: torch.Size([48, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.adopter_layer.channel.2.weight, Model Shape: torch.Size([192, 48]) <-> Ckpt Shape: torch.Size([192, 48])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.adopter_layer.norm.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.adopter_layer.norm.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.adopter_layer.spatial.0.weight, Model Shape: torch.Size([192, 192, 3, 3]) <-> Ckpt Shape: torch.Size([192, 192, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.adopter_layer.spatial.2.weight, Model Shape: torch.Size([192, 192, 4, 4]) <-> Ckpt Shape: torch.Size([192, 192, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.bias, Model Shape: torch.Size([389]) <-> Ckpt Shape: torch.Size([389])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.weight, Model Shape: torch.Size([389, 192]) <-> Ckpt Shape: torch.Size([389, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([192, 1, 9, 9]) <-> Ckpt Shape: torch.Size([192, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.weight, Model Shape: torch.Size([384, 192, 3, 3]) <-> Ckpt Shape: torch.Size([384, 192, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.adopter_layer.channel.0.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.adopter_layer.channel.2.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.adopter_layer.norm.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.adopter_layer.norm.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.adopter_layer.spatial.0.weight, Model Shape: torch.Size([384, 384, 3, 3]) <-> Ckpt Shape: torch.Size([384, 384, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.adopter_layer.spatial.2.weight, Model Shape: torch.Size([384, 384, 4, 4]) <-> Ckpt Shape: torch.Size([384, 384, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.bias, Model Shape: torch.Size([773]) <-> Ckpt Shape: torch.Size([773])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.weight, Model Shape: torch.Size([773, 384]) <-> Ckpt Shape: torch.Size([773, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([384, 1, 9, 9]) <-> Ckpt Shape: torch.Size([384, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.adopter_layer.channel.0.weight, Model Shape: torch.Size([96, 384]) <-> Ckpt Shape: torch.Size([96, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.adopter_layer.channel.2.weight, Model Shape: torch.Size([384, 96]) <-> Ckpt Shape: torch.Size([384, 96])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.adopter_layer.norm.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.adopter_layer.norm.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.adopter_layer.spatial.0.weight, Model Shape: torch.Size([384, 384, 3, 3]) <-> Ckpt Shape: torch.Size([384, 384, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.adopter_layer.spatial.2.weight, Model Shape: torch.Size([384, 384, 4, 4]) <-> Ckpt Shape: torch.Size([384, 384, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.bias, Model Shape: torch.Size([773]) <-> Ckpt Shape: torch.Size([773])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.weight, Model Shape: torch.Size([773, 384]) <-> Ckpt Shape: torch.Size([773, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([384, 1, 9, 9]) <-> Ckpt Shape: torch.Size([384, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.weight, Model Shape: torch.Size([768, 384, 3, 3]) <-> Ckpt Shape: torch.Size([768, 384, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.adopter_layer.channel.0.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.adopter_layer.channel.2.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.adopter_layer.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.adopter_layer.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.adopter_layer.spatial.0.weight, Model Shape: torch.Size([768, 768, 3, 3]) <-> Ckpt Shape: torch.Size([768, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.adopter_layer.spatial.2.weight, Model Shape: torch.Size([768, 768, 4, 4]) <-> Ckpt Shape: torch.Size([768, 768, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.weight, Model Shape: torch.Size([1536, 768, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.adopter_layer.channel.0.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.adopter_layer.channel.2.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.adopter_layer.norm.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.adopter_layer.norm.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.adopter_layer.spatial.0.weight, Model Shape: torch.Size([1536, 1536, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 1536, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.adopter_layer.spatial.2.weight, Model Shape: torch.Size([1536, 1536, 4, 4]) <-> Ckpt Shape: torch.Size([1536, 1536, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_1, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_2, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([6144]) <-> Ckpt Shape: torch.Size([6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([6144, 1536]) <-> Ckpt Shape: torch.Size([6144, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([1536, 6144]) <-> Ckpt Shape: torch.Size([1536, 6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.bias, Model Shape: torch.Size([3077]) <-> Ckpt Shape: torch.Size([3077])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.weight, Model Shape: torch.Size([3077, 1536]) <-> Ckpt Shape: torch.Size([3077, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([1536, 1, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([1536, 1, 5, 5]) <-> Ckpt Shape: torch.Size([1536, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([1536, 1, 7, 7]) <-> Ckpt Shape: torch.Size([1536, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([1536, 1, 9, 9]) <-> Ckpt Shape: torch.Size([1536, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.weight, Model Shape: torch.Size([1536, 1536, 1, 1]) <-> Ckpt Shape: torch.Size([1536, 1536, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.weight, Model Shape: torch.Size([1536, 1536]) <-> Ckpt Shape: torch.Size([1536, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.adopter_layer.channel.0.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.adopter_layer.channel.2.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.adopter_layer.norm.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.adopter_layer.norm.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.adopter_layer.spatial.0.weight, Model Shape: torch.Size([1536, 1536, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 1536, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.adopter_layer.spatial.2.weight, Model Shape: torch.Size([1536, 1536, 4, 4]) <-> Ckpt Shape: torch.Size([1536, 1536, 4, 4])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_1, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_2, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([6144]) <-> Ckpt Shape: torch.Size([6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([6144, 1536]) <-> Ckpt Shape: torch.Size([6144, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([1536, 6144]) <-> Ckpt Shape: torch.Size([1536, 6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.bias, Model Shape: torch.Size([3077]) <-> Ckpt Shape: torch.Size([3077])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.weight, Model Shape: torch.Size([3077, 1536]) <-> Ckpt Shape: torch.Size([3077, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([1536, 1, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([1536, 1, 5, 5]) <-> Ckpt Shape: torch.Size([1536, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([1536, 1, 7, 7]) <-> Ckpt Shape: torch.Size([1536, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([1536, 1, 9, 9]) <-> Ckpt Shape: torch.Size([1536, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.weight, Model Shape: torch.Size([1536, 1536, 1, 1]) <-> Ckpt Shape: torch.Size([1536, 1536, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.weight, Model Shape: torch.Size([1536, 1536]) <-> Ckpt Shape: torch.Size([1536, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.norm0.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.norm0.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.norm3.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.norm3.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.norm.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.norm.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.proj.weight, Model Shape: torch.Size([192, 3, 7, 7]) <-> Ckpt Shape: torch.Size([192, 3, 7, 7])\n",
            "INFO:utils.model:Loaded criterion.empty_weight, Model Shape: torch.Size([3]) <-> Ckpt Shape: torch.Size([3])\n",
            "INFO:utils.model:Loaded dilation_kernel, Model Shape: torch.Size([1, 1, 3, 3]) <-> Ckpt Shape: torch.Size([1, 1, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_1.weight, Model Shape: torch.Size([512, 192, 1, 1]) <-> Ckpt Shape: torch.Size([512, 192, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_2.weight, Model Shape: torch.Size([512, 384, 1, 1]) <-> Ckpt Shape: torch.Size([512, 384, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_3.weight, Model Shape: torch.Size([512, 768, 1, 1]) <-> Ckpt Shape: torch.Size([512, 768, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.input_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.input_proj.weight, Model Shape: torch.Size([512, 1536, 1, 1]) <-> Ckpt Shape: torch.Size([512, 1536, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_1.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_2.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_3.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_4.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.mask_features.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.mask_features.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.class_embed, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.decoder_norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.decoder_norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding, Model Shape: torch.Size([77, 512]) <-> Ckpt Shape: torch.Size([77, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.token_embedding.weight, Model Shape: torch.Size([49408, 512]) <-> Ckpt Shape: torch.Size([49408, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_proj, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.logit_scale, Model Shape: torch.Size([]) <-> Ckpt Shape: torch.Size([])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.level_embed.weight, Model Shape: torch.Size([3, 512]) <-> Ckpt Shape: torch.Size([3, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.0.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.0.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.1.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.2.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_sptial_embed.0, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_sptial_embed.1, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_sptial_embed.2, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.pn_indicator.weight, Model Shape: torch.Size([2, 512]) <-> Ckpt Shape: torch.Size([2, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.query_embed.weight, Model Shape: torch.Size([101, 512]) <-> Ckpt Shape: torch.Size([101, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.query_feat.weight, Model Shape: torch.Size([101, 512]) <-> Ckpt Shape: torch.Size([101, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.spatial_embed.weight, Model Shape: torch.Size([32, 512]) <-> Ckpt Shape: torch.Size([32, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.spatial_featured.weight, Model Shape: torch.Size([32, 512]) <-> Ckpt Shape: torch.Size([32, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:detectron2.data.common:Serializing 30 elements to byte tensors and concatenating them all ...\n",
            "INFO:detectron2.data.common:Serialized dataset takes 0.01 MiB\n",
            "/usr/local/lib/python3.10/dist-packages/torch/utils/data/dataloader.py:557: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n",
            "  warnings.warn(_create_warning_msg(\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "INFO:pipeline.XDecoderPipeline:Task bdd10k_val_sem_seg. Inference done 11/15. Dataloading: 0.0024 s/iter. Inference: 0.1838 s/iter. Eval: 0.0326 s/iter. Total: 0.2188 s/iter. ETA=0:00:00\n",
            "INFO:datasets.evaluation.segmentation_evaluation:OrderedDict([('sem_seg', {'mIoU': 88.68577851321902, 'fwIoU': 96.69484173530284, 'IoU-void': 98.09390927661379, 'IoU-liver': 79.27764774982424, 'mACC': 95.08924754397108, 'pACC': 98.22345733642578, 'Recall-void': 98.77095846941614, 'Recall-liver': 91.40753661852602, 'Precision-void': 99.30605564648118, 'Precision-liver': 85.66132987162844})])\n",
            "INFO:trainer.default_trainer:{'bdd10k_val_sem_seg/sem_seg': OrderedDict([('sem_seg', {'mIoU': 88.68577851321902, 'fwIoU': 96.69484173530284, 'IoU-void': 98.09390927661379, 'IoU-liver': 79.27764774982424, 'mACC': 95.08924754397108, 'pACC': 98.22345733642578, 'Recall-void': 98.77095846941614, 'Recall-liver': 91.40753661852602, 'Precision-void': 99.30605564648118, 'Precision-liver': 85.66132987162844})])}\n"
          ]
        }
      ],
      "source": [
        "!python entry.py evaluate \\\n",
        "--conf_files configs/seem/focall_unicl_lang_v1.yaml \\\n",
        "--overrides \\\n",
        "COCO.INPUT.IMAGE_SIZE 256 \\\n",
        "MODEL.DECODER.HIDDEN_DIM 512 \\\n",
        "MODEL.ENCODER.CONVS_DIM 512 \\\n",
        "MODEL.ENCODER.MASK_DIM 512 \\\n",
        "VOC.TEST.BATCH_SIZE_TOTAL 1 \\\n",
        "TEST.BATCH_SIZE_TOTAL 1 \\\n",
        "REF.TEST.BATCH_SIZE_TOTAL 1 \\\n",
        "MODEL.ENCODER.NUM_CLASSES 2 \\\n",
        "MODEL.DECODER.TEST.PANOPTIC_ON False \\\n",
        "MODEL.DECODER.TEST.INSTANCE_ON False \\\n",
        "WEIGHT True \\\n",
        "RESUME_FROM /content/data/output/test/model_state_dict_23.pt"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4anbgtxcUu8C"
      },
      "source": [
        "To save the training state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2dL4UZNHapv",
        "outputId": "6e69b196-1a3b-4b84-aa68-297fb0cfdebb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "cp: cannot stat '/content/data/output/test/module_training_states.pt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "!cp /content/data/output/test/module_training_states.pt /content/drive/My\\ Drive/SEEM\\ weights/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "n3PQXis45Y-1"
      },
      "outputs": [],
      "source": [
        "!cp /content/data/output/test/model_state_dict_adopter.pt /content/drive/My\\ Drive/SEEM\\ weights/"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tvW4mGDJU4i3"
      },
      "source": [
        "To save model state"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aKJKTWYfXFgR",
        "outputId": "3e8ab3ba-8c99-46a1-8141-a5ddd3a857cb"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "WARNING:utils.arguments:Overrided COCO.INPUT.IMAGE_SIZE from 1024 to 1024\n",
            "WARNING:utils.arguments:Overrided MODEL.DECODER.HIDDEN_DIM from 512 to 512\n",
            "WARNING:utils.arguments:Overrided MODEL.ENCODER.CONVS_DIM from 512 to 512\n",
            "WARNING:utils.arguments:Overrided MODEL.ENCODER.MASK_DIM from 512 to 512\n",
            "WARNING:utils.arguments:Overrided VOC.TEST.BATCH_SIZE_TOTAL from 8 to 1\n",
            "WARNING:utils.arguments:Overrided TEST.BATCH_SIZE_TOTAL from 8 to 1\n",
            "WARNING:utils.arguments:Overrided REF.TEST.BATCH_SIZE_TOTAL from 1 to 1\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "WARNING:datasets.registration.register_vlp_datasets:WARNING: Cannot find VLPreDataset. Make sure datasets are accessible if you want to use them for training or evaluation.\n",
            "Deformable Transformer Encoder is not available.\n",
            "INFO:trainer.distributed_trainer:Setting SAVE_DIR as /content/data/output/test\n",
            "INFO:trainer.distributed_trainer:Using CUDA\n",
            "WARNING:trainer.utils.mpi_adapter:----------------\n",
            "WARNING:trainer.utils.mpi_adapter:MPI Adapter data\n",
            "WARNING:trainer.utils.mpi_adapter:----------------\n",
            "WARNING:trainer.utils.mpi_adapter:environment info: no MPI\n",
            "WARNING:trainer.utils.mpi_adapter:init method url: tcp://127.0.0.1:36873\n",
            "WARNING:trainer.utils.mpi_adapter:world size: 1\n",
            "WARNING:trainer.utils.mpi_adapter:local size: 1\n",
            "WARNING:trainer.utils.mpi_adapter:rank: 0\n",
            "WARNING:trainer.utils.mpi_adapter:local rank: 0\n",
            "WARNING:trainer.utils.mpi_adapter:master address: 127.0.0.1\n",
            "WARNING:trainer.utils.mpi_adapter:master port: 36873\n",
            "WARNING:trainer.utils.mpi_adapter:----------------\n",
            "INFO:trainer.distributed_trainer:Save config file to /content/data/output/test/conf_copy.yaml\n",
            "INFO:trainer.distributed_trainer:Base learning rate: 0.0001\n",
            "INFO:trainer.distributed_trainer:Number of GPUs: 1\n",
            "INFO:trainer.distributed_trainer:Gradient accumulation steps: 1\n",
            "INFO:trainer.default_trainer:Imported base_dir at base_path ./\n",
            "INFO:trainer.default_trainer:-----------------------------------------------\n",
            "INFO:trainer.default_trainer:Evaluating model ... \n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init weight of Linear/Conv2d from trunc norm\n",
            "INFO:modeling.language.LangEncoder.transformer:=> init bias of Linear/Conv2d to zeros\n",
            "INFO:pipeline.XDecoderPipeline:GeneralizedSEEM(\n",
            "  (backbone): D2FocalNet(\n",
            "    (patch_embed): PatchEmbed(\n",
            "      (proj): Conv2d(3, 192, kernel_size=(7, 7), stride=(4, 4), padding=(2, 2))\n",
            "      (norm): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "    )\n",
            "    (pos_drop): Dropout(p=0.0, inplace=False)\n",
            "    (layers): ModuleList(\n",
            "      (0): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0): FocalModulationBlock(\n",
            "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=192, out_features=389, bias=True)\n",
            "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): Identity()\n",
            "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "          (1): FocalModulationBlock(\n",
            "            (norm1): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=192, out_features=389, bias=True)\n",
            "              (h): Conv2d(192, 192, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=192, out_features=192, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(192, 192, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=192, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=192, out_features=768, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=768, out_features=192, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchEmbed(\n",
            "          (proj): Conv2d(192, 384, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (norm): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (1): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0-1): 2 x FocalModulationBlock(\n",
            "            (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=384, out_features=773, bias=True)\n",
            "              (h): Conv2d(384, 384, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=384, out_features=384, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(384, 384, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=384, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(384, 384, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=384, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(384, 384, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=384, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(384, 384, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=384, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=384, out_features=1536, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=1536, out_features=384, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchEmbed(\n",
            "          (proj): Conv2d(384, 768, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (norm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (2): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0-17): 18 x FocalModulationBlock(\n",
            "            (norm1): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=768, out_features=1541, bias=True)\n",
            "              (h): Conv2d(768, 768, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=768, out_features=768, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(768, 768, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=768, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(768, 768, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=768, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(768, 768, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=768, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(768, 768, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=768, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=768, out_features=3072, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=3072, out_features=768, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "        (downsample): PatchEmbed(\n",
            "          (proj): Conv2d(768, 1536, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1))\n",
            "          (norm): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (3): BasicLayer(\n",
            "        (blocks): ModuleList(\n",
            "          (0-1): 2 x FocalModulationBlock(\n",
            "            (norm1): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (modulation): FocalModulation(\n",
            "              (f): Linear(in_features=1536, out_features=3077, bias=True)\n",
            "              (h): Conv2d(1536, 1536, kernel_size=(1, 1), stride=(1, 1))\n",
            "              (act): GELU(approximate='none')\n",
            "              (proj): Linear(in_features=1536, out_features=1536, bias=True)\n",
            "              (proj_drop): Dropout(p=0.0, inplace=False)\n",
            "              (focal_layers): ModuleList(\n",
            "                (0): Sequential(\n",
            "                  (0): Conv2d(1536, 1536, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), groups=1536, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (1): Sequential(\n",
            "                  (0): Conv2d(1536, 1536, kernel_size=(5, 5), stride=(1, 1), padding=(2, 2), groups=1536, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (2): Sequential(\n",
            "                  (0): Conv2d(1536, 1536, kernel_size=(7, 7), stride=(1, 1), padding=(3, 3), groups=1536, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "                (3): Sequential(\n",
            "                  (0): Conv2d(1536, 1536, kernel_size=(9, 9), stride=(1, 1), padding=(4, 4), groups=1536, bias=False)\n",
            "                  (1): GELU(approximate='none')\n",
            "                )\n",
            "              )\n",
            "            )\n",
            "            (drop_path): DropPath()\n",
            "            (norm2): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "            (mlp): Mlp(\n",
            "              (fc1): Linear(in_features=1536, out_features=6144, bias=True)\n",
            "              (act): GELU(approximate='none')\n",
            "              (fc2): Linear(in_features=6144, out_features=1536, bias=True)\n",
            "              (drop): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "    (norm0): LayerNorm((192,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm1): LayerNorm((384,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm2): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n",
            "    (norm3): LayerNorm((1536,), eps=1e-05, elementwise_affine=True)\n",
            "  )\n",
            "  (sem_seg_head): XdecoderHead(\n",
            "    (pixel_decoder): TransformerEncoderPixelDecoder(\n",
            "      (adapter_1): Conv2d(\n",
            "        192, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_1): Conv2d(\n",
            "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (adapter_2): Conv2d(\n",
            "        384, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_2): Conv2d(\n",
            "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (adapter_3): Conv2d(\n",
            "        768, 512, kernel_size=(1, 1), stride=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (layer_3): Conv2d(\n",
            "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "      (mask_features): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
            "      (input_proj): Conv2d(1536, 512, kernel_size=(1, 1), stride=(1, 1))\n",
            "      (transformer): TransformerEncoderOnly(\n",
            "        (encoder): TransformerEncoder(\n",
            "          (layers): ModuleList(\n",
            "            (0-5): 6 x TransformerEncoderLayer(\n",
            "              (self_attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "              )\n",
            "              (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "              (dropout): Dropout(p=0.0, inplace=False)\n",
            "              (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              (norm1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "              (norm2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "              (dropout1): Dropout(p=0.0, inplace=False)\n",
            "              (dropout2): Dropout(p=0.0, inplace=False)\n",
            "            )\n",
            "          )\n",
            "        )\n",
            "      )\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 256\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (layer_4): Conv2d(\n",
            "        512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False\n",
            "        (norm): GroupNorm(32, 512, eps=1e-05, affine=True)\n",
            "      )\n",
            "    )\n",
            "    (predictor): SEEMDecoder(\n",
            "      (pe_layer): Positional encoding PositionEmbeddingSine\n",
            "          num_pos_feats: 256\n",
            "          temperature: 10000\n",
            "          normalize: True\n",
            "          scale: 6.283185307179586\n",
            "      (transformer_self_attention_layers): ModuleList(\n",
            "        (0-8): 9 x SelfAttentionLayer(\n",
            "          (self_attn): MultiheadAttention(\n",
            "            (out_proj): _LinearWithBias(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_cross_attention_layers): ModuleList(\n",
            "        (0-8): 9 x CrossAttentionLayer(\n",
            "          (multihead_attn): MultiheadAttention(\n",
            "            (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "          )\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers): ModuleList(\n",
            "        (0-8): 9 x FFNLayer(\n",
            "          (linear1): Linear(in_features=512, out_features=2048, bias=True)\n",
            "          (dropout): Dropout(p=0.0, inplace=False)\n",
            "          (linear2): Linear(in_features=2048, out_features=512, bias=True)\n",
            "          (norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "        )\n",
            "      )\n",
            "      (transformer_ffn_layers_adapters): ModuleList()\n",
            "      (transformer_cross_attention_layers_adapters): ModuleList()\n",
            "      (transformer_self_attention_layers_adapters): ModuleList()\n",
            "      (predictions_heads_mask_embs_adapters): ModuleList(\n",
            "        (0): Adapter(\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "          (layer_norm_input): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (layer_norm_output): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "          (down_proj): Linear(in_features=512, out_features=256, bias=True)\n",
            "          (non_linear_func): ReLU()\n",
            "          (up_proj): Linear(in_features=256, out_features=512, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (decoder_norm): LayerNorm((512,), eps=1e-05, elementwise_affine=True)\n",
            "      (query_feat): Embedding(101, 512)\n",
            "      (query_embed): Embedding(101, 512)\n",
            "      (level_embed): Embedding(3, 512)\n",
            "      (input_proj): ModuleList(\n",
            "        (0-2): 3 x Sequential()\n",
            "      )\n",
            "      (lang_encoder): LanguageEncoder(\n",
            "        (lang_encoder): Transformer(\n",
            "          (token_embedding): Embedding(49408, 512)\n",
            "          (resblocks): ModuleList(\n",
            "            (0-11): 12 x ResidualAttentionBlock(\n",
            "              (attn): MultiheadAttention(\n",
            "                (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)\n",
            "              )\n",
            "              (ln_1): LayerNorm()\n",
            "              (mlp): Sequential(\n",
            "                (c_fc): Linear(in_features=512, out_features=2048, bias=True)\n",
            "                (gelu): QuickGELU()\n",
            "                (c_proj): Linear(in_features=2048, out_features=512, bias=True)\n",
            "              )\n",
            "              (ln_2): LayerNorm()\n",
            "              (drop_path): Identity()\n",
            "            )\n",
            "          )\n",
            "          (ln_final): LayerNorm()\n",
            "        )\n",
            "      )\n",
            "      (mask_embed): MLP(\n",
            "        (layers): ModuleList(\n",
            "          (0-2): 3 x Linear(in_features=512, out_features=512, bias=True)\n",
            "        )\n",
            "      )\n",
            "      (mask_sptial_embed): ParameterList(\n",
            "          (0): Parameter containing: [torch.float32 of size 512x512]\n",
            "          (1): Parameter containing: [torch.float32 of size 512x512]\n",
            "          (2): Parameter containing: [torch.float32 of size 512x512]\n",
            "      )\n",
            "      (spatial_embed): Embedding(32, 512)\n",
            "      (spatial_featured): Embedding(32, 512)\n",
            "      (pn_indicator): Embedding(2, 512)\n",
            "      (attention_data): AttentionDataStruct()\n",
            "    )\n",
            "  )\n",
            "  (criterion): Criterion SetCriterion\n",
            "      matcher: Matcher HungarianMatcher\n",
            "          cost_class: 2.0\n",
            "          cost_mask: 5.0\n",
            "          cost_dice: 5.0\n",
            "      losses: []\n",
            "      weight_dict: {'loss_mask_ce_0': 2.0, 'loss_mask_dice_0': 5.0, 'loss_mask_bce_0': 5.0, 'loss_spatial_ce_0': 0.4, 'loss_spatial_dice_0': 1.0, 'loss_spatial_bce_0': 1.0, 'loss_grounding_ce_0': 0.4, 'loss_grounding_dice_0': 1.0, 'loss_grounding_bce_0': 1.0, 'loss_openimage_ce_0': 0.4, 'loss_openimage_dice_0': 1.0, 'loss_openimage_bce_0': 1.0, 'loss_mask_ce_1': 2.0, 'loss_mask_dice_1': 5.0, 'loss_mask_bce_1': 5.0, 'loss_spatial_ce_1': 0.4, 'loss_spatial_dice_1': 1.0, 'loss_spatial_bce_1': 1.0, 'loss_grounding_ce_1': 0.4, 'loss_grounding_dice_1': 1.0, 'loss_grounding_bce_1': 1.0, 'loss_openimage_ce_1': 0.4, 'loss_openimage_dice_1': 1.0, 'loss_openimage_bce_1': 1.0, 'loss_mask_ce_2': 2.0, 'loss_mask_dice_2': 5.0, 'loss_mask_bce_2': 5.0, 'loss_spatial_ce_2': 0.4, 'loss_spatial_dice_2': 1.0, 'loss_spatial_bce_2': 1.0, 'loss_grounding_ce_2': 0.4, 'loss_grounding_dice_2': 1.0, 'loss_grounding_bce_2': 1.0, 'loss_openimage_ce_2': 0.4, 'loss_openimage_dice_2': 1.0, 'loss_openimage_bce_2': 1.0, 'loss_mask_ce_3': 2.0, 'loss_mask_dice_3': 5.0, 'loss_mask_bce_3': 5.0, 'loss_spatial_ce_3': 0.4, 'loss_spatial_dice_3': 1.0, 'loss_spatial_bce_3': 1.0, 'loss_grounding_ce_3': 0.4, 'loss_grounding_dice_3': 1.0, 'loss_grounding_bce_3': 1.0, 'loss_openimage_ce_3': 0.4, 'loss_openimage_dice_3': 1.0, 'loss_openimage_bce_3': 1.0, 'loss_mask_ce_4': 2.0, 'loss_mask_dice_4': 5.0, 'loss_mask_bce_4': 5.0, 'loss_spatial_ce_4': 0.4, 'loss_spatial_dice_4': 1.0, 'loss_spatial_bce_4': 1.0, 'loss_grounding_ce_4': 0.4, 'loss_grounding_dice_4': 1.0, 'loss_grounding_bce_4': 1.0, 'loss_openimage_ce_4': 0.4, 'loss_openimage_dice_4': 1.0, 'loss_openimage_bce_4': 1.0, 'loss_mask_ce_5': 2.0, 'loss_mask_dice_5': 5.0, 'loss_mask_bce_5': 5.0, 'loss_spatial_ce_5': 0.4, 'loss_spatial_dice_5': 1.0, 'loss_spatial_bce_5': 1.0, 'loss_grounding_ce_5': 0.4, 'loss_grounding_dice_5': 1.0, 'loss_grounding_bce_5': 1.0, 'loss_openimage_ce_5': 0.4, 'loss_openimage_dice_5': 1.0, 'loss_openimage_bce_5': 1.0, 'loss_mask_ce_6': 2.0, 'loss_mask_dice_6': 5.0, 'loss_mask_bce_6': 5.0, 'loss_spatial_ce_6': 0.4, 'loss_spatial_dice_6': 1.0, 'loss_spatial_bce_6': 1.0, 'loss_grounding_ce_6': 0.4, 'loss_grounding_dice_6': 1.0, 'loss_grounding_bce_6': 1.0, 'loss_openimage_ce_6': 0.4, 'loss_openimage_dice_6': 1.0, 'loss_openimage_bce_6': 1.0, 'loss_mask_ce_7': 2.0, 'loss_mask_dice_7': 5.0, 'loss_mask_bce_7': 5.0, 'loss_spatial_ce_7': 0.4, 'loss_spatial_dice_7': 1.0, 'loss_spatial_bce_7': 1.0, 'loss_grounding_ce_7': 0.4, 'loss_grounding_dice_7': 1.0, 'loss_grounding_bce_7': 1.0, 'loss_openimage_ce_7': 0.4, 'loss_openimage_dice_7': 1.0, 'loss_openimage_bce_7': 1.0, 'loss_mask_ce_8': 2.0, 'loss_mask_dice_8': 5.0, 'loss_mask_bce_8': 5.0, 'loss_spatial_ce_8': 0.4, 'loss_spatial_dice_8': 1.0, 'loss_spatial_bce_8': 1.0, 'loss_grounding_ce_8': 0.4, 'loss_grounding_dice_8': 1.0, 'loss_grounding_bce_8': 1.0, 'loss_openimage_ce_8': 0.4, 'loss_openimage_dice_8': 1.0, 'loss_openimage_bce_8': 1.0, 'loss_mask_ce_9': 2.0, 'loss_mask_dice_9': 5.0, 'loss_mask_bce_9': 5.0, 'loss_spatial_ce_9': 0.4, 'loss_spatial_dice_9': 1.0, 'loss_spatial_bce_9': 1.0, 'loss_grounding_ce_9': 0.4, 'loss_grounding_dice_9': 1.0, 'loss_grounding_bce_9': 1.0, 'loss_openimage_ce_9': 0.4, 'loss_openimage_dice_9': 1.0, 'loss_openimage_bce_9': 1.0}\n",
            "      num_classes: 133\n",
            "      eos_coef: 0.1\n",
            "      num_points: 12544\n",
            "      oversample_ratio: 3.0\n",
            "      importance_sample_ratio: 0.75\n",
            ")\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.bias, Model Shape: torch.Size([389]) <-> Ckpt Shape: torch.Size([389])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.f.weight, Model Shape: torch.Size([389, 192]) <-> Ckpt Shape: torch.Size([389, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([192, 1, 9, 9]) <-> Ckpt Shape: torch.Size([192, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.0.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_1, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.gamma_2, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([768, 192]) <-> Ckpt Shape: torch.Size([768, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([192, 768]) <-> Ckpt Shape: torch.Size([192, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.bias, Model Shape: torch.Size([389]) <-> Ckpt Shape: torch.Size([389])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.f.weight, Model Shape: torch.Size([389, 192]) <-> Ckpt Shape: torch.Size([389, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([192, 1, 3, 3]) <-> Ckpt Shape: torch.Size([192, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([192, 1, 5, 5]) <-> Ckpt Shape: torch.Size([192, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([192, 1, 7, 7]) <-> Ckpt Shape: torch.Size([192, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([192, 1, 9, 9]) <-> Ckpt Shape: torch.Size([192, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.h.weight, Model Shape: torch.Size([192, 192, 1, 1]) <-> Ckpt Shape: torch.Size([192, 192, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.modulation.proj.weight, Model Shape: torch.Size([192, 192]) <-> Ckpt Shape: torch.Size([192, 192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm1.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.blocks.1.norm2.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.norm.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.0.downsample.proj.weight, Model Shape: torch.Size([384, 192, 3, 3]) <-> Ckpt Shape: torch.Size([384, 192, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.bias, Model Shape: torch.Size([773]) <-> Ckpt Shape: torch.Size([773])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.f.weight, Model Shape: torch.Size([773, 384]) <-> Ckpt Shape: torch.Size([773, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([384, 1, 9, 9]) <-> Ckpt Shape: torch.Size([384, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.0.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_1, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.gamma_2, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([1536, 384]) <-> Ckpt Shape: torch.Size([1536, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([384, 1536]) <-> Ckpt Shape: torch.Size([384, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.bias, Model Shape: torch.Size([773]) <-> Ckpt Shape: torch.Size([773])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.f.weight, Model Shape: torch.Size([773, 384]) <-> Ckpt Shape: torch.Size([773, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([384, 1, 3, 3]) <-> Ckpt Shape: torch.Size([384, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([384, 1, 5, 5]) <-> Ckpt Shape: torch.Size([384, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([384, 1, 7, 7]) <-> Ckpt Shape: torch.Size([384, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([384, 1, 9, 9]) <-> Ckpt Shape: torch.Size([384, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.h.weight, Model Shape: torch.Size([384, 384, 1, 1]) <-> Ckpt Shape: torch.Size([384, 384, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.modulation.proj.weight, Model Shape: torch.Size([384, 384]) <-> Ckpt Shape: torch.Size([384, 384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.blocks.1.norm2.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.norm.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.1.downsample.proj.weight, Model Shape: torch.Size([768, 384, 3, 3]) <-> Ckpt Shape: torch.Size([768, 384, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.0.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.1.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.10.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.11.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.12.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.13.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.14.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.15.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.16.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.17.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.2.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.3.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.4.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.5.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.6.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.7.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.8.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.gamma_1, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.gamma_2, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc1.bias, Model Shape: torch.Size([3072]) <-> Ckpt Shape: torch.Size([3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc1.weight, Model Shape: torch.Size([3072, 768]) <-> Ckpt Shape: torch.Size([3072, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.mlp.fc2.weight, Model Shape: torch.Size([768, 3072]) <-> Ckpt Shape: torch.Size([768, 3072])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.f.bias, Model Shape: torch.Size([1541]) <-> Ckpt Shape: torch.Size([1541])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.f.weight, Model Shape: torch.Size([1541, 768]) <-> Ckpt Shape: torch.Size([1541, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([768, 1, 3, 3]) <-> Ckpt Shape: torch.Size([768, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([768, 1, 5, 5]) <-> Ckpt Shape: torch.Size([768, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([768, 1, 7, 7]) <-> Ckpt Shape: torch.Size([768, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([768, 1, 9, 9]) <-> Ckpt Shape: torch.Size([768, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.h.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.h.weight, Model Shape: torch.Size([768, 768, 1, 1]) <-> Ckpt Shape: torch.Size([768, 768, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.proj.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.modulation.proj.weight, Model Shape: torch.Size([768, 768]) <-> Ckpt Shape: torch.Size([768, 768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm1.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm1.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.blocks.9.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.norm.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.2.downsample.proj.weight, Model Shape: torch.Size([1536, 768, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 768, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_1, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.gamma_2, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.bias, Model Shape: torch.Size([6144]) <-> Ckpt Shape: torch.Size([6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc1.weight, Model Shape: torch.Size([6144, 1536]) <-> Ckpt Shape: torch.Size([6144, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.mlp.fc2.weight, Model Shape: torch.Size([1536, 6144]) <-> Ckpt Shape: torch.Size([1536, 6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.bias, Model Shape: torch.Size([3077]) <-> Ckpt Shape: torch.Size([3077])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.f.weight, Model Shape: torch.Size([3077, 1536]) <-> Ckpt Shape: torch.Size([3077, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([1536, 1, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([1536, 1, 5, 5]) <-> Ckpt Shape: torch.Size([1536, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([1536, 1, 7, 7]) <-> Ckpt Shape: torch.Size([1536, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([1536, 1, 9, 9]) <-> Ckpt Shape: torch.Size([1536, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.h.weight, Model Shape: torch.Size([1536, 1536, 1, 1]) <-> Ckpt Shape: torch.Size([1536, 1536, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.modulation.proj.weight, Model Shape: torch.Size([1536, 1536]) <-> Ckpt Shape: torch.Size([1536, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm1.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.0.norm2.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_1, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.gamma_2, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.bias, Model Shape: torch.Size([6144]) <-> Ckpt Shape: torch.Size([6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc1.weight, Model Shape: torch.Size([6144, 1536]) <-> Ckpt Shape: torch.Size([6144, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.mlp.fc2.weight, Model Shape: torch.Size([1536, 6144]) <-> Ckpt Shape: torch.Size([1536, 6144])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.bias, Model Shape: torch.Size([3077]) <-> Ckpt Shape: torch.Size([3077])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.f.weight, Model Shape: torch.Size([3077, 1536]) <-> Ckpt Shape: torch.Size([3077, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.0.0.weight, Model Shape: torch.Size([1536, 1, 3, 3]) <-> Ckpt Shape: torch.Size([1536, 1, 3, 3])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.1.0.weight, Model Shape: torch.Size([1536, 1, 5, 5]) <-> Ckpt Shape: torch.Size([1536, 1, 5, 5])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.2.0.weight, Model Shape: torch.Size([1536, 1, 7, 7]) <-> Ckpt Shape: torch.Size([1536, 1, 7, 7])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.focal_layers.3.0.weight, Model Shape: torch.Size([1536, 1, 9, 9]) <-> Ckpt Shape: torch.Size([1536, 1, 9, 9])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.h.weight, Model Shape: torch.Size([1536, 1536, 1, 1]) <-> Ckpt Shape: torch.Size([1536, 1536, 1, 1])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.modulation.proj.weight, Model Shape: torch.Size([1536, 1536]) <-> Ckpt Shape: torch.Size([1536, 1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm1.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.layers.3.blocks.1.norm2.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.norm0.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.norm0.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.norm1.bias, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.norm1.weight, Model Shape: torch.Size([384]) <-> Ckpt Shape: torch.Size([384])\n",
            "INFO:utils.model:Loaded backbone.norm2.bias, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.norm2.weight, Model Shape: torch.Size([768]) <-> Ckpt Shape: torch.Size([768])\n",
            "INFO:utils.model:Loaded backbone.norm3.bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.norm3.weight, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.norm.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.norm.weight, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.proj.bias, Model Shape: torch.Size([192]) <-> Ckpt Shape: torch.Size([192])\n",
            "INFO:utils.model:Loaded backbone.patch_embed.proj.weight, Model Shape: torch.Size([192, 3, 7, 7]) <-> Ckpt Shape: torch.Size([192, 3, 7, 7])\n",
            "INFO:utils.model:Loaded criterion.empty_weight, Model Shape: torch.Size([134]) <-> Ckpt Shape: torch.Size([134])\n",
            "INFO:utils.model:Loaded dilation_kernel, Model Shape: torch.Size([1, 1, 3, 3]) <-> Ckpt Shape: torch.Size([1, 1, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_1.weight, Model Shape: torch.Size([512, 192, 1, 1]) <-> Ckpt Shape: torch.Size([512, 192, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_2.weight, Model Shape: torch.Size([512, 384, 1, 1]) <-> Ckpt Shape: torch.Size([512, 384, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.adapter_3.weight, Model Shape: torch.Size([512, 768, 1, 1]) <-> Ckpt Shape: torch.Size([512, 768, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.input_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.input_proj.weight, Model Shape: torch.Size([512, 1536, 1, 1]) <-> Ckpt Shape: torch.Size([512, 1536, 1, 1])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_1.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_2.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_3.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.layer_4.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.mask_features.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.mask_features.weight, Model Shape: torch.Size([512, 512, 3, 3]) <-> Ckpt Shape: torch.Size([512, 512, 3, 3])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.norm2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.pixel_decoder.transformer.encoder.layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.class_embed, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.decoder_norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.decoder_norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.ln_final.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.positional_embedding, Model Shape: torch.Size([77, 512]) <-> Ckpt Shape: torch.Size([77, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.0.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.1.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.10.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.11.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.2.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.3.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.4.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.5.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.6.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.7.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.8.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_1.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.ln_2.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_fc.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.resblocks.9.mlp.c_proj.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_encoder.token_embedding.weight, Model Shape: torch.Size([49408, 512]) <-> Ckpt Shape: torch.Size([49408, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.lang_proj, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.lang_encoder.logit_scale, Model Shape: torch.Size([]) <-> Ckpt Shape: torch.Size([])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.level_embed.weight, Model Shape: torch.Size([3, 512]) <-> Ckpt Shape: torch.Size([3, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.0.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.0.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.1.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.1.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_embed.layers.2.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_sptial_embed.0, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_sptial_embed.1, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.mask_sptial_embed.2, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.pn_indicator.weight, Model Shape: torch.Size([2, 512]) <-> Ckpt Shape: torch.Size([2, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.query_embed.weight, Model Shape: torch.Size([101, 512]) <-> Ckpt Shape: torch.Size([101, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.query_feat.weight, Model Shape: torch.Size([101, 512]) <-> Ckpt Shape: torch.Size([101, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.spatial_embed.weight, Model Shape: torch.Size([32, 512]) <-> Ckpt Shape: torch.Size([32, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.spatial_featured.weight, Model Shape: torch.Size([32, 512]) <-> Ckpt Shape: torch.Size([32, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.0.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.5.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.6.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.7.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.multihead_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_cross_attention_layers.8.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.0.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.5.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.6.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.7.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear1.bias, Model Shape: torch.Size([2048]) <-> Ckpt Shape: torch.Size([2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear1.weight, Model Shape: torch.Size([2048, 512]) <-> Ckpt Shape: torch.Size([2048, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear2.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.linear2.weight, Model Shape: torch.Size([512, 2048]) <-> Ckpt Shape: torch.Size([512, 2048])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_ffn_layers.8.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.0.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.1.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.2.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.3.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.4.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.5.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.6.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.7.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.norm.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.norm.weight, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_bias, Model Shape: torch.Size([1536]) <-> Ckpt Shape: torch.Size([1536])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.in_proj_weight, Model Shape: torch.Size([1536, 512]) <-> Ckpt Shape: torch.Size([1536, 512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.bias, Model Shape: torch.Size([512]) <-> Ckpt Shape: torch.Size([512])\n",
            "INFO:utils.model:Loaded sem_seg_head.predictor.transformer_self_attention_layers.8.self_attn.out_proj.weight, Model Shape: torch.Size([512, 512]) <-> Ckpt Shape: torch.Size([512, 512])\n",
            "WARNING:utils.model:*UNLOADED* sem_seg_head.predictor.predictions_heads_mask_embs_adapters.0.down_proj.bias, Model Shape: torch.Size([256])\n",
            "WARNING:utils.model:*UNLOADED* sem_seg_head.predictor.predictions_heads_mask_embs_adapters.0.down_proj.weight, Model Shape: torch.Size([256, 512])\n",
            "WARNING:utils.model:*UNLOADED* sem_seg_head.predictor.predictions_heads_mask_embs_adapters.0.layer_norm_input.bias, Model Shape: torch.Size([512])\n",
            "WARNING:utils.model:*UNLOADED* sem_seg_head.predictor.predictions_heads_mask_embs_adapters.0.layer_norm_input.weight, Model Shape: torch.Size([512])\n",
            "WARNING:utils.model:*UNLOADED* sem_seg_head.predictor.predictions_heads_mask_embs_adapters.0.layer_norm_output.bias, Model Shape: torch.Size([512])\n",
            "WARNING:utils.model:*UNLOADED* sem_seg_head.predictor.predictions_heads_mask_embs_adapters.0.layer_norm_output.weight, Model Shape: torch.Size([512])\n",
            "WARNING:utils.model:*UNLOADED* sem_seg_head.predictor.predictions_heads_mask_embs_adapters.0.scale, Model Shape: torch.Size([1])\n",
            "WARNING:utils.model:*UNLOADED* sem_seg_head.predictor.predictions_heads_mask_embs_adapters.0.up_proj.bias, Model Shape: torch.Size([512])\n",
            "WARNING:utils.model:*UNLOADED* sem_seg_head.predictor.predictions_heads_mask_embs_adapters.0.up_proj.weight, Model Shape: torch.Size([512, 256])\n",
            "INFO:trainer.default_trainer:Evaluation start ...\n",
            "INFO:detectron2.data.common:Serializing 500 elements to byte tensors and concatenating them all ...\n",
            "INFO:detectron2.data.common:Serialized dataset takes 0.12 MiB\n",
            "/usr/lib/python3.10/multiprocessing/popen_fork.py:66: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
            "  self.pid = os.fork()\n",
            "class_embed.shape= torch.Size([2, 101, 512])\n",
            "in language line 184\n",
            "t_emb.shape = torch.Size([36, 512])\n",
            "line 428 in seemv_1\n",
            "outputs_class.shape= torch.Size([2, 101, 36])\n",
            "outputs_mask.shape= torch.Size([2, 101, 200, 400])\n",
            "mask_embed.shape= torch.Size([2, 101, 512])\n",
            "class_embed.shape= torch.Size([2, 101, 512])\n",
            "in language line 184\n",
            "t_emb.shape = torch.Size([36, 512])\n",
            "line 428 in seemv_1\n",
            "outputs_class.shape= torch.Size([2, 101, 36])\n",
            "outputs_mask.shape= torch.Size([2, 101, 200, 400])\n",
            "mask_embed.shape= torch.Size([2, 101, 512])\n",
            "class_embed.shape= torch.Size([2, 101, 512])\n",
            "in language line 184\n",
            "t_emb.shape = torch.Size([36, 512])\n",
            "line 428 in seemv_1\n",
            "outputs_class.shape= torch.Size([2, 101, 36])\n",
            "outputs_mask.shape= torch.Size([2, 101, 200, 400])\n",
            "mask_embed.shape= torch.Size([2, 101, 512])\n",
            "class_embed.shape= torch.Size([2, 101, 512])\n",
            "in language line 184\n",
            "t_emb.shape = torch.Size([36, 512])\n",
            "line 428 in seemv_1\n",
            "outputs_class.shape= torch.Size([2, 101, 36])\n",
            "outputs_mask.shape= torch.Size([2, 101, 200, 400])\n",
            "mask_embed.shape= torch.Size([2, 101, 512])\n",
            "class_embed.shape= torch.Size([2, 101, 512])\n",
            "in language line 184\n",
            "t_emb.shape = torch.Size([36, 512])\n",
            "line 428 in seemv_1\n",
            "outputs_class.shape= torch.Size([2, 101, 36])\n",
            "outputs_mask.shape= torch.Size([2, 101, 200, 400])\n",
            "mask_embed.shape= torch.Size([2, 101, 512])\n",
            "class_embed.shape= torch.Size([2, 101, 512])\n",
            "in language line 184\n",
            "t_emb.shape = torch.Size([36, 512])\n",
            "line 428 in seemv_1\n",
            "outputs_class.shape= torch.Size([2, 101, 36])\n",
            "outputs_mask.shape= torch.Size([2, 101, 200, 400])\n",
            "mask_embed.shape= torch.Size([2, 101, 512])\n",
            "class_embed.shape= torch.Size([2, 101, 512])\n",
            "in language line 184\n",
            "t_emb.shape = torch.Size([36, 512])\n",
            "line 428 in seemv_1\n",
            "outputs_class.shape= torch.Size([2, 101, 36])\n",
            "outputs_mask.shape= torch.Size([2, 101, 200, 400])\n",
            "mask_embed.shape= torch.Size([2, 101, 512])\n",
            "class_embed.shape= torch.Size([2, 101, 512])\n",
            "in language line 184\n",
            "t_emb.shape = torch.Size([36, 512])\n",
            "line 428 in seemv_1\n",
            "outputs_class.shape= torch.Size([2, 101, 36])\n",
            "outputs_mask.shape= torch.Size([2, 101, 200, 400])\n",
            "mask_embed.shape= torch.Size([2, 101, 512])\n",
            "class_embed.shape= torch.Size([2, 101, 512])\n",
            "in language line 184\n",
            "t_emb.shape = torch.Size([36, 512])\n",
            "line 428 in seemv_1\n",
            "outputs_class.shape= torch.Size([2, 101, 36])\n",
            "outputs_mask.shape= torch.Size([2, 101, 200, 400])\n",
            "mask_embed.shape= torch.Size([2, 101, 512])\n",
            "class_embed.shape= torch.Size([2, 101, 512])\n",
            "in language line 184\n",
            "t_emb.shape = torch.Size([36, 512])\n",
            "line 428 in seemv_1\n",
            "outputs_class.shape= torch.Size([2, 101, 36])\n",
            "outputs_mask.shape= torch.Size([2, 101, 200, 400])\n",
            "mask_embed.shape= torch.Size([2, 101, 512])\n"
          ]
        }
      ],
      "source": [
        "!python entry.py evaluate \\\n",
        "--conf_files configs/seem/focall_unicl_lang_v1.yaml \\\n",
        "--overrides \\\n",
        "COCO.INPUT.IMAGE_SIZE 1024 \\\n",
        "MODEL.DECODER.HIDDEN_DIM 512 \\\n",
        "MODEL.ENCODER.CONVS_DIM 512 \\\n",
        "MODEL.ENCODER.MASK_DIM 512 \\\n",
        "VOC.TEST.BATCH_SIZE_TOTAL 1 \\\n",
        "TEST.BATCH_SIZE_TOTAL 1 \\\n",
        "REF.TEST.BATCH_SIZE_TOTAL 1 \\\n",
        "WEIGHT True \\\n",
        "RESUME_FROM /content/datasets/xdecoder_data/pretrained/seem_focall_v1.pt"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}